{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.etl.logs_parser import parse_bullet_logs, clean_bullets\n",
    "from src.modeling.openai_ner import compute_ner_for_all_bullets, extract_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_bullets = parse_bullet_logs(logs_path=\"data/raw/logs_2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullet_points = clean_bullets(bullets=raw_bullets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullet_points_as_dict = bullet_points.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': '',\n",
       "  'what': 'recommendation system',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Tensorflow, nlp, fargate, Python, Pytorch',\n",
       "  'why': 'To provide personalized recommendations based on customer preferences and behavior',\n",
       "  'how': 'unsupervized learning and content-based filtering',\n",
       "  'whom': 'a major relocation company',\n",
       "  'raw_bullet': 'Developed recommendation system using Tensorflow, nlp, fargate, Python and Pytorch leveraging onunsupervized learning and content-based filtering for a major relocation company To provide personalized recommendations based on customer preferences and behavior',\n",
       "  'corrected_bullet': 'Developed a recommendation system using Tensorflow, NLP, Fargate, Python, and Pytorch, leveraging unsupervised learning and content-based filtering for a major relocation company. The system provided personalized recommendations based on customer preferences and behavior.'},\n",
       " {'user_id': '',\n",
       "  'what': 'recommendation system',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Tensorflow, nlp, Python',\n",
       "  'why': 'To provide personalized recommendations based on customer preferences and behavior',\n",
       "  'how': 'unsupervized learning and content-based filtering',\n",
       "  'whom': 'a major relocation company',\n",
       "  'raw_bullet': 'Developed recommendation system using Tensorflow, nlp and Python leveraging onunsupervized learning and content-based filtering for a major relocation company To provide personalized recommendations based on customer preferences and behavior',\n",
       "  'corrected_bullet': 'I developed a recommendation system using Tensorflow, NLP, and Python, leveraging unsupervised learning and content-based filtering for a major relocation company. This system provided personalized recommendations based on customer preferences and behavior.'},\n",
       " {'user_id': '',\n",
       "  'what': 'general goal performance dashboard',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Tableau, Python, Sql',\n",
       "  'why': 'To provide a visual representation of key performance indicators (KPIs)',\n",
       "  'how': 'advanced timeline visualizations',\n",
       "  'whom': 'a retail company',\n",
       "  'raw_bullet': 'Built general goal performance dashboard using Tableau, Python and Sql leveraging onadvanced timeline visualizations for a retail company To provide a visual representation of key performance indicators (KPIs)',\n",
       "  'corrected_bullet': 'I built a general goal performance dashboard using Tableau, Python, and SQL, leveraging advanced timeline visualizations for a retail company to provide a visual representation of key performance indicators (KPIs).'},\n",
       " {'user_id': '',\n",
       "  'what': 'desktop application',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, React, Sql',\n",
       "  'why': 'To enable offline access and functionality, reducing dependence on internet connectivity',\n",
       "  'how': 'hyperthreading and java applets',\n",
       "  'whom': 'a software company',\n",
       "  'raw_bullet': 'Built desktop application using Python, React and Sql leveraging onhyperthreading and java applets for a software company To enable offline access and functionality, reducing dependence on internet connectivity',\n",
       "  'corrected_bullet': 'Built a desktop application using Python, React, and SQL, leveraging hyperthreading and Java applets for a software company. This enabled offline access and functionality, reducing dependence on internet connectivity.'},\n",
       " {'user_id': '',\n",
       "  'what': 'arima time series model',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Sklearn, Sktime',\n",
       "  'why': 'To identify and detect anomalies or outliers in time series data',\n",
       "  'how': 'Windowed partial prediction schemas',\n",
       "  'whom': 'A retail company',\n",
       "  'raw_bullet': 'Built arima time series model using Python, Sklearn and Sktime leveraging onWindowed partial prediction schemas for A retail company To identify and detect anomalies or outliers in time series data',\n",
       "  'corrected_bullet': 'I built an ARIMA time series model using Python, Sklearn, and Sktime, leveraging windowed partial prediction schemas for a retail company to identify and detect anomalies or outliers in time series data.'},\n",
       " {'user_id': '',\n",
       "  'what': 'arima time series model',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python',\n",
       "  'why': 'To detect anomalies or outliers in the time series data',\n",
       "  'how': 'using arima',\n",
       "  'whom': 'a fintech area',\n",
       "  'raw_bullet': 'Built arima time series model using Python leveraging onusing arima for a fintech area To detect anomalies or outliers in the time series data',\n",
       "  'corrected_bullet': 'I built an ARIMA time series model using Python, leveraging ARIMA for a fintech area, to detect anomalies or outliers in the time series data.'},\n",
       " {'user_id': '',\n",
       "  'what': 'workshop',\n",
       "  'verb': 'Conducted',\n",
       "  'skills': 'Jira, scrum, coach101',\n",
       "  'why': 'To enhance communication and problem-solving abilities',\n",
       "  'how': 'vocal coaching techniques',\n",
       "  'whom': 'a talent development company',\n",
       "  'raw_bullet': 'Conducted workshop using Jira, scrum and coach101 leveraging onvocal coaching techniques for a talent development company To enhance communication and problem-solving abilities',\n",
       "  'corrected_bullet': 'Conducted a workshop using Jira, Scrum, and Coach101, leveraging on vocal coaching techniques for a talent development company to enhance communication and problem-solving abilities.'},\n",
       " {'user_id': '',\n",
       "  'what': 'text clustering model',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Tensorflow',\n",
       "  'why': 'To support decision-making processes by providing relevant and organized information',\n",
       "  'how': '',\n",
       "  'whom': 'a retail company',\n",
       "  'raw_bullet': 'Built text clustering model using Python and Tensorflow  for a retail company To support decision-making processes by providing relevant and organized information',\n",
       "  'corrected_bullet': 'I built a text clustering model using Python and Tensorflow for a retail company to support decision-making processes by providing relevant and organized information.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl pipelines',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Sql, Airflow',\n",
       "  'why': 'To extract data from various sources and transform it into a usable format for analysis and storage',\n",
       "  'how': '',\n",
       "  'whom': 'a financial company',\n",
       "  'raw_bullet': 'Developed etl pipelines using Sql and Airflow  for a financial company To extract data from various sources and transform it into a usable format for analysis and storage',\n",
       "  'corrected_bullet': 'Developed ETL pipelines using SQL and Airflow for a financial company to extract data from various sources and transform it into a usable format for analysis and storage.'},\n",
       " {'user_id': '',\n",
       "  'what': 'machine learning platform',\n",
       "  'verb': 'Azured',\n",
       "  'skills': 'Aws',\n",
       "  'why': 'Facilitate the development of intelligent applications and services',\n",
       "  'how': '',\n",
       "  'whom': 'a retail company',\n",
       "  'raw_bullet': 'Azured machine learning platform using Aws  for a retail company Facilitate the development of intelligent applications and services',\n",
       "  'corrected_bullet': 'Using aws, the azure machine learning platform facilitated the development of intelligent applications and services for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl data pipelines',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Aws, Databricks, Airflow',\n",
       "  'why': 'To efficiently extract, transform, and load (ETL) large volumes of data into Azure data warehouses or data lakes',\n",
       "  'how': '',\n",
       "  'whom': 'a retail company',\n",
       "  'raw_bullet': 'Developed etl data pipelines using Aws, Databricks and Airflow  for a retail company To efficiently extract, transform, and load (ETL) large volumes of data into Azure data warehouses or data lakes',\n",
       "  'corrected_bullet': 'Developed etl data pipelines using aws, databricks, and airflow for a retail company to efficiently extract, transform, and load (etl) large volumes of data into azure data warehouses or data lakes.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl data pipeline',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Airflow, Snowflake',\n",
       "  'why': 'To facilitate data migration between different systems or platforms',\n",
       "  'how': 'a data warehouse',\n",
       "  'whom': 'a retail company',\n",
       "  'raw_bullet': 'Developed etl data pipeline using Airflow and Snowflake leveraging ona data warehouse To facilitate data migration between different systems or platforms for a retail company',\n",
       "  'corrected_bullet': 'Developed an etl data pipeline using airflow and snowflake, leveraging a data warehouse to facilitate data migration between different systems or platforms for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'daily forecasting model',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Transformers',\n",
       "  'why': 'To make accurate predictions about future sales and demand',\n",
       "  'how': 'ARIMA model',\n",
       "  'whom': 'a financial company',\n",
       "  'raw_bullet': 'Developed daily forecasting model using Python and Transformers leveraging onARIMA model To make accurate predictions about future sales and demand for a financial company',\n",
       "  'corrected_bullet': 'Developed a daily forecasting model using Python and Transformers, leveraging the ARIMA model, to make accurate predictions about future sales and demand for a financial company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'neural ml networks',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Pytorch, Python, Spark',\n",
       "  'why': 'To reduce costs and increase profitability',\n",
       "  'how': '',\n",
       "  'whom': 'a healthcare company',\n",
       "  'raw_bullet': 'Built neural ml networks using Pytorch, Python and Spark  for a healthcare company To reduce costs and increase profitability',\n",
       "  'corrected_bullet': 'Built neural ML networks using PyTorch, Python, and Spark for a healthcare company to reduce costs and increase profitability.'},\n",
       " {'user_id': '',\n",
       "  'what': 'balanced scorecard dashboard',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Tableau, Sql',\n",
       "  'why': \"To provide a comprehensive view of the company's performance\",\n",
       "  'how': '',\n",
       "  'whom': 'financial services company',\n",
       "  'raw_bullet': \"Developed balanced scorecard dashboard using Tableau and Sql  To provide a comprehensive view of the company's performance for financial services company\",\n",
       "  'corrected_bullet': \"Developed a balanced scorecard dashboard using Tableau and SQL to provide a comprehensive view of the financial services company's performance.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'balanced scorecard dashboard',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Tableau, Sql',\n",
       "  'why': \"To provide a comprehensive view of the company's performance\",\n",
       "  'how': '',\n",
       "  'whom': 'financial services company',\n",
       "  'raw_bullet': \"Developed balanced scorecard dashboard using Tableau and Sql  To provide a comprehensive view of the company's performance for financial services company\",\n",
       "  'corrected_bullet': \"Developed a balanced scorecard dashboard using Tableau and SQL to provide a comprehensive view of the financial services company's performance.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'regression model',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Scikit Learn',\n",
       "  'why': 'To assess the impact of different scenarios or changes in variables on business performance',\n",
       "  'how': 'linear regression',\n",
       "  'whom': 'a b2b',\n",
       "  'raw_bullet': 'Developed regression model using Python and Scikit Learn leveraging onlinear regression To assess the impact of different scenarios or changes in variables on business performance for a b2b',\n",
       "  'corrected_bullet': 'Developed a regression model using Python and Scikit Learn, leveraging onlinear regression, to assess the impact of different scenarios or changes in variables on business performance for a B2B.'},\n",
       " {'user_id': '',\n",
       "  'what': 'frontend',\n",
       "  'verb': 'Developed Design',\n",
       "  'skills': 'Typescript',\n",
       "  'why': 'To create a visually appealing and user-friendly interface for their website or application',\n",
       "  'how': '',\n",
       "  'whom': 'MLCommons ',\n",
       "  'raw_bullet': 'Developed Design frontend using Typescript  To create a visually appealing and user-friendly interface for their website or application for MLCommons ',\n",
       "  'corrected_bullet': \"I developed the frontend design using Typescript to create a visually appealing and user-friendly interface for MLCommons' website or application.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'statistical  models',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Sql, Sas, Pandas',\n",
       "  'why': 'To improve efficiency and productivity',\n",
       "  'how': \"leveraging company's data\",\n",
       "  'whom': 'for a US based Fintech company',\n",
       "  'raw_bullet': \"Developed statistical  models using Sql, Sas and Pandas leveraging onleveraging company's data To improve efficiency and productivity for for a US based Fintech company\",\n",
       "  'corrected_bullet': \"Developed statistical models using SQL, SAS, and Pandas, leveraging the company's data to improve efficiency and productivity for a US-based Fintech company.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'dimensional  models',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Snowflake',\n",
       "  'why': 'To enable scalability and adaptability to changing business needs',\n",
       "  'how': '',\n",
       "  'whom': 'for a big retailer',\n",
       "  'raw_bullet': 'Developed dimensional  models using Python and Snowflake  To enable scalability and adaptability to changing business needs for for a big retailer',\n",
       "  'corrected_bullet': 'Developed dimensional models using Python and Snowflake to enable scalability and adaptability to changing business needs for a big retailer.'},\n",
       " {'user_id': '',\n",
       "  'what': 'dimensional  models',\n",
       "  'verb': 'Designed',\n",
       "  'skills': 'Python, Snowflake',\n",
       "  'why': 'To enable scalability and adaptability to changing business needs',\n",
       "  'how': '',\n",
       "  'whom': 'for a big international retailer',\n",
       "  'raw_bullet': 'Designed dimensional  models using Python and Snowflake  To enable scalability and adaptability to changing business needs for for a big international retailer',\n",
       "  'corrected_bullet': 'Designed dimensional models using Python and Snowflake to enable scalability and adaptability to changing business needs for a big international retailer.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipelines',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Airflow, Sql',\n",
       "  'why': 'To enable data-driven decision-making and business intelligence initiatives',\n",
       "  'how': 'Pipelines Development',\n",
       "  'whom': 'Digital Marketing Company',\n",
       "  'raw_bullet': 'Developed data pipelines using Airflow and Sql leveraging onPipelines Development To enable data-driven decision-making and business intelligence initiatives for Digital Marketing Company',\n",
       "  'corrected_bullet': 'Developed data pipelines using Airflow and SQL, leveraging on pipelines development to enable data-driven decision-making and business intelligence initiatives for a Digital Marketing Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data processing pipeline',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Airflow, Sql, Hive',\n",
       "  'why': 'To enable data-driven decision-making and business intelligence initiatives',\n",
       "  'how': 'Efficency analysis and developing',\n",
       "  'whom': 'Digital Marketing Company',\n",
       "  'raw_bullet': 'Developed data processing pipeline using Airflow, Sql and Hive leveraging onEfficency analysis and developing To enable data-driven decision-making and business intelligence initiatives for Digital Marketing Company',\n",
       "  'corrected_bullet': 'Developed a data processing pipeline using Airflow, SQL, and Hive, leveraging efficiency analysis and developing to enable data-driven decision-making and business intelligence initiatives for a Digital Marketing Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Teaching Courses',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To enable data-driven decision-making and business intelligence initiatives',\n",
       "  'how': 'Efficency analysis and developing',\n",
       "  'whom': 'Digital Marketing Company',\n",
       "  'raw_bullet': 'Developed Teaching Courses using  and undefined leveraging onEfficency analysis and developing To enable data-driven decision-making and business intelligence initiatives for Digital Marketing Company',\n",
       "  'corrected_bullet': 'Developed teaching courses using efficiency analysis and developing to enable data-driven decision-making and business intelligence initiatives for a digital marketing company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'training plan',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Databricks, Spark, SQL, Scrapy',\n",
       "  'why': 'To improve employee skills and knowledge',\n",
       "  'how': 'Slides, code dojos, market analysis, tests, certification pathways.',\n",
       "  'whom': 'Major insurance company in Canada and Consultant Company in USA and Brazil',\n",
       "  'raw_bullet': 'Created training plan using Databricks, Spark, SQL and Scrapy leveraging onSlides, code dojos, market analysis, tests, certification pathways. To improve employee skills and knowledge for Major insurance company in Canada and Consultant Company in USA and Brazil',\n",
       "  'corrected_bullet': 'I created a training plan using Databricks, Spark, SQL, and Scrapy, leveraging onSlides, code dojos, market analysis, tests, and certification pathways. This plan aimed to improve the skills and knowledge of employees for a major insurance company in Canada and a consultant company in the USA and Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'technical leadership',\n",
       "  'verb': 'Provided',\n",
       "  'skills': 'Databricks, Azure, AWS, Python, SQL, Shell',\n",
       "  'why': 'To improve employee skills and knowledge',\n",
       "  'how': 'Slides, code dojos, market analysis, tests, certification pathways.',\n",
       "  'whom': 'Major insurance company in Canada and Consultant Company in USA and Brazil',\n",
       "  'raw_bullet': 'Provided technical leadership using Databricks, Azure, AWS, Python, SQL and Shell leveraging onSlides, code dojos, market analysis, tests, certification pathways. To improve employee skills and knowledge for Major insurance company in Canada and Consultant Company in USA and Brazil',\n",
       "  'corrected_bullet': 'I provided technical leadership by utilizing Databricks, Azure, AWS, Python, SQL, and Shell. I leveraged onSlides, code dojos, market analysis, tests, and certification pathways to enhance employee skills and knowledge for a major insurance company in Canada and a consultant company in the USA and Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'tracking campaign tool',\n",
       "  'verb': 'Did',\n",
       "  'skills': 'Tableau',\n",
       "  'why': 'To make data-driven decisions and improve ROI',\n",
       "  'how': '',\n",
       "  'whom': 'Factored',\n",
       "  'raw_bullet': 'Did tracking campaign tool using Tableau  To make data-driven decisions and improve ROI for Factored',\n",
       "  'corrected_bullet': 'I tracked the campaign using Tableau to make data-driven decisions and improve ROI for Factored.'},\n",
       " {'user_id': '',\n",
       "  'what': 'tracking campaign tool',\n",
       "  'verb': 'Did',\n",
       "  'skills': 'Tableau',\n",
       "  'why': 'To make data-driven decisions and improve ROI',\n",
       "  'how': '',\n",
       "  'whom': 'Factored',\n",
       "  'raw_bullet': 'Did tracking campaign tool using Tableau  To make data-driven decisions and improve ROI for Factored',\n",
       "  'corrected_bullet': 'I tracked the campaign using Tableau to make data-driven decisions and improve ROI for Factored.'},\n",
       " {'user_id': '',\n",
       "  'what': 'tracking campaign tool',\n",
       "  'verb': 'Did',\n",
       "  'skills': 'Tableau',\n",
       "  'why': 'To make data-driven decisions and improve ROI',\n",
       "  'how': '',\n",
       "  'whom': 'Factored',\n",
       "  'raw_bullet': 'Did tracking campaign tool using Tableau  To make data-driven decisions and improve ROI for Factored',\n",
       "  'corrected_bullet': 'I tracked the campaign using Tableau to make data-driven decisions and improve ROI for Factored.'},\n",
       " {'user_id': '',\n",
       "  'what': 'tracking campaign tool',\n",
       "  'verb': 'Did',\n",
       "  'skills': 'Tableau',\n",
       "  'why': 'To make data-driven decisions and improve ROI',\n",
       "  'how': '',\n",
       "  'whom': 'Factored2',\n",
       "  'raw_bullet': 'Did tracking campaign tool using Tableau  To make data-driven decisions and improve ROI for Factored2',\n",
       "  'corrected_bullet': 'I tracked the campaign using Tableau to make data-driven decisions and improve ROI for Factored2.'},\n",
       " {'user_id': '',\n",
       "  'what': 'tracking campaign tool',\n",
       "  'verb': 'Did',\n",
       "  'skills': 'Tableau',\n",
       "  'why': 'To make data-driven decisions and improve ROI',\n",
       "  'how': '',\n",
       "  'whom': 'Factored2',\n",
       "  'raw_bullet': 'Did tracking campaign tool using Tableau  To make data-driven decisions and improve ROI for Factored2',\n",
       "  'corrected_bullet': 'Tracked the campaign using Tableau to make data-driven decisions and improve ROI for Factored2.'},\n",
       " {'user_id': '',\n",
       "  'what': 'machine learning model',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Tensorflow, Pytorch',\n",
       "  'why': 'To improve efficiency and accuracy in various tasks and processes',\n",
       "  'how': 'gei',\n",
       "  'whom': 'a fintech company',\n",
       "  'raw_bullet': 'Developed machine learning model using Tensorflow and Pytorch leveraging ongei To improve efficiency and accuracy in various tasks and processes for a fintech company',\n",
       "  'corrected_bullet': 'Developed a machine learning model using Tensorflow and Pytorch, leveraging ongei to improve efficiency and accuracy in various tasks and processes for a fintech company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl pipeline',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': 'Python, Airflow',\n",
       "  'why': 'To enhance scalability and performance by optimizing data processing and storage',\n",
       "  'how': 'a data warehouse',\n",
       "  'whom': 'a healthcare group',\n",
       "  'raw_bullet': 'Implemented etl pipeline using Python and Airflow leveraging ona data warehouse To enhance scalability and performance by optimizing data processing and storage for a healthcare group',\n",
       "  'corrected_bullet': 'Mplemented an ETL pipeline using Python and Airflow, leveraging a data warehouse to enhance scalability and performance by optimizing data processing and storage for a healthcare group.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl pipeline',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': 'Airflow, Python',\n",
       "  'why': 'To efficiently extract data from various sources',\n",
       "  'how': '',\n",
       "  'whom': 'a healthcare group',\n",
       "  'raw_bullet': 'Implemented etl pipeline using Airflow and Python  To efficiently extract data from various sources for a healthcare group',\n",
       "  'corrected_bullet': 'Implemented an ETL pipeline using Airflow and Python to efficiently extract data from various sources for a healthcare group.'},\n",
       " {'user_id': '',\n",
       "  'what': 'neural ml networks',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Tensorflow, Pytorch',\n",
       "  'why': 'To develop predictive models for forecasting and planning',\n",
       "  'how': '',\n",
       "  'whom': 'a healthcare group',\n",
       "  'raw_bullet': 'Built neural ml networks using Tensorflow and Pytorch  To develop predictive models for forecasting and planning for a healthcare group',\n",
       "  'corrected_bullet': 'Built neural ML networks using TensorFlow and PyTorch to develop predictive models for forecasting and planning for a healthcare group.'},\n",
       " {'user_id': '',\n",
       "  'what': 'big data solutions',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': 'Spark',\n",
       "  'why': 'To streamline and automate processes',\n",
       "  'how': '',\n",
       "  'whom': 'a retail',\n",
       "  'raw_bullet': 'Implemented big data solutions using Spark  To streamline and automate processes for a retail',\n",
       "  'corrected_bullet': 'Implemented big data solutions using Spark to streamline and automate processes for a retail.'},\n",
       " {'user_id': '',\n",
       "  'what': 'big data solutions',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': 'Sql, Python, Spark',\n",
       "  'why': 'To detect and prevent fraud or security breaches',\n",
       "  'how': '',\n",
       "  'whom': 'a company',\n",
       "  'raw_bullet': 'Implemented big data solutions using Sql, Python and Spark  To detect and prevent fraud or security breaches for a company',\n",
       "  'corrected_bullet': 'Implemented big data solutions using SQL, Python, and Spark to detect and prevent fraud or security breaches for a company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'computer vision',\n",
       "  'verb': 'Used',\n",
       "  'skills': 'Python',\n",
       "  'why': 'To enhance customer service and support through facial recognition and sentiment analysis',\n",
       "  'how': '',\n",
       "  'whom': 'an ml company',\n",
       "  'raw_bullet': 'Used computer vision using Python  To enhance customer service and support through facial recognition and sentiment analysis for an ml company',\n",
       "  'corrected_bullet': 'Used computer vision using Python to enhance customer service and support through facial recognition and sentiment analysis for an ML company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'optimization algorithm',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Excel, Data, Sql, Spark',\n",
       "  'why': 'To maximize profitability and financial performance',\n",
       "  'how': 'Lorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum',\n",
       "  'whom': 'Google',\n",
       "  'raw_bullet': 'Built optimization algorithm using Python, Excel, Data, Sql and Spark leveraging onLorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum To maximize profitability and financial performance for Google',\n",
       "  'corrected_bullet': 'Built an optimization algorithm using Python, Excel, Data, SQL, and Spark, leveraging on Lorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum to maximize profitability and financial performance for Google.'},\n",
       " {'user_id': '',\n",
       "  'what': 'optimization algorithm',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Excel, Data, Sql, Spark',\n",
       "  'why': 'To maximize profitability and financial performance',\n",
       "  'how': 'Lorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum',\n",
       "  'whom': 'Google',\n",
       "  'raw_bullet': 'Built optimization algorithm using Python, Excel, Data, Sql and Spark leveraging onLorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum To maximize profitability and financial performance for Google',\n",
       "  'corrected_bullet': 'Built an optimization algorithm using Python, Excel, Data, SQL, and Spark, leveraging on Lorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum to maximize profitability and financial performance for Google.'},\n",
       " {'user_id': '',\n",
       "  'what': 'optimization algorithm',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Excel, Data, Sql, Spark',\n",
       "  'why': 'To maximize profitability and financial performance',\n",
       "  'how': 'Lorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum',\n",
       "  'whom': 'Google',\n",
       "  'raw_bullet': 'Built optimization algorithm using Python, Excel, Data, Sql and Spark leveraging onLorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum To maximize profitability and financial performance for Google',\n",
       "  'corrected_bullet': 'Built an optimization algorithm using Python, Excel, Data, SQL, and Spark, leveraging Lorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum to maximize profitability and financial performance for Google.'},\n",
       " {'user_id': '',\n",
       "  'what': 'optimization algorithm',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Excel, Data, Sql, Spark',\n",
       "  'why': \"To automate repetitive tasks and free up employees' time for more valuable work\",\n",
       "  'how': 'Lorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum',\n",
       "  'whom': 'Google',\n",
       "  'raw_bullet': \"Built optimization algorithm using Python, Excel, Data, Sql and Spark leveraging onLorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum To automate repetitive tasks and free up employees' time for more valuable work for Google\",\n",
       "  'corrected_bullet': \"Built an optimization algorithm using Python, Excel, Data, SQL, and Spark, leveraging Lorem Ipsum Lorem Ipsum Lorem Ipsum Lorem Ipsum to automate repetitive tasks and free up employees' time for more valuable work at Google.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'etl data pipelines',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Sql, Airflow, Databricks',\n",
       "  'why': 'To enhance data governance and security by implementing data pipelines with proper controls and monitoring mechanisms',\n",
       "  'how': '',\n",
       "  'whom': 'Telco',\n",
       "  'raw_bullet': 'Developed etl data pipelines using Sql, Airflow and Databricks  To enhance data governance and security by implementing data pipelines with proper controls and monitoring mechanisms for Telco',\n",
       "  'corrected_bullet': 'Developed ETL data pipelines using SQL, Airflow, and Databricks to enhance data governance and security by implementing data pipelines with proper controls and monitoring mechanisms for Telco.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl data pipelines',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Sql, Airflow, Dataform',\n",
       "  'why': 'To support business intelligence and decision-making',\n",
       "  'how': 'writing the whole code',\n",
       "  'whom': 'Livup',\n",
       "  'raw_bullet': 'Built etl data pipelines using Sql, Airflow and Dataform leveraging onwriting the whole code To support business intelligence and decision-making for Livup',\n",
       "  'corrected_bullet': 'Built ETL data pipelines using SQL, Airflow, and Dataform, leveraging on writing the whole code to support business intelligence and decision-making for Livup.'},\n",
       " {'user_id': '',\n",
       "  'what': 'operational  dashboards',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Sql, Superset, BigQuery',\n",
       "  'why': 'To enhance transparency and accountability within the organization',\n",
       "  'how': '',\n",
       "  'whom': 'a music analytics company',\n",
       "  'raw_bullet': 'Developed operational  dashboards using Sql, Superset and BigQuery  To enhance transparency and accountability within the organization for a music analytics company',\n",
       "  'corrected_bullet': 'Operational dashboards were developed using SQL, Superset, and BigQuery to enhance transparency and accountability within the organization for a music analytics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Full data cycle process (ELT and Dashboard generation) for external benchmarking medical information',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'using Google Sheets, Big Query, Scheduled Queries, dbt and Looker Studio',\n",
       "  'why': 'To provide relevant insights to the CS team to establish service agreements with prospect clients (guaranteeing above-market averga E&M Levels per clinical note)',\n",
       "  'how': \"using Big Query Scheduled Queries for data ingestion, dbt's semantic layer for data transformations and generating an interactive EDA in a dashboard using Looker Studio\",\n",
       "  'whom': 'provided relevant insights to the CS team to establish service agreements with prospect clients (guaranteeing above-market averga E&M Levels per clinical note)',\n",
       "  'raw_bullet': \"Developed Full data cycle process (ELT and Dashboard generation) for external benchmarking medical information using using Google Sheets, Big Query, Scheduled Queries, dbt and Looker Studio leveraging onusing Big Query Scheduled Queries for data ingestion, dbt's semantic layer for data transformations and generating an interactive EDA in a dashboard using Looker Studio To provide relevant insights to the CS team to establish service agreements with prospect clients (guaranteeing above-market averga E&M Levels per clinical note) for provided relevant insights to the CS team to establish service agreements with prospect clients (guaranteeing above-market averga E&M Levels per clinical note)\",\n",
       "  'corrected_bullet': \"Developed the full data cycle process (ELT and dashboard generation) for external benchmarking medical information using Google Sheets, Big Query, Scheduled Queries, dbt, and Looker Studio. I leveraged Big Query Scheduled Queries for data ingestion, dbt's semantic layer for data transformations, and generated an interactive EDA in a dashboard using Looker Studio. This provided relevant insights to the CS team to establish service agreements with prospect clients, guaranteeing above-market average E&M Levels per clinical note.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'etl data pipelines',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Airflow, Databricks',\n",
       "  'why': 'To increase income',\n",
       "  'how': 'with the help of Mac',\n",
       "  'whom': 'Factored',\n",
       "  'raw_bullet': 'Developed etl data pipelines using Python, Airflow and Databricks leveraging onwith the help of Mac To increase income for Factored',\n",
       "  'corrected_bullet': 'Developed ETL data pipelines using Python, Airflow, and Databricks, leveraging the help of Mac to increase income for Factored.'},\n",
       " {'user_id': '',\n",
       "  'what': 'dashboards',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Tableau',\n",
       "  'why': 'analyze sales and profitability',\n",
       "  'how': '',\n",
       "  'whom': 'American office furniture, equipment and home furnishings producer ',\n",
       "  'raw_bullet': 'Developed dashboards using Tableau  analyze sales and profitability for American office furniture, equipment and home furnishings producer ',\n",
       "  'corrected_bullet': 'Developed dashboards using Tableau to analyze sales and profitability for an American office furniture, equipment, and home furnishings producer.'},\n",
       " {'user_id': '',\n",
       "  'what': 'dashboards',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Tableau',\n",
       "  'why': 'analyze sales and profitability',\n",
       "  'how': '',\n",
       "  'whom': 'American office furniture, equipment and home furnishings producer ',\n",
       "  'raw_bullet': 'Developed dashboards using Tableau  analyze sales and profitability for American office furniture, equipment and home furnishings producer ',\n",
       "  'corrected_bullet': 'Developed dashboards using Tableau to analyze sales and profitability for an American office furniture, equipment, and home furnishings producer.'},\n",
       " {'user_id': '',\n",
       "  'what': 'dashboards',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Tableau',\n",
       "  'why': 'To analyze sales and profitability',\n",
       "  'how': '',\n",
       "  'whom': 'American office furniture, equipment and home furnishings producer ',\n",
       "  'raw_bullet': 'Developed dashboards using Tableau  To analyze sales and profitability for American office furniture, equipment and home furnishings producer ',\n",
       "  'corrected_bullet': 'Developed dashboards using Tableau to analyze sales and profitability for an American office furniture, equipment, and home furnishings producer.'},\n",
       " {'user_id': '',\n",
       "  'what': 'customer segmentation model',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, sklearn',\n",
       "  'why': 'To tailor marketing strategies and campaigns for each segment',\n",
       "  'how': 'KMeans clustering',\n",
       "  'whom': 'Retail company',\n",
       "  'raw_bullet': 'Developed customer segmentation model using Python and sklearn leveraging onKMeans clustering To tailor marketing strategies and campaigns for each segment for Retail company',\n",
       "  'corrected_bullet': 'Developed a customer segmentation model using Python and sklearn, leveraging KMeans clustering, to tailor marketing strategies and campaigns for each segment for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'customer segmentation model',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, sklearn',\n",
       "  'why': 'To tailor marketing strategies and campaigns for each segment',\n",
       "  'how': 'K-means algorithm',\n",
       "  'whom': 'Retail company',\n",
       "  'raw_bullet': 'Developed customer segmentation model using Python and sklearn leveraging onK-means algorithm To tailor marketing strategies and campaigns for each segment for Retail company',\n",
       "  'corrected_bullet': 'Developed a customer segmentation model using Python and sklearn, leveraging the K-means algorithm, to tailor marketing strategies and campaigns for each segment for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'customer segmentation model',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, sklearn',\n",
       "  'why': 'To tailor marketing strategies and campaigns for each segment',\n",
       "  'how': 'K-means and decison tree classification algorithms',\n",
       "  'whom': 'Retail company',\n",
       "  'raw_bullet': 'Developed customer segmentation model using Python and sklearn leveraging onK-means and decison tree classification algorithms To tailor marketing strategies and campaigns for each segment for Retail company',\n",
       "  'corrected_bullet': 'Developed a customer segmentation model using Python and sklearn, leveraging K-means and decision tree classification algorithms, to tailor marketing strategies and campaigns for each segment for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'customer segmentation model',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, sklearn',\n",
       "  'why': 'To increase overall profitability and revenue by maximizing the effectiveness of marketing and sales efforts through targeted segmentation',\n",
       "  'how': 'K-means and decison tree classification algorithms',\n",
       "  'whom': 'Retail company',\n",
       "  'raw_bullet': 'Developed customer segmentation model using Python and sklearn leveraging onK-means and decison tree classification algorithms To increase overall profitability and revenue by maximizing the effectiveness of marketing and sales efforts through targeted segmentation for Retail company',\n",
       "  'corrected_bullet': 'Developed a customer segmentation model using Python and sklearn, leveraging K-means and decision tree classification algorithms, to increase overall profitability and revenue by maximizing the effectiveness of marketing and sales efforts through targeted segmentation for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'customer segmentation model',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, sklearn',\n",
       "  'why': 'To increase overall profitability and revenue by maximizing the effectiveness of marketing and sales efforts through targeted segmentation',\n",
       "  'how': 'K-means and decison tree classification algorithms',\n",
       "  'whom': 'A large US retailer',\n",
       "  'raw_bullet': 'Developed customer segmentation model using Python and sklearn leveraging onK-means and decison tree classification algorithms To increase overall profitability and revenue by maximizing the effectiveness of marketing and sales efforts through targeted segmentation for A large US retailer',\n",
       "  'corrected_bullet': 'Developed a customer segmentation model using Python and sklearn, leveraging K-means and decision tree classification algorithms, to increase overall profitability and revenue by maximizing the effectiveness of marketing and sales efforts through targeted segmentation for a large US retailer.'},\n",
       " {'user_id': '',\n",
       "  'what': 'market basket analysis',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python',\n",
       "  'why': 'To identify patterns and relationships between products',\n",
       "  'how': '',\n",
       "  'whom': 'A large US retailer',\n",
       "  'raw_bullet': 'Developed market basket analysis using Python  To identify patterns and relationships between products for A large US retailer',\n",
       "  'corrected_bullet': 'Developed market basket analysis using Python to identify patterns and relationships between products for a large US retailer.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt core model with a python logic to export to a dashboard leveraging google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt core model with a python logic to export to a dashboard leveraging google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt core model with a Python logic to export to a dashboard, and leveraging Google API to automate data processing tasks and reduce manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt core model with a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt core model with a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt core model with a Python logic to export to a dashboard using Google API. This automated data processing tasks and reduced manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt core model with a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt core model with a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt core model with a Python logic to export to a dashboard using Google API. This automated data processing tasks and reduced manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt core model with a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt core model with a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt core model with a Python logic to export to a dashboard using Google API. This automated data processing tasks and reduced manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt model and a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt model and a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt model and a Python logic to export to a dashboard using Google API. This was done to automate data processing tasks and reduce manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt model and a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt model and a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt model and a Python logic to export to a dashboard using Google API. This was done to automate data processing tasks and reduce manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt model and a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt model and a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt model and a Python logic to export to a dashboard using Google API. This was done to automate data processing tasks and reduce manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt model and a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt model and a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt model and a Python logic to export to a dashboard using Google API. This was done to automate data processing tasks and reduce manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt model and a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt model and a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt model and a Python logic to export to a dashboard using Google API. This was done to automate data processing tasks and reduce manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt model and a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt model and a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt model and a Python logic to export to a dashboard using Google API. This was done to automate data processing tasks and reduce manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt model and a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt model and a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt model and a Python logic to export to a dashboard using Google API. This was done to automate data processing tasks and reduce manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt model and a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt model and a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt model and a Python logic to export to a dashboard using Google API. This was done to automate data processing tasks and reduce manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt model and a python logic to export to a dashboard using google api',\n",
       "  'whom': 'Recruitment Team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt model and a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for Recruitment Team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt model and a Python logic to export to a dashboard using Google API. This was done to automate data processing tasks and reduce manual effort for the Recruitment Team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt core model with a python logic to export to a dashboard using google api',\n",
       "  'whom': 'recruitment team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt core model with a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for recruitment team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt core model with a Python logic to export to a dashboard using Google API. This automated data processing tasks and reduced manual effort for the recruitment team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'plate enhancement model',\n",
       "  'verb': 'Conducted',\n",
       "  'skills': 'Python, Tableau',\n",
       "  'why': 'To improve the overall performance and efficiency of the company',\n",
       "  'how': '',\n",
       "  'whom': 'a healthcare company',\n",
       "  'raw_bullet': 'Conducted plate enhancement model using Python and Tableau  To improve the overall performance and efficiency of the company for a healthcare company',\n",
       "  'corrected_bullet': 'Conducted a plate enhancement model using Python and Tableau to improve the overall performance and efficiency of a healthcare company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'creating a dbt core model with a python logic to export to a dashboard using google api',\n",
       "  'whom': 'recruitment team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging oncreating a dbt core model with a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for recruitment team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging on creating a dbt core model with a Python logic to export to a dashboard using Google API. This automated data processing tasks and reduced manual effort for the recruitment team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'dbt core model with a python logic to export to a dashboard using google api',\n",
       "  'whom': 'recruitment team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging ondbt core model with a python logic to export to a dashboard using google api To automate data processing tasks and reduce manual effort for recruitment team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging the ondbt core model with a Python logic to export to a dashboard using Google API. This automated data processing tasks and reduced manual effort for the recruitment team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Dbt, Python',\n",
       "  'why': 'To automate data processing tasks and reduce manual effort',\n",
       "  'how': 'dbt core model implementation and a python script to export to a dashboard using google api',\n",
       "  'whom': 'recruitment team',\n",
       "  'raw_bullet': 'Built data pipeline using Dbt and Python leveraging ondbt core model implementation and a python script to export to a dashboard using google api To automate data processing tasks and reduce manual effort for recruitment team',\n",
       "  'corrected_bullet': 'Built a data pipeline using Dbt and Python, leveraging the ondbt core model implementation and a Python script to export to a dashboard using Google API. This automated data processing tasks and reduced manual effort for the recruitment team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'periodic retraining of cross selling models',\n",
       "  'verb': 'do',\n",
       "  'skills': 'R',\n",
       "  'why': 'to have the most updated model',\n",
       "  'how': '',\n",
       "  'whom': 'a top insurance company from the US',\n",
       "  'raw_bullet': 'do periodic retraining of cross selling models using R  to have the most updated model for a top insurance company from the US',\n",
       "  'corrected_bullet': 'Periodic retraining of cross-selling models using R is done to ensure that the most updated model is available for a top insurance company from the US.'},\n",
       " {'user_id': '',\n",
       "  'what': 'analytical  processes',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'c',\n",
       "  'why': 'Creating a printf',\n",
       "  'how': 'No idea',\n",
       "  'whom': 'Nacho',\n",
       "  'raw_bullet': 'Developed analytical  processes using c leveraging onNo idea Creating a printf for Nacho',\n",
       "  'corrected_bullet': 'Developed analytical processes using C, leveraging on the idea of creating a printf for Nacho.'},\n",
       " {'user_id': '',\n",
       "  'what': 'analytical  processes',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'c',\n",
       "  'why': 'Creating a printf',\n",
       "  'how': 'Crying a lot',\n",
       "  'whom': 'Nacho',\n",
       "  'raw_bullet': 'Developed analytical  processes using c leveraging onCrying a lot Creating a printf for Nacho',\n",
       "  'corrected_bullet': 'Developed analytical processes using C, leveraging on creating a printf for Nacho.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Weekly demand forecast for more than 3.5k different items on division and store-level for 13 weeks into the future',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Databricks, Prophet, StatsForecast, PySpark, Snowflake, MLflow',\n",
       "  'why': 'To accurately predict and plan for future demand on promotional items',\n",
       "  'how': 'Statistical models like Exponential Smoothing (ETS), Complex ETS, Theta, and ML techniques like Prophet and model ensembles and evaluationg based on an OOS 4-folds cross-validation',\n",
       "  'whom': 'one of the biggest US retailers',\n",
       "  'raw_bullet': 'Developed Weekly demand forecast for more than 3.5k different items on division and store-level for 13 weeks into the future using Databricks, Prophet, StatsForecast, PySpark, Snowflake and MLflow leveraging onStatistical models like Exponential Smoothing (ETS), Complex ETS, Theta, and ML techniques like Prophet and model ensembles and evaluationg based on an OOS 4-folds cross-validation To accurately predict and plan for future demand on promotional items for one of the biggest US retailers',\n",
       "  'corrected_bullet': 'Developed a weekly demand forecast for over 3.5k different items at the division and store-level for 13 weeks into the future using Databricks, Prophet, StatsForecast, PySpark, Snowflake, and MLflow. I leveraged statistical models like Exponential Smoothing (ETS), Complex ETS, Theta, and ML techniques such as Prophet and model ensembles. I evaluated the forecast accuracy based on an out-of-sample 4-fold cross-validation. This allowed me to accurately predict and plan for future demand on promotional items for one of the largest US retailers.'},\n",
       " {'user_id': '',\n",
       "  'what': 'nlp frameworks',\n",
       "  'verb': 'Used',\n",
       "  'skills': 'Python',\n",
       "  'why': 'To improve natural language understanding and processing capabilities',\n",
       "  'how': 'logging user interactions',\n",
       "  'whom': 'fintech',\n",
       "  'raw_bullet': 'Used nlp frameworks using Python leveraging onlogging user interactions To improve natural language understanding and processing capabilities for fintech',\n",
       "  'corrected_bullet': 'Used NLP frameworks using Python, leveraging on logging user interactions, to improve natural language understanding and processing capabilities for fintech.'},\n",
       " {'user_id': '',\n",
       "  'what': 'kpi  reports',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Financial modeling',\n",
       "  'why': 'ESG Impact measurement ',\n",
       "  'how': 'Analysis of current situation, and future planning',\n",
       "  'whom': 'private clients',\n",
       "  'raw_bullet': 'Created kpi  reports using Financial modeling leveraging onAnalysis of current situation, and future planning ESG Impact measurement  for private clients',\n",
       "  'corrected_bullet': 'Created KPI reports using financial modeling, leveraging analysis of the current situation and future planning for ESG impact measurement for private clients.'},\n",
       " {'user_id': '',\n",
       "  'what': 'kpi  reports',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Financial modeling',\n",
       "  'why': 'ESG Impact measurement ',\n",
       "  'how': 'Analysis of current situation, and future planning',\n",
       "  'whom': 'private clients',\n",
       "  'raw_bullet': 'Created kpi  reports using Financial modeling leveraging onAnalysis of current situation, and future planning ESG Impact measurement  for private clients',\n",
       "  'corrected_bullet': 'Created KPI reports using financial modeling, leveraging analysis of the current situation and future planning for ESG impact measurement for private clients.'},\n",
       " {'user_id': '',\n",
       "  'what': 'kpi  reports',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Financial modeling',\n",
       "  'why': 'ESG Impact measurement ',\n",
       "  'how': 'Analysis of current situation, and future planning',\n",
       "  'whom': 'private clients',\n",
       "  'raw_bullet': 'Created kpi  reports using Financial modeling leveraging onAnalysis of current situation, and future planning ESG Impact measurement  for private clients',\n",
       "  'corrected_bullet': 'Created KPI reports using financial modeling, leveraging analysis of the current situation and future planning for ESG impact measurement for private clients.'},\n",
       " {'user_id': '',\n",
       "  'what': 'kpi  reports',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Financial modeling',\n",
       "  'why': 'ESG Impact measurement ',\n",
       "  'how': 'Analysis of current situation, and future planning',\n",
       "  'whom': 'private clients',\n",
       "  'raw_bullet': 'Created kpi  reports using Financial modeling leveraging onAnalysis of current situation, and future planning ESG Impact measurement  for private clients',\n",
       "  'corrected_bullet': 'Created KPI reports using financial modeling, leveraging analysis of the current situation and future planning for ESG impact measurement for private clients.'},\n",
       " {'user_id': '',\n",
       "  'what': 'constructed sustainability reports',\n",
       "  'verb': 'constructed',\n",
       "  'skills': 'redaction',\n",
       "  'why': 'for compliance',\n",
       "  'how': '',\n",
       "  'whom': 'private clients',\n",
       "  'raw_bullet': 'constructed constructed sustainability reports using redaction  for compliance for private clients',\n",
       "  'corrected_bullet': 'Constructed sustainability reports using redaction for compliance for private clients.'},\n",
       " {'user_id': '',\n",
       "  'what': 'constructed sustainability reports',\n",
       "  'verb': 'constructed',\n",
       "  'skills': 'redaction',\n",
       "  'why': 'for compliance',\n",
       "  'how': '',\n",
       "  'whom': 'private clients',\n",
       "  'raw_bullet': 'constructed constructed sustainability reports using redaction  for compliance for private clients',\n",
       "  'corrected_bullet': 'Constructed sustainability reports using redaction for compliance for private clients.'},\n",
       " {'user_id': '',\n",
       "  'what': 'constructed sustainability reports',\n",
       "  'verb': 'constructed',\n",
       "  'skills': 'redaction',\n",
       "  'why': 'for compliance',\n",
       "  'how': '',\n",
       "  'whom': 'private clients',\n",
       "  'raw_bullet': 'constructed constructed sustainability reports using redaction  for compliance for private clients',\n",
       "  'corrected_bullet': 'Constructed sustainability reports using redaction for compliance for private clients.'},\n",
       " {'user_id': '',\n",
       "  'what': 'constructed sustainability reports',\n",
       "  'verb': 'constructed',\n",
       "  'skills': 'redaction',\n",
       "  'why': 'compliance',\n",
       "  'how': '',\n",
       "  'whom': 'private clients',\n",
       "  'raw_bullet': 'constructed constructed sustainability reports using redaction  compliance for private clients',\n",
       "  'corrected_bullet': 'Constructed sustainability reports for private clients using redaction compliance.'},\n",
       " {'user_id': 'test-frontend',\n",
       "  'what': 'etl data pipelines',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Sql, Python, Airflow, Databricks, Spark',\n",
       "  'why': 'To enable real-time data access and enable timely decision-making in the healthcare industry',\n",
       "  'how': 'data quality techniques',\n",
       "  'whom': 'a healthcare company',\n",
       "  'raw_bullet': 'Developed etl data pipelines using Sql Python Airflow Databricks Spark leveraging on data quality techniques in order to To enable real-time data access and enable timely decision-making in the healthcare industry for a healthcare company',\n",
       "  'corrected_bullet': 'Developed ETL data pipelines using SQL, Python, Airflow, Databricks, and Spark, leveraging data quality techniques to enable real-time data access and facilitate timely decision-making in the healthcare industry for a healthcare company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Led a team of engineers in the creation of a document analysiss pileline',\n",
       "  'verb': 'led and developed',\n",
       "  'skills': 'python, paddle paddle, pytorch',\n",
       "  'why': 'standardize dvelopment of ML tools for document analysis',\n",
       "  'how': '',\n",
       "  'whom': 'Millenium BPO',\n",
       "  'raw_bullet': 'led and developed Led a team of engineers in the creation of a document analysiss pileline using python, paddle paddle, pytorch  standardize dvelopment of ML tools for document analysis for Millenium BPO',\n",
       "  'corrected_bullet': 'Led and developed a team of engineers in the creation of a document analysis pipeline using Python, PaddlePaddle, and PyTorch to standardize the development of machine learning tools for document analysis at Millennium BPO.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Led a team of engineers in the creation of a document analysiss framework',\n",
       "  'verb': 'led and developed',\n",
       "  'skills': 'python, paddle paddle, pytorch',\n",
       "  'why': 'standardize dvelopment of ML tools for document analysis',\n",
       "  'how': '',\n",
       "  'whom': 'Millenium BPO',\n",
       "  'raw_bullet': 'led and developed Led a team of engineers in the creation of a document analysiss framework using python, paddle paddle, pytorch  standardize dvelopment of ML tools for document analysis for Millenium BPO',\n",
       "  'corrected_bullet': 'Led and developed a team of engineers in the creation of a document analysis framework using Python, PaddlePaddle, and PyTorch to standardize the development of machine learning tools for document analysis at Millenium BPO.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Automated pipelines for document processing (IDP), primarily for customer requests, complaints, and claims',\n",
       "  'verb': 'Automated',\n",
       "  'skills': '',\n",
       "  'why': 'By automating the document processing pipelines, the company can reduce the need for additional staff to resolve more cases, , as a result, increasing agent productivity by 4% - 10%',\n",
       "  'how': '',\n",
       "  'whom': 'Utilities and telecommunications companies in colombia',\n",
       "  'raw_bullet': 'Automated Automated pipelines for document processing (IDP), primarily for customer requests, complaints, and claims using  and undefined  By automating the document processing pipelines, the company can reduce the need for additional staff to resolve more cases, , as a result, increasing agent productivity by 4% - 10% for Utilities and telecommunications companies in colombia',\n",
       "  'corrected_bullet': 'Automated pipelines for document processing (IDP) are primarily used for customer requests, complaints, and claims. By automating these pipelines, the company can reduce the need for additional staff to resolve more cases, resulting in an increase in agent productivity by 4% - 10% for utilities and telecommunications companies in Colombia.'},\n",
       " {'user_id': '',\n",
       "  'what': 'a range of innovative devices, including mobile robots, drones, and facial recognition devices ',\n",
       "  'verb': 'Designed ',\n",
       "  'skills': 'Robotics, Computer Vision, NLP',\n",
       "  'why': 'To explore new business opportunities and revenue streams in emerging markets',\n",
       "  'how': '',\n",
       "  'whom': 'hotel companies and the companies in the agricultural sector',\n",
       "  'raw_bullet': 'Designed  a range of innovative devices, including mobile robots, drones, and facial recognition devices  using Robotics, Computer Vision, NLP  To explore new business opportunities and revenue streams in emerging markets for hotel companies and the companies in the agricultural sector',\n",
       "  'corrected_bullet': 'Designed a range of innovative devices, including mobile robots, drones, and facial recognition devices, using Robotics, Computer Vision, and NLP. This was done to explore new business opportunities and revenue streams in emerging markets for hotel companies and companies in the agricultural sector.'},\n",
       " {'user_id': '',\n",
       "  'what': 'interaction systems for robotics',\n",
       "  'verb': 'developed',\n",
       "  'skills': 'CV, NLP',\n",
       "  'why': 'To enable seamless integration of robots into existing company processes',\n",
       "  'how': '',\n",
       "  'whom': 'hotel companies',\n",
       "  'raw_bullet': 'developed interaction systems for robotics using CV, NLP  To enable seamless integration of robots into existing company processes for hotel companies',\n",
       "  'corrected_bullet': 'Developed interaction systems for robotics using CV and NLP to enable seamless integration of robots into existing company processes for hotel companies.'},\n",
       " {'user_id': '',\n",
       "  'what': 'tenant chatbot product',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'langchain, mongodb',\n",
       "  'why': 'To personalize and tailor customer experiences',\n",
       "  'how': '',\n",
       "  'whom': 'entertainment company',\n",
       "  'raw_bullet': 'Built tenant chatbot product using langchain and mongodb  To personalize and tailor customer experiences for entertainment company',\n",
       "  'corrected_bullet': 'Built a tenant chatbot product using Langchain and MongoDB to personalize and tailor customer experiences for an entertainment company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl data pipelines',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Databricks',\n",
       "  'why': 'To extract, transform, and load (ETL) data pipelines are developed by companies for several reasons:',\n",
       "  'how': '',\n",
       "  'whom': 'for telcos ',\n",
       "  'raw_bullet': 'Developed etl data pipelines using Python and Databricks  To extract, transform, and load (ETL) data pipelines are developed by companies for several reasons: for for telcos ',\n",
       "  'corrected_bullet': 'Developed ETL data pipelines using Python and Databricks. Companies develop ETL data pipelines for several reasons, such as for telcos.'},\n",
       " {'user_id': '',\n",
       "  'what': 'utilization prediction model',\n",
       "  'verb': 'Improved credit distribution access to current customers to increase credit line',\n",
       "  'skills': 'python, bentoml',\n",
       "  'why': 'to improve line increasing process at alocation resources to the right customers who will use the credit card more and create bigger future income',\n",
       "  'how': 'LGBM classification model',\n",
       "  'whom': 'a fintech company',\n",
       "  'raw_bullet': 'Improved credit distribution access to current customers to increase credit line utilization prediction model using python, bentoml leveraging onLGBM classification model to improve line increasing process at alocation resources to the right customers who will use the credit card more and create bigger future income for a fintech company',\n",
       "  'corrected_bullet': 'Improved credit distribution access to current customers by using a Python-based bentoml to leverage the LGBM classification model. This enhancement aims to improve the prediction model for credit line utilization and streamline the process of increasing credit lines. By allocating resources to the right customers who have a higher likelihood of using the credit card more frequently, the fintech company can generate greater future income.'},\n",
       " {'user_id': '',\n",
       "  'what': 'analytical  processes',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Sql, Tableau, Python',\n",
       "  'why': 'To identify and target new market opportunities',\n",
       "  'how': 'by creating dashboards',\n",
       "  'whom': 'for an oil and gas company',\n",
       "  'raw_bullet': 'Developed analytical  processes using Sql, Tableau and Python leveraging onby creating dashboards To identify and target new market opportunities for for an oil and gas company',\n",
       "  'corrected_bullet': 'Developed analytical processes using SQL, Tableau, and Python to leverage the creation of dashboards in order to identify and target new market opportunities for an oil and gas company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl data pipelines',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Airflow, Databricks',\n",
       "  'why': 'To enable real-time or near-real-time data integration and analysis',\n",
       "  'how': '',\n",
       "  'whom': 'a fintech',\n",
       "  'raw_bullet': 'Developed etl data pipelines using Airflow and Databricks  To enable real-time or near-real-time data integration and analysis for a fintech',\n",
       "  'corrected_bullet': 'Developed ETL data pipelines using Airflow and Databricks to enable real-time or near-real-time data integration and analysis for a fintech.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis reports',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Tableau, Sql',\n",
       "  'why': 'To make informed business decisions based on data-driven insights',\n",
       "  'how': '',\n",
       "  'whom': 'a fintech',\n",
       "  'raw_bullet': 'Created data analysis reports using Tableau and Sql  To make informed business decisions based on data-driven insights for a fintech',\n",
       "  'corrected_bullet': 'Created data analysis reports using Tableau and SQL to make informed business decisions based on data-driven insights for a fintech.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis reports',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Tableau, Sql',\n",
       "  'why': 'To enable real time business decisions based on data-driven insights',\n",
       "  'how': '',\n",
       "  'whom': 'a fintech',\n",
       "  'raw_bullet': 'Created data analysis reports using Tableau and Sql  To enable real time business decisions based on data-driven insights for a fintech',\n",
       "  'corrected_bullet': 'Created data analysis reports using Tableau and SQL to enable real-time business decisions based on data-driven insights for a fintech.'},\n",
       " {'user_id': '',\n",
       "  'what': 'frontend',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Typescript, React',\n",
       "  'why': 'To optimize website performance and loading speed, ensuring a smooth browsing experience',\n",
       "  'how': '',\n",
       "  'whom': 'a benchmarking ngo',\n",
       "  'raw_bullet': 'Developed frontend using Typescript and React  To optimize website performance and loading speed, ensuring a smooth browsing experience for a benchmarking ngo',\n",
       "  'corrected_bullet': 'Developed the frontend using Typescript and React to optimize website performance and loading speed, ensuring a smooth browsing experience for a benchmarking NGO.'},\n",
       " {'user_id': '',\n",
       "  'what': 'backend application',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, FastAPI',\n",
       "  'why': 'To comply with industry standards and regulations',\n",
       "  'how': 'Endpoints following the single responsibility principle',\n",
       "  'whom': 'a benchmarking ngo',\n",
       "  'raw_bullet': 'Developed backend application using Python and FastAPI leveraging onEndpoints following the single responsibility principle To comply with industry standards and regulations for a benchmarking ngo',\n",
       "  'corrected_bullet': 'Developed a backend application using Python and FastAPI, leveraging onEndpoints and following the single responsibility principle to comply with industry standards and regulations for a benchmarking NGO.'},\n",
       " {'user_id': '',\n",
       "  'what': 'database orm',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Sql, Postgresql, SQLAlchemy',\n",
       "  'why': 'To improve data accessibility and retrieval',\n",
       "  'how': 'Models for each of the tables available in the database',\n",
       "  'whom': 'a benchmarking ngo',\n",
       "  'raw_bullet': 'Created database orm using Sql, Postgresql and SQLAlchemy leveraging onModels for each of the tables available in the database To improve data accessibility and retrieval for a benchmarking ngo',\n",
       "  'corrected_bullet': 'Created a database ORM using SQL, PostgreSQL, and SQLAlchemy, leveraging on models for each of the tables available in the database. This was done to improve data accessibility and retrieval for a benchmarking NGO.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Spark, Snowflake',\n",
       "  'why': 'To enable scalability and flexibility for future data growth and needs',\n",
       "  'how': 'migrate current codebase',\n",
       "  'whom': 'Ads Company',\n",
       "  'raw_bullet': 'Developed data pipeline using Spark and Snowflake leveraging onmigrate current codebase To enable scalability and flexibility for future data growth and needs for Ads Company',\n",
       "  'corrected_bullet': 'Developed a data pipeline using Spark and Snowflake to leverage and migrate the current codebase, enabling scalability and flexibility for future data growth and the needs of the Ads Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Monitoring Dashboard',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'dbt, Superset, BigQuery, Snowplow',\n",
       "  'why': 'To track operational metrics and monitor data pipelines performance, costs, storage and time spent, as well as forecasting amount of events generated by users each day',\n",
       "  'how': 'creating metrics based on dbt models and tests logging, BigQuery usage metadata, and real-time event tracking from iOS and android platforms',\n",
       "  'whom': 'a music data analytics company',\n",
       "  'raw_bullet': 'Developed Monitoring Dashboard using dbt, Superset, BigQuery and Snowplow leveraging oncreating metrics based on dbt models and tests logging, BigQuery usage metadata, and real-time event tracking from iOS and android platforms To track operational metrics and monitor data pipelines performance, costs, storage and time spent, as well as forecasting amount of events generated by users each day for a music data analytics company',\n",
       "  'corrected_bullet': 'Developed a Monitoring Dashboard using dbt, Superset, BigQuery, and Snowplow. This involved leveraging the creation of metrics based on dbt models and tests logging, BigQuery usage metadata, and real-time event tracking from iOS and Android platforms. The purpose was to track operational metrics and monitor the performance, costs, storage, and time spent on data pipelines. Additionally, the goal was to forecast the amount of events generated by users each day for a music data analytics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Trending content algorithm',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Redis, Python, Snowplow, BigQuery, Airflow',\n",
       "  'why': 'Provide daily trending content to users inside the application, discovering new content that is relevant at that moment ',\n",
       "  'how': '',\n",
       "  'whom': 'a music data analytics company',\n",
       "  'raw_bullet': 'Created Trending content algorithm using Redis, Python, Snowplow, BigQuery and Airflow  Provide daily trending content to users inside the application, discovering new content that is relevant at that moment  for a music data analytics company',\n",
       "  'corrected_bullet': 'Created a trending content algorithm using Redis, Python, Snowplow, BigQuery, and Airflow. This algorithm provided daily trending content to users inside the application, allowing them to discover new content that was relevant at that moment. This was done for a music data analytics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Trending content algorithm',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Redis, Python, Snowplow, BigQuery, Airflow',\n",
       "  'why': 'Provide daily trending content to users inside the application, discovering new content that is relevant at that moment ',\n",
       "  'how': '',\n",
       "  'whom': 'a music data analytics company',\n",
       "  'raw_bullet': 'Created Trending content algorithm using Redis, Python, Snowplow, BigQuery and Airflow  Provide daily trending content to users inside the application, discovering new content that is relevant at that moment  for a music data analytics company',\n",
       "  'corrected_bullet': 'Created a trending content algorithm using Redis, Python, Snowplow, BigQuery, and Airflow. This algorithm provided daily trending content to users inside the application, allowing them to discover new content that was relevant at that moment. This was done for a music data analytics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'machine learning apis',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Docker, GCS, Cloud Run',\n",
       "  'why': \"To identify feelings based on Song's lyrics\",\n",
       "  'how': 'using Huggingface models on FastAPI ',\n",
       "  'whom': 'a music data analytics company',\n",
       "  'raw_bullet': \"Built machine learning apis using Python, Docker, GCS and Cloud Run leveraging onusing Huggingface models on FastAPI  To identify feelings based on Song's lyrics for a music data analytics company\",\n",
       "  'corrected_bullet': 'Built machine learning APIs using Python, Docker, GCS, and Cloud Run, leveraging Huggingface models on FastAPI to identify feelings based on song lyrics for a music data analytics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'product data dashboards',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Snowplow, BigQuery, Superset, dbt',\n",
       "  'why': \"To discover user's behaviour on search, idenfiying the most searched content, CTRs and Search attempts\",\n",
       "  'how': 'doing real-time event tracking with Snowplow pipelines',\n",
       "  'whom': 'a music data analytics company',\n",
       "  'raw_bullet': \"Developed product data dashboards using Snowplow, BigQuery, Superset and dbt leveraging ondoing real-time event tracking with Snowplow pipelines To discover user's behaviour on search, idenfiying the most searched content, CTRs and Search attempts for a music data analytics company\",\n",
       "  'corrected_bullet': \"Developed product data dashboards using Snowplow, BigQuery, Superset, and dbt, leveraging ongoing real-time event tracking with Snowplow pipelines to discover users' behavior on search. Identified the most searched content, CTRs, and search attempts for a music data analytics company.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'etl solutions',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': 'Sql, Python, Snowflake',\n",
       "  'why': 'To enhance business intelligence and reporting capabilities',\n",
       "  'how': '',\n",
       "  'whom': 'for a logistics company',\n",
       "  'raw_bullet': 'Implemented etl solutions using Sql, Python and Snowflake  To enhance business intelligence and reporting capabilities for for a logistics company',\n",
       "  'corrected_bullet': 'Implemented ETL solutions using SQL, Python, and Snowflake to enhance business intelligence and reporting capabilities for a logistics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'streaming data pipeline',\n",
       "  'verb': 'developed',\n",
       "  'skills': 'Kubernetes, Kafka, Flink, ksqlDB, Databricks',\n",
       "  'why': 'to generate real-time geolocated recommendations',\n",
       "  'how': 'serverless producer, medallion architecture',\n",
       "  'whom': 'tourist mobile application',\n",
       "  'raw_bullet': 'developed streaming data pipeline using Kubernetes, Kafka, Flink, ksqlDB, Databricks leveraging onserverless producer, medallion architecture to generate real-time geolocated recommendations for tourist mobile application',\n",
       "  'corrected_bullet': 'Developed a streaming data pipeline using Kubernetes, Kafka, Flink, ksqlDB, and Databricks. I leveraged onserverless producer and medallion architecture to generate real-time geolocated recommendations for a tourist mobile application.'},\n",
       " {'user_id': '',\n",
       "  'what': 'statistical  models',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Python, Sql, Spark, Pandas',\n",
       "  'why': 'To make predictions and forecasts based on historical data',\n",
       "  'how': 'Three phase model to p',\n",
       "  'whom': 'BBVA',\n",
       "  'raw_bullet': 'Created statistical  models using Python, Sql, Spark and Pandas leveraging onThree phase model to p To make predictions and forecasts based on historical data for BBVA',\n",
       "  'corrected_bullet': 'Created statistical models using Python, SQL, Spark, and Pandas, leveraging the Three-Phase Model to make predictions and forecasts based on historical data for BBVA.'},\n",
       " {'user_id': '',\n",
       "  'what': 'machine learning apis',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Sql, Pandas, Aws, pytorch, sklearn',\n",
       "  'why': 'To streamline and optimize business operations',\n",
       "  'how': '',\n",
       "  'whom': 'Kuona',\n",
       "  'raw_bullet': 'Built machine learning apis using Python, Sql, Pandas, Aws, pytorch and sklearn  To streamline and optimize business operations for Kuona',\n",
       "  'corrected_bullet': 'Built machine learning APIs using Python, SQL, Pandas, AWS, PyTorch, and sklearn to streamline and optimize business operations for Kuona.'},\n",
       " {'user_id': '',\n",
       "  'what': 'functional  tests',\n",
       "  'verb': 'Wrote',\n",
       "  'skills': 'Python, pytorch, sklearn',\n",
       "  'why': 'To streamline and optimize business operations',\n",
       "  'how': '',\n",
       "  'whom': 'Kuona',\n",
       "  'raw_bullet': 'Wrote functional  tests using Python, pytorch and sklearn  To streamline and optimize business operations for Kuona',\n",
       "  'corrected_bullet': 'Wrote functional tests using Python, PyTorch, and sklearn to streamline and optimize business operations for Kuona.'},\n",
       " {'user_id': '',\n",
       "  'what': 'image generation',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Python, pytorch, weights & biases, elastic search',\n",
       "  'why': 'To increase brand awareness and visibility',\n",
       "  'how': 'Implementing the state of the art(on that time) text-to-image generation diffusion model GLIDE',\n",
       "  'whom': 'Suggestic',\n",
       "  'raw_bullet': 'Created image generation using Python, pytorch, weights & biases and elastic search leveraging onImplementing the state of the art(on that time) text-to-image generation diffusion model GLIDE To increase brand awareness and visibility for Suggestic',\n",
       "  'corrected_bullet': 'Created image generation using Python, PyTorch, Weights & Biases, and Elastic Search by leveraging the implementation of the state-of-the-art text-to-image generation diffusion model GLIDE at that time. This was done to increase brand awareness and visibility for Suggestic.'},\n",
       " {'user_id': '',\n",
       "  'what': 'fraud botnet detector',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'mongodb, python, text classification',\n",
       "  'why': 'To detect and mitigate any fraudulent activities conducted through botnets',\n",
       "  'how': '',\n",
       "  'whom': 'Suggestic',\n",
       "  'raw_bullet': 'Created fraud botnet detector using mongodb, python and text classification  To detect and mitigate any fraudulent activities conducted through botnets for Suggestic',\n",
       "  'corrected_bullet': 'Created a fraud botnet detector using MongoDB, Python, and text classification to detect and mitigate any fraudulent activities conducted through botnets for Suggestic.'},\n",
       " {'user_id': '',\n",
       "  'what': 'I built',\n",
       "  'verb': 'I',\n",
       "  'skills': 'I',\n",
       "  'why': 'I',\n",
       "  'how': 'I',\n",
       "  'whom': 'I',\n",
       "  'raw_bullet': 'I I built using I leveraging onI I for I',\n",
       "  'corrected_bullet': 'Built using leverage on myself for success.'},\n",
       " {'user_id': '',\n",
       "  'what': 'i',\n",
       "  'verb': 'dsad',\n",
       "  'skills': 'fsdf',\n",
       "  'why': 'fds',\n",
       "  'how': 'f',\n",
       "  'whom': 'asdfdsf',\n",
       "  'raw_bullet': 'dsad i using fsdf leveraging onf fds for asdfdsf',\n",
       "  'corrected_bullet': 'Leveraged onf fds for asdfdsf while dsad i using fsdf.'},\n",
       " {'user_id': '',\n",
       "  'what': 'i built',\n",
       "  'verb': 'dsad',\n",
       "  'skills': 'fsdf',\n",
       "  'why': 'fds',\n",
       "  'how': 'f',\n",
       "  'whom': 'asdfdsf',\n",
       "  'raw_bullet': 'dsad i built using fsdf leveraging onf fds for asdfdsf',\n",
       "  'corrected_bullet': 'Built using fsdf, leveraging onf fds for asdfdsf.'},\n",
       " {'user_id': '',\n",
       "  'what': 'I ',\n",
       "  'verb': 'dsad',\n",
       "  'skills': 'fsdf',\n",
       "  'why': 'fds',\n",
       "  'how': 'f',\n",
       "  'whom': 'asdfdsf',\n",
       "  'raw_bullet': 'dsad I  using fsdf leveraging onf fds for asdfdsf',\n",
       "  'corrected_bullet': 'Used leveraging on fds for asdfdsf.'},\n",
       " {'user_id': '',\n",
       "  'what': 'I ',\n",
       "  'verb': 'I ',\n",
       "  'skills': 'AD',\n",
       "  'why': 'fds',\n",
       "  'how': 'f',\n",
       "  'whom': 'I ',\n",
       "  'raw_bullet': 'I  I  using AD leveraging onf fds for I ',\n",
       "  'corrected_bullet': 'Am using AD, leveraging on fds for myself.'},\n",
       " {'user_id': '',\n",
       "  'what': 'I ',\n",
       "  'verb': 'I ',\n",
       "  'skills': 'AD',\n",
       "  'why': 'fds',\n",
       "  'how': '',\n",
       "  'whom': 'I ',\n",
       "  'raw_bullet': 'I  I  using AD  fds for I ',\n",
       "  'corrected_bullet': 'Am using AD fds for I.'},\n",
       " {'user_id': '',\n",
       "  'what': 'I ',\n",
       "  'verb': 'I ',\n",
       "  'skills': 'AD',\n",
       "  'why': 'fds',\n",
       "  'how': '',\n",
       "  'whom': 'I ',\n",
       "  'raw_bullet': 'I  I  using AD  fds for I ',\n",
       "  'corrected_bullet': 'Am using AD fds for I.'},\n",
       " {'user_id': '',\n",
       "  'what': 'I I ',\n",
       "  'verb': 'I I ',\n",
       "  'skills': 'AD',\n",
       "  'why': 'I I ',\n",
       "  'how': '',\n",
       "  'whom': 'I I ',\n",
       "  'raw_bullet': 'I I  I I  using AD  I I  for I I ',\n",
       "  'corrected_bullet': 'Am using AD for I.'},\n",
       " {'user_id': '',\n",
       "  'what': 'machine learning challenge',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Pytorch, HuggingFace, FastAPI',\n",
       "  'why': 'to optimize language model training given a fixed data budget ',\n",
       "  'how': 'a system to evaluate submissions on a set of downstream NLP tasks used commonly to evaluate Large Language Models (LLMs)',\n",
       "  'whom': 'a non-profit organization',\n",
       "  'raw_bullet': 'Developed machine learning challenge using Pytorch, HuggingFace and FastAPI leveraging ona system to evaluate submissions on a set of downstream NLP tasks used commonly to evaluate Large Language Models (LLMs) to optimize language model training given a fixed data budget  for a non-profit organization',\n",
       "  'corrected_bullet': 'Developed a machine learning challenge using PyTorch, HuggingFace, and FastAPI. The challenge leveraged a system to evaluate submissions on a set of downstream NLP tasks commonly used to evaluate Large Language Models (LLMs). The goal was to optimize language model training given a fixed data budget for a non-profit organization.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data visualization framework',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Tableau, Sql, Python',\n",
       "  'why': 'To enhance communication and collaboration among different teams and departments within the company',\n",
       "  'how': 'By building an ETL ',\n",
       "  'whom': 'for an oil & gas company',\n",
       "  'raw_bullet': 'Created data visualization framework using Tableau, Sql and Python leveraging onBy building an ETL  To enhance communication and collaboration among different teams and departments within the company for for an oil & gas company',\n",
       "  'corrected_bullet': 'Created a data visualization framework using Tableau, SQL, and Python by building an ETL to enhance communication and collaboration among different teams and departments within the company for an oil & gas company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'business intelligence solutions',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': 'Tableau, Powerbi',\n",
       "  'why': 'To optimize resource allocation and budgeting',\n",
       "  'how': 'by creating an ETL',\n",
       "  'whom': 'for a fintech',\n",
       "  'raw_bullet': 'Implemented business intelligence solutions using Tableau and Powerbi leveraging onby creating an ETL To optimize resource allocation and budgeting for for a fintech',\n",
       "  'corrected_bullet': 'Implemented business intelligence solutions using Tableau and PowerBI, leveraging on creating an ETL to optimize resource allocation and budgeting for a fintech.'},\n",
       " {'user_id': '',\n",
       "  'what': 'pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Terraform, Airflow, Python, Jenkins, Dbt',\n",
       "  'why': 'To expand business operations and reach new markets',\n",
       "  'how': '',\n",
       "  'whom': 'Fintech',\n",
       "  'raw_bullet': 'Built pipeline using Terraform, Airflow, Python, Jenkins and Dbt  To expand business operations and reach new markets for Fintech',\n",
       "  'corrected_bullet': 'Built a pipeline using Terraform, Airflow, Python, Jenkins, and Dbt to expand business operations and reach new markets for Fintech.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': 'Airflow, Snowflake',\n",
       "  'why': 'To automate data ingestion and transformation processes, reducing manual effort and errors',\n",
       "  'how': '',\n",
       "  'whom': 'US Fintech company',\n",
       "  'raw_bullet': 'Implemented data pipeline using Airflow and Snowflake  To automate data ingestion and transformation processes, reducing manual effort and errors for US Fintech company',\n",
       "  'corrected_bullet': 'Implemented data pipeline using Airflow and Snowflake to automate data ingestion and transformation processes, reducing manual effort and errors for a US Fintech company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': 'Airflow, Snowflake',\n",
       "  'why': 'To automate data ingestion and transformation processes, reducing manual effort and errors',\n",
       "  'how': 'Using configuration driven approach',\n",
       "  'whom': 'US Fintech company',\n",
       "  'raw_bullet': 'Implemented data pipeline using Airflow and Snowflake leveraging onUsing configuration driven approach To automate data ingestion and transformation processes, reducing manual effort and errors for US Fintech company',\n",
       "  'corrected_bullet': 'Implemented data pipeline using Airflow and Snowflake, leveraging a configuration-driven approach to automate data ingestion and transformation processes, reducing manual effort and errors for a US Fintech company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data observability framework',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python',\n",
       "  'why': 'To provide visibility and insights into data being ingested from various sources (SFTP, Postgres, Snowflake, S3, etc). Allowing users to spot errors in vendor data earlier than usual.',\n",
       "  'how': '',\n",
       "  'whom': 'US Fintech company',\n",
       "  'raw_bullet': 'Developed data observability framework using Python  To provide visibility and insights into data being ingested from various sources (SFTP, Postgres, Snowflake, S3, etc). Allowing users to spot errors in vendor data earlier than usual. for US Fintech company',\n",
       "  'corrected_bullet': 'Developed a data observability framework using Python to provide visibility and insights into data that was being ingested from various sources (SFTP, Postgres, Snowflake, S3, etc). This allowed users to spot errors in vendor data earlier than usual for a US Fintech company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Scraped multiple websites',\n",
       "  'verb': 'Scraped',\n",
       "  'skills': 'Python, Beautiful Soup, Requests',\n",
       "  'why': 'To gather data from different sources in real time and access alternative data for pricing and company power',\n",
       "  'how': 'Using only lightweight packages Beautiful Soup and Requests',\n",
       "  'whom': 'Investement Management Company',\n",
       "  'raw_bullet': 'Scraped Scraped multiple websites using Python, Beautiful Soup, Requests leveraging onUsing only lightweight packages Beautiful Soup and Requests To gather data from different sources in real time and access alternative data for pricing and company power for Investement Management Company',\n",
       "  'corrected_bullet': 'Scraped multiple websites using Python, Beautiful Soup, and Requests, leveraging only lightweight packages Beautiful Soup and Requests to gather data from different sources in real time and access alternative data for pricing and company power for an Investment Management Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Fast, light weight and recurrent scrapes across multiple websites (>20)',\n",
       "  'verb': 'Scraped, Deployed and Maintain',\n",
       "  'skills': 'Python, Beautiful Soup, Requests',\n",
       "  'why': 'To gather data from different sources in real time and access alternative data for pricing and company power',\n",
       "  'how': 'Using only lightweight packages Beautiful Soup and Requests',\n",
       "  'whom': 'Investement Management Company',\n",
       "  'raw_bullet': 'Scraped, Deployed and Maintain Fast, light weight and recurrent scrapes across multiple websites (>20) using Python, Beautiful Soup, Requests leveraging onUsing only lightweight packages Beautiful Soup and Requests To gather data from different sources in real time and access alternative data for pricing and company power for Investement Management Company',\n",
       "  'corrected_bullet': 'Scraped, deployed, and maintained fast, lightweight, and recurrent scrapes across multiple websites (>20) using Python, Beautiful Soup, and Requests. Leveraged only lightweight packages Beautiful Soup and Requests to gather data from different sources in real time and access alternative data for pricing and company power for an Investment Management Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Fast, light weight and recurrent scrapes across multiple websites',\n",
       "  'verb': 'Scraped, Deployed and Maintain',\n",
       "  'skills': 'Python, Beautiful Soup, Requests',\n",
       "  'why': 'To gather data from different sources in real time and access alternative data for pricing and company power',\n",
       "  'how': 'Using only lightweight packages Beautiful Soup and Requests',\n",
       "  'whom': 'Investement Management Company',\n",
       "  'raw_bullet': 'Scraped, Deployed and Maintain Fast, light weight and recurrent scrapes across multiple websites using Python, Beautiful Soup, Requests leveraging onUsing only lightweight packages Beautiful Soup and Requests To gather data from different sources in real time and access alternative data for pricing and company power for Investement Management Company',\n",
       "  'corrected_bullet': 'Scraped, deployed, and maintained fast, lightweight, and recurrent scrapes across multiple websites using Python, Beautiful Soup, and Requests. Leveraged only lightweight packages Beautiful Soup and Requests to gather data from different sources in real time and access alternative data for pricing and company power for an Investment Management Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Fast, lightweight and recurrent scrapes across multiple websites',\n",
       "  'verb': 'Scraped, Deployed and Maintain',\n",
       "  'skills': 'Python, Beautiful Soup, Requests',\n",
       "  'why': 'To gather data from different sources and obtain alternative data to support business decision',\n",
       "  'how': 'Using only lightweight libraries to optimize speed',\n",
       "  'whom': 'an Investement Management Company',\n",
       "  'raw_bullet': 'Scraped, Deployed and Maintain Fast, lightweight and recurrent scrapes across multiple websites using Python, Beautiful Soup and Requests leveraging onUsing only lightweight libraries to optimize speed To gather data from different sources and obtain alternative data to support business decision for an Investement Management Company',\n",
       "  'corrected_bullet': 'Scraped, deployed, and maintained fast, lightweight, and recurrent scrapes across multiple websites using Python, Beautiful Soup, and Requests. Leveraged only lightweight libraries to optimize speed in gathering data from different sources and obtaining alternative data to support business decisions for an Investment Management Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'ETL processes which included ingestion, transformation, quality validation and lineage',\n",
       "  'verb': 'Designed, Build and Deployed',\n",
       "  'skills': 'Python, Airflow, Snowflake, Great Expectations, AWS Batch',\n",
       "  'why': 'To ingest multiple datasources from a different set of vendors',\n",
       "  'how': '',\n",
       "  'whom': 'an Investment Management Company',\n",
       "  'raw_bullet': 'Designed, Build and Deployed ETL processes which included ingestion, transformation, quality validation and lineage using Python, Airflow, Snowflake, Great Expectations and AWS Batch  To ingest multiple datasources from a different set of vendors for an Investment Management Company',\n",
       "  'corrected_bullet': 'Designed, built, and deployed ETL processes that included ingestion, transformation, quality validation, and lineage using Python, Airflow, Snowflake, Great Expectations, and AWS Batch to ingest multiple data sources from a different set of vendors for an Investment Management Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Python Package for data quality based on Great Expectations',\n",
       "  'verb': 'Designed and Create',\n",
       "  'skills': 'Python, Great Expectations',\n",
       "  'why': 'build a package usable on our platform and removing a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas)',\n",
       "  'how': '',\n",
       "  'whom': 'an Investment Management Company',\n",
       "  'raw_bullet': 'Designed and Create Python Package for data quality based on Great Expectations using Python and Great Expectations  build a package usable on our platform and removing a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas) for an Investment Management Company',\n",
       "  'corrected_bullet': 'Designed and created a Python package for data quality based on Great Expectations using Python and Great Expectations. The package is usable on our platform and removes a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark, and Pandas) for an Investment Management Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Python Package for data quality validation based on Great Expectations',\n",
       "  'verb': 'Designed and Create',\n",
       "  'skills': 'Python, Great Expectations',\n",
       "  'why': 'build a package usable on our platform and removing a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas)',\n",
       "  'how': '',\n",
       "  'whom': 'an Investment Management Company',\n",
       "  'raw_bullet': 'Designed and Create Python Package for data quality validation based on Great Expectations using Python and Great Expectations  build a package usable on our platform and removing a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas) for an Investment Management Company',\n",
       "  'corrected_bullet': 'Designed and created a Python package for data quality validation based on Great Expectations using Python and Great Expectations. The package is usable on our platform and removes a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark, and Pandas) for an Investment Management Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Python Package for data quality validation based on Great Expectations',\n",
       "  'verb': 'Designed and Create',\n",
       "  'skills': 'Python, SQLAlchemy',\n",
       "  'why': 'build a package usable on our platform and removing a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas)',\n",
       "  'how': '',\n",
       "  'whom': 'an Investment Management Company',\n",
       "  'raw_bullet': 'Designed and Create Python Package for data quality validation based on Great Expectations using Python and SQLAlchemy  build a package usable on our platform and removing a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas) for an Investment Management Company',\n",
       "  'corrected_bullet': 'Designed and created a Python package for data quality validation based on Great Expectations using Python and SQLAlchemy. The package was built to be usable on our platform and to remove a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark, and Pandas) for an Investment Management Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Python Package for data quality validation based on Great Expectations',\n",
       "  'verb': 'Designed and Create',\n",
       "  'skills': 'Python, SQLAlchemy, PySpark',\n",
       "  'why': 'build a package usable on our platform and removing a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas)',\n",
       "  'how': '',\n",
       "  'whom': 'an Investment Management Company',\n",
       "  'raw_bullet': 'Designed and Create Python Package for data quality validation based on Great Expectations using Python, SQLAlchemy and PySpark  build a package usable on our platform and removing a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas) for an Investment Management Company',\n",
       "  'corrected_bullet': 'Designed and created a Python package for data quality validation based on Great Expectations using Python, SQLAlchemy, and PySpark. The package was built to be usable on our platform and to remove a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark, and Pandas) for an Investment Management Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Python Package for data quality validation based on Great Expectations',\n",
       "  'verb': 'Designed and Create',\n",
       "  'skills': 'Python, SQLAlchemy, PySpark',\n",
       "  'why': 'build a package aim to be used on our platform and removing a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas)',\n",
       "  'how': '',\n",
       "  'whom': 'an Investment Management Company',\n",
       "  'raw_bullet': 'Designed and Create Python Package for data quality validation based on Great Expectations using Python, SQLAlchemy and PySpark  build a package aim to be used on our platform and removing a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas) for an Investment Management Company',\n",
       "  'corrected_bullet': 'Designed and created a Python package for data quality validation based on Great Expectations using Python, SQLAlchemy, and PySpark. The package aims to be used on our platform and removes a lot of the complexity from Great Expectations for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark, and Pandas) for an Investment Management Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Python Package for data quality validation based on Great Expectations',\n",
       "  'verb': 'Designed and Create',\n",
       "  'skills': 'Python, SQLAlchemy, PySpark',\n",
       "  'why': 'build a package aim to be used on our platform for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas) and removing a lot of the complexity from Great Expectations',\n",
       "  'how': '',\n",
       "  'whom': 'an Investment Management Company',\n",
       "  'raw_bullet': 'Designed and Create Python Package for data quality validation based on Great Expectations using Python, SQLAlchemy and PySpark  build a package aim to be used on our platform for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark and Pandas) and removing a lot of the complexity from Great Expectations for an Investment Management Company',\n",
       "  'corrected_bullet': 'Designed and created a Python package for data quality validation based on Great Expectations using Python, SQLAlchemy, and PySpark. The package aims to be used on our platform for all of our databases and engines (Snowflake, PostgreSQL, Oracle, Spark, and Pandas), removing a lot of the complexity from Great Expectations for an Investment Management Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Alternative Data provider vendors',\n",
       "  'verb': 'Analized, Scoped and Compared',\n",
       "  'skills': 'Python, Snowflake, Plotly',\n",
       "  'why': \"To pick a vendor's alternative data for fundamental investment portfolio analysis\",\n",
       "  'how': 'Using backtesting, time series analysis, cohort analysis, bias exploration and regex',\n",
       "  'whom': 'an Investment Management Company',\n",
       "  'raw_bullet': \"Analized, Scoped and Compared Alternative Data provider vendors using Python, Snowflake and Plotly leveraging onUsing backtesting, time series analysis, cohort analysis, bias exploration and regex To pick a vendor's alternative data for fundamental investment portfolio analysis for an Investment Management Company\",\n",
       "  'corrected_bullet': \"Analyzed, scoped, and compared alternative data provider vendors using Python, Snowflake, and Plotly, leveraging backtesting, time series analysis, cohort analysis, bias exploration, and regex to select a vendor's alternative data for fundamental investment portfolio analysis for an Investment Management Company.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'Alternative Data provider vendors',\n",
       "  'verb': 'Analized, Scoped and Compared',\n",
       "  'skills': 'Python, Snowflake, Plotly',\n",
       "  'why': \"To pick or not the best vendor's alternative data for fundamental investment portfolio analysis\",\n",
       "  'how': 'Using backtesting, time series analysis, cohort analysis, bias exploration and regex',\n",
       "  'whom': 'an Investment Management Company',\n",
       "  'raw_bullet': \"Analized, Scoped and Compared Alternative Data provider vendors using Python, Snowflake and Plotly leveraging onUsing backtesting, time series analysis, cohort analysis, bias exploration and regex To pick or not the best vendor's alternative data for fundamental investment portfolio analysis for an Investment Management Company\",\n",
       "  'corrected_bullet': \"Analyzed, scoped, and compared alternative data provider vendors using Python, Snowflake, and Plotly, leveraging backtesting, time series analysis, cohort analysis, bias exploration, and regex to determine the best vendor's alternative data for fundamental investment portfolio analysis for an Investment Management Company.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'etl solutions',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': 'Sql, Python, Spark',\n",
       "  'why': 'To load the transformed data into a target database or data warehouse',\n",
       "  'how': \"by extracting info from company's sources\",\n",
       "  'whom': 'for an oil & gas company',\n",
       "  'raw_bullet': \"Implemented etl solutions using Sql, Python and Spark leveraging onby extracting info from company's sources To load the transformed data into a target database or data warehouse for for an oil & gas company\",\n",
       "  'corrected_bullet': \"Implemented ETL solutions using SQL, Python, and Spark to extract information from the company's sources. Loaded the transformed data into a target database or data warehouse for an oil & gas company.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'recommendation model ',\n",
       "  'verb': 'develop',\n",
       "  'skills': 'pytorch, python, embeddings',\n",
       "  'why': 'To provide similar artists recommendations for a given artist ID ',\n",
       "  'how': 'At first, the artist data is used to generate the features for the model. Then, the Instructor Model, an instruction-finetuned text embedding model is used to generate the artist embeddings. Later, this embeddings will populate a vectorial database. Finally, we will get the most similar artist for a given artist by doing vector search on the database generated.',\n",
       "  'whom': 'music streaming platform ',\n",
       "  'raw_bullet': 'develop recommendation model  using pytorch, python, embeddings leveraging onAt first, the artist data is used to generate the features for the model. Then, the Instructor Model, an instruction-finetuned text embedding model is used to generate the artist embeddings. Later, this embeddings will populate a vectorial database. Finally, we will get the most similar artist for a given artist by doing vector search on the database generated. To provide similar artists recommendations for a given artist ID  for music streaming platform ',\n",
       "  'corrected_bullet': 'Developed a recommendation model using PyTorch and Python, leveraging embeddings. Initially, the artist data was used to generate features for the model. Then, the Instructor Model, a text embedding model fine-tuned for instructions, was used to generate artist embeddings. These embeddings were then used to populate a vectorial database. Finally, the most similar artist for a given artist can be obtained by conducting a vector search on the generated database. This allows for providing similar artist recommendations for a given artist ID on a music streaming platform.'},\n",
       " {'user_id': '',\n",
       "  'what': 'synthetic personas exploratory analysis',\n",
       "  'verb': 'collaborated',\n",
       "  'skills': 'LLM and python',\n",
       "  'why': 'predict sales based on model responses',\n",
       "  'how': '',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'collaborated synthetic personas exploratory analysis using LLM and python  predict sales based on model responses for American multinational consumer products company',\n",
       "  'corrected_bullet': 'Collaborated on an exploratory analysis using LLM and Python to predict sales based on model responses for an American multinational consumer products company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'synthetic personas exploratory analysis',\n",
       "  'verb': 'collaborated',\n",
       "  'skills': 'LLM and python',\n",
       "  'why': 'predict sales based on model responses',\n",
       "  'how': 'translating model responses of product ratings',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'collaborated synthetic personas exploratory analysis using LLM and python leveraging ontranslating model responses of product ratings predict sales based on model responses for American multinational consumer products company',\n",
       "  'corrected_bullet': 'Collaborated on an exploratory analysis using LLM and Python to leverage translating model responses of product ratings in order to predict sales for an American multinational consumer products company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'exploratory analysis project',\n",
       "  'verb': 'collaborated',\n",
       "  'skills': 'chatGPT',\n",
       "  'why': 'explore feasibility of LLM these models for marketing purposes',\n",
       "  'how': 'generate synthetic personas for product inquiries, subsequently translating their feedback into sales predictions',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'collaborated exploratory analysis project using chatGPT leveraging ongenerate synthetic personas for product inquiries, subsequently translating their feedback into sales predictions explore feasibility of LLM these models for marketing purposes for American multinational consumer products company',\n",
       "  'corrected_bullet': 'Collaborated on an exploratory analysis project using chatGPT to leverage on-generated synthetic personas for product inquiries. Subsequently, I translated their feedback into sales predictions to explore the feasibility of using LLM models for marketing purposes for an American multinational consumer products company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'dataset',\n",
       "  'verb': 'Ingest',\n",
       "  'skills': 'airflow, bigquery',\n",
       "  'why': 'to allow product concept discovery and trend prediction',\n",
       "  'how': 'Implementing a pipeline',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'Ingest dataset using airflow and bigquery leveraging onImplementing a pipeline to allow product concept discovery and trend prediction for American multinational consumer products company',\n",
       "  'corrected_bullet': 'To ingest the dataset using Airflow and BigQuery, we leverage on implementing a pipeline to allow product concept discovery and trend prediction for an American multinational consumer products company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'dataset',\n",
       "  'verb': 'Ingestion',\n",
       "  'skills': 'airflow, bigquery',\n",
       "  'why': 'to allow product concept discovery and trend prediction',\n",
       "  'how': 'Implementing a pipeline',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'Ingestion dataset using airflow and bigquery leveraging onImplementing a pipeline to allow product concept discovery and trend prediction for American multinational consumer products company',\n",
       "  'corrected_bullet': 'Ingested dataset using airflow and bigquery, leveraging on implementing a pipeline to allow product concept discovery and trend prediction for an American multinational consumer products company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'product and ingredients dataset',\n",
       "  'verb': 'Ingestion',\n",
       "  'skills': 'airflow, bigquery',\n",
       "  'why': 'to allow product concept discovery and trend prediction',\n",
       "  'how': 'Implementing a pipeline',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'Ingestion product and ingredients dataset using airflow and bigquery leveraging onImplementing a pipeline to allow product concept discovery and trend prediction for American multinational consumer products company',\n",
       "  'corrected_bullet': 'Ingested product and ingredients dataset using airflow and bigquery, leveraging on implementing a pipeline to allow product concept discovery and trend prediction for an American multinational consumer products company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'product and ingredients dataset',\n",
       "  'verb': 'Ingestion',\n",
       "  'skills': 'airflow, bigquery',\n",
       "  'why': 'to allow product concept discovery and trend prediction',\n",
       "  'how': 'Airflow pipeline',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'Ingestion product and ingredients dataset using airflow and bigquery leveraging onAirflow pipeline to allow product concept discovery and trend prediction for American multinational consumer products company',\n",
       "  'corrected_bullet': 'Ingested the product and ingredients dataset using Airflow and BigQuery, leveraging the onAirflow pipeline to enable product concept discovery and trend prediction for an American multinational consumer products company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'product and ingredients dataset',\n",
       "  'verb': 'Ingestion',\n",
       "  'skills': 'airflow, bigquery',\n",
       "  'why': 'to allow product concept discovery and trend prediction',\n",
       "  'how': '',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'Ingestion product and ingredients dataset using airflow and bigquery  to allow product concept discovery and trend prediction for American multinational consumer products company',\n",
       "  'corrected_bullet': 'Ingested the product and ingredients dataset using Airflow and BigQuery to enable product concept discovery and trend prediction for an American multinational consumer products company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'consumption layer',\n",
       "  'verb': 'model',\n",
       "  'skills': 'airflow, dbt, snowflake, terraform',\n",
       "  'why': 'To ensure consistency and accuracy in data consumption across different tools and teams',\n",
       "  'how': 'dbt best practices',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'model consumption layer using airflow, dbt, snowflake and terraform leveraging ondbt best practices To ensure consistency and accuracy in data consumption across different tools and teams for American multinational consumer products company',\n",
       "  'corrected_bullet': 'To ensure consistency and accuracy in data consumption across different tools and teams for an American multinational consumer products company, I modeled the consumption layer using Airflow, dbt, Snowflake, and Terraform, leveraging on dbt best practices.'},\n",
       " {'user_id': '',\n",
       "  'what': 'consumption layer',\n",
       "  'verb': 'modeled',\n",
       "  'skills': 'airflow, dbt, snowflake, terraform',\n",
       "  'why': 'To ensure consistency and accuracy in data consumption across different tools and teams',\n",
       "  'how': 'dbt best practices',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'modeled consumption layer using airflow, dbt, snowflake and terraform leveraging ondbt best practices To ensure consistency and accuracy in data consumption across different tools and teams for American multinational consumer products company',\n",
       "  'corrected_bullet': 'Modeled the consumption layer using Airflow, dbt, Snowflake, and Terraform, leveraging on dbt best practices to ensure consistency and accuracy in data consumption across different tools and teams for an American multinational consumer products company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'observability report',\n",
       "  'verb': 'built',\n",
       "  'skills': 'domo, sql, snowflake',\n",
       "  'why': 'To ensure pipelines are working correctly and ingested data is healty',\n",
       "  'how': '',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'built observability report using domo, sql and snowflake  To ensure pipelines are working correctly and ingested data is healty for American multinational consumer products company',\n",
       "  'corrected_bullet': 'Built an observability report using Domo, SQL, and Snowflake to ensure that pipelines were working correctly and ingested data was healthy for an American multinational consumer products company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'observability report',\n",
       "  'verb': 'built',\n",
       "  'skills': 'domo, sql, snowflake',\n",
       "  'why': 'To ensure pipelines are working correctly and ingested data is in good quality',\n",
       "  'how': '',\n",
       "  'whom': 'American multinational consumer products company',\n",
       "  'raw_bullet': 'built observability report using domo, sql and snowflake  To ensure pipelines are working correctly and ingested data is in good quality for American multinational consumer products company',\n",
       "  'corrected_bullet': 'Built an observability report using Domo, SQL, and Snowflake to ensure that pipelines were working correctly and that ingested data was of good quality for an American multinational consumer products company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Activity Reports',\n",
       "  'verb': 'Automated a report that was being run manually on a weekly basis without any big changes, as well as communicate with the business team to understand their needs and translate them into enhacenemnts to the existing reports as well as new reporting and monitoring tools',\n",
       "  'skills': 'Python, SQL, NLP',\n",
       "  'why': 'To monitor an application in production and generate useful insights for the business team that would help them determine whether this app was working as intended and helping users',\n",
       "  'how': '',\n",
       "  'whom': 'One of the largest pharmaceutical companies in the US',\n",
       "  'raw_bullet': 'Automated a report that was being run manually on a weekly basis without any big changes, as well as communicate with the business team to understand their needs and translate them into enhacenemnts to the existing reports as well as new reporting and monitoring tools Activity Reports using Python, SQL, NLP  To monitor an application in production and generate useful insights for the business team that would help them determine whether this app was working as intended and helping users for One of the largest pharmaceutical companies in the US',\n",
       "  'corrected_bullet': 'Automated a report that was being manually run on a weekly basis without any significant changes. Additionally, communicated with the business team to comprehend their requirements and translate them into enhancements for the existing reports, as well as new reporting and monitoring tools called Activity Reports. This was achieved using Python, SQL, and NLP. The purpose was to monitor an application in production and generate valuable insights for the business team. These insights would assist them in determining whether the application was functioning as intended and benefiting users. This project was undertaken for one of the largest pharmaceutical companies in the US.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Duplicate Record Detection',\n",
       "  'verb': 'Designed and implemented a model to detect duplicated outputs/records coming from a batch process, generating a similarity score for relevant pairs of records so that the threshold of what is duplicated or not could be easily adjusted',\n",
       "  'skills': 'Python, SQL, NLP',\n",
       "  'why': 'To detect duplicates being generated by a model in charge of evaluating multiple rules on multiple database tables',\n",
       "  'how': 'NLP',\n",
       "  'whom': 'One of the largest pharmaceutical companies in the US',\n",
       "  'raw_bullet': 'Designed and implemented a model to detect duplicated outputs/records coming from a batch process, generating a similarity score for relevant pairs of records so that the threshold of what is duplicated or not could be easily adjusted Duplicate Record Detection using Python, SQL, NLP leveraging onNLP To detect duplicates being generated by a model in charge of evaluating multiple rules on multiple database tables for One of the largest pharmaceutical companies in the US',\n",
       "  'corrected_bullet': 'Designed and implemented a model to detect duplicated outputs/records that were coming from a batch process. The model generated a similarity score for relevant pairs of records, allowing for easy adjustment of the threshold to determine what is considered duplicated or not. The duplicate record detection was performed using Python, SQL, and NLP techniques. This was done to identify duplicates being generated by a model responsible for evaluating multiple rules on multiple database tables for one of the largest pharmaceutical companies in the US.'},\n",
       " {'user_id': '',\n",
       "  'what': 'learning experiences',\n",
       "  'verb': 'Led',\n",
       "  'skills': '',\n",
       "  'why': 'to support and increase success of students',\n",
       "  'how': '',\n",
       "  'whom': 'Henry Bootcamp',\n",
       "  'raw_bullet': 'Led learning experiences using  and undefined  to support and increase success of students for Henry Bootcamp',\n",
       "  'corrected_bullet': 'Led learning experiences using undefined resources to support and increase the success of students for Henry Bootcamp.'},\n",
       " {'user_id': '',\n",
       "  'what': 'learning experiences',\n",
       "  'verb': 'Led',\n",
       "  'skills': '',\n",
       "  'why': 'to support and increase success of students',\n",
       "  'how': 'coordinated the resolution of exercises and promoted group collaboration (Pair Programming)',\n",
       "  'whom': 'Henry Bootcamp',\n",
       "  'raw_bullet': 'Led learning experiences using  and undefined leveraging oncoordinated the resolution of exercises and promoted group collaboration (Pair Programming) to support and increase success of students for Henry Bootcamp',\n",
       "  'corrected_bullet': 'Led learning experiences using undefined and coordinated the resolution of exercises, leveraging group collaboration (Pair Programming), to support and increase the success of students at Henry Bootcamp.'},\n",
       " {'user_id': '',\n",
       "  'what': 'learning experiences',\n",
       "  'verb': 'Led',\n",
       "  'skills': '',\n",
       "  'why': 'to support and increase success of students',\n",
       "  'how': 'coordinated the resolution of exercises and promoted group collaboration (Pair Programming)',\n",
       "  'whom': 'Henry Bootcamp',\n",
       "  'raw_bullet': 'Led learning experiences using  and undefined leveraging oncoordinated the resolution of exercises and promoted group collaboration (Pair Programming) to support and increase success of students for Henry Bootcamp',\n",
       "  'corrected_bullet': 'Led learning experiences using undefined and coordinated the resolution of exercises, leveraging group collaboration (Pair Programming), to support and increase the success of students at Henry Bootcamp.'},\n",
       " {'user_id': '',\n",
       "  'what': 'learning experiences',\n",
       "  'verb': 'Led',\n",
       "  'skills': '',\n",
       "  'why': 'to support and increase success of students',\n",
       "  'how': 'coordinated the resolution of exercises and promoted group collaboration (Pair Programming)',\n",
       "  'whom': 'Henry Bootcamp',\n",
       "  'raw_bullet': 'Led learning experiences using  and undefined leveraging oncoordinated the resolution of exercises and promoted group collaboration (Pair Programming) to support and increase success of students for Henry Bootcamp',\n",
       "  'corrected_bullet': 'Led learning experiences using undefined and coordinated the resolution of exercises, leveraging group collaboration (Pair Programming), to support and increase the success of students at Henry Bootcamp.'},\n",
       " {'user_id': '',\n",
       "  'what': '[What did you do?]',\n",
       "  'verb': '[Action]',\n",
       "  'skills': '[Skills]',\n",
       "  'why': '[What was the purpose?]',\n",
       "  'how': '[How? (Optional)]',\n",
       "  'whom': '[What company was it done for?]',\n",
       "  'raw_bullet': '[Action] [What did you do?] using [Skills] leveraging on[How? (Optional)] [What was the purpose?] for [What company was it done for?]',\n",
       "  'corrected_bullet': '[Performed] [What did you do?] using [Skills] [by leveraging] [How? (Optional)] [What was the purpose?] [for which company was it done?]. \\nPreserve the past tense verb at the start of the sentence.'},\n",
       " {'user_id': '',\n",
       "  'what': 'pop up, header modifications, and bug fixes',\n",
       "  'verb': 'Maintained',\n",
       "  'skills': 'React, CSS, HTML, git',\n",
       "  'why': 'To fix any technical issues and inform users more effficiently',\n",
       "  'how': '',\n",
       "  'whom': 'one of the largest banks in Colombia',\n",
       "  'raw_bullet': 'Maintained pop up, header modifications, and bug fixes using React, CSS, HTML and git  To fix any technical issues and inform users more effficiently for one of the largest banks in Colombia',\n",
       "  'corrected_bullet': 'Maintained pop-up, header modifications, and bug fixes using React, CSS, HTML, and Git to fix any technical issues and inform users more efficiently for one of the largest banks in Colombia.'},\n",
       " {'user_id': '',\n",
       "  'what': 'pop up, header modifications, and bug fixes',\n",
       "  'verb': 'Maintained',\n",
       "  'skills': 'React, CSS, HTML, git',\n",
       "  'why': 'To fix any technical issues and inform users more effficiently',\n",
       "  'how': '',\n",
       "  'whom': 'one of the largest banks in Colombia',\n",
       "  'raw_bullet': 'Maintained pop up, header modifications, and bug fixes using React, CSS, HTML and git  To fix any technical issues and inform users more effficiently for one of the largest banks in Colombia',\n",
       "  'corrected_bullet': 'Maintained pop-up, header modifications, and bug fixes using React, CSS, HTML, and Git to fix any technical issues and inform users more efficiently for one of the largest banks in Colombia.'},\n",
       " {'user_id': '',\n",
       "  'what': 'bi pipeline',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python',\n",
       "  'why': 'To improve the efficiency of transporting goods or resources',\n",
       "  'how': '',\n",
       "  'whom': 'a fintech',\n",
       "  'raw_bullet': 'Developed bi pipeline using Python  To improve the efficiency of transporting goods or resources for a fintech',\n",
       "  'corrected_bullet': 'Developed a pipeline using Python to improve the efficiency of transporting goods or resources for a fintech.'},\n",
       " {'user_id': '',\n",
       "  'what': 'pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python',\n",
       "  'why': 'To ensure a steady and reliable supply of resources or materials',\n",
       "  'how': '',\n",
       "  'whom': 'IBM',\n",
       "  'raw_bullet': 'Built pipeline using Python  To ensure a steady and reliable supply of resources or materials for IBM',\n",
       "  'corrected_bullet': 'Built a pipeline using Python to ensure a steady and reliable supply of resources or materials for IBM.'},\n",
       " {'user_id': '',\n",
       "  'what': 'pipeline',\n",
       "  'verb': 'built',\n",
       "  'skills': 'AWS Cloud, Terraform, Python, FastAPI, Docker',\n",
       "  'why': 'To annotate and train a Computer vision Model for Table detection',\n",
       "  'how': '',\n",
       "  'whom': 'Eyelevel',\n",
       "  'raw_bullet': 'built pipeline using AWS Cloud, Terraform, Python, FastAPI, Docker  To annotate and train a Computer vision Model for Table detection for Eyelevel',\n",
       "  'corrected_bullet': 'Built a pipeline using AWS Cloud, Terraform, Python, FastAPI, and Docker to annotate and train a computer vision model for table detection for Eyelevel.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Annotation and structured data extraction pipeline',\n",
       "  'verb': 'built',\n",
       "  'skills': 'AWS Cloud, Terraform, Python, FastAPI, Docker',\n",
       "  'why': 'To annotate and train a Computer vision Model for Table detection',\n",
       "  'how': '',\n",
       "  'whom': 'Eyelevel',\n",
       "  'raw_bullet': 'built Annotation and structured data extraction pipeline using AWS Cloud, Terraform, Python, FastAPI, Docker  To annotate and train a Computer vision Model for Table detection for Eyelevel',\n",
       "  'corrected_bullet': 'Built an Annotation and structured data extraction pipeline using AWS Cloud, Terraform, Python, FastAPI, and Docker to annotate and train a computer vision model for table detection for Eyelevel.'},\n",
       " {'user_id': '',\n",
       "  'what': 'machine learning model',\n",
       "  'verb': 'predicted',\n",
       "  'skills': 'r, xgboost, tidyverse, ggplot2, github',\n",
       "  'why': 'evaluate whether potential insured person needs a comprehensive medical examination for better life insurance risk assessments and policy pricing accuracy. ',\n",
       "  'how': 'ML Modeling, Tabular Data, Supervised Learning, Natural Language Processing',\n",
       "  'whom': 'life insurance company',\n",
       "  'raw_bullet': 'predicted machine learning model using r, xgboost, tidyverse, ggplot2 and github leveraging onML Modeling, Tabular Data, Supervised Learning, Natural Language Processing evaluate whether potential insured person needs a comprehensive medical examination for better life insurance risk assessments and policy pricing accuracy.  for life insurance company',\n",
       "  'corrected_bullet': 'Predicted a machine learning model using R, xgboost, tidyverse, ggplot2, and GitHub. I leveraged on ML Modeling, Tabular Data, Supervised Learning, and Natural Language Processing to evaluate whether a potential insured person needed a comprehensive medical examination for better life insurance risk assessments and policy pricing accuracy. This was done for a life insurance company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'benchmarking  analysis',\n",
       "  'verb': 'Performed',\n",
       "  'skills': 'Excel, PowerPoint, Python',\n",
       "  'why': \"To compare company's products' prices and characteristics with competitors similar products\",\n",
       "  'how': 'By gathering data from competitors catalogues and comparing with our own catalogue. Identifying differences in margin for those products that were outperforming. ',\n",
       "  'whom': 'MillerKnoll',\n",
       "  'raw_bullet': \"Performed benchmarking  analysis using Excel, PowerPoint and Python leveraging onBy gathering data from competitors catalogues and comparing with our own catalogue. Identifying differences in margin for those products that were outperforming.  To compare company's products' prices and characteristics with competitors similar products for MillerKnoll\",\n",
       "  'corrected_bullet': \"Performed benchmarking analysis using Excel, PowerPoint, and Python by gathering data from competitors' catalogues and comparing it with our own catalogue. I identified differences in margin for those products that outperformed. To compare MillerKnoll's products' prices and characteristics with competitors' similar products.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'etl data etl pipelines',\n",
       "  'verb': 'Designed and built',\n",
       "  'skills': 'Sql, Python, Spark, Snowflake, Dbt',\n",
       "  'why': 'To integrate data from different systems and formats into a unified and consistent format',\n",
       "  'how': 'researching for other tech',\n",
       "  'whom': 'major electronics tv company',\n",
       "  'raw_bullet': 'Designed and built etl data etl pipelines using Sql, Python, Spark, Snowflake and Dbt leveraging onresearching for other tech To integrate data from different systems and formats into a unified and consistent format for major electronics tv company',\n",
       "  'corrected_bullet': 'Designed and built ETL data pipelines using SQL, Python, Spark, Snowflake, and Dbt while researching other technologies to integrate data from different systems and formats into a unified and consistent format for a major electronics TV company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'benchmarking  analysis',\n",
       "  'verb': 'Performed',\n",
       "  'skills': 'Excel, PowerPoint, Python',\n",
       "  'why': 'Understand current pricing of leading products and evaluate if our prices can be modified in order to be more competitive',\n",
       "  'how': 'By gathering data from competitors catalogues and comparing with our own catalogue. Identifying differences in margin for those products that were outperforming. ',\n",
       "  'whom': 'MillerKnoll',\n",
       "  'raw_bullet': 'Performed benchmarking  analysis using Excel, PowerPoint and Python leveraging onBy gathering data from competitors catalogues and comparing with our own catalogue. Identifying differences in margin for those products that were outperforming.  Understand current pricing of leading products and evaluate if our prices can be modified in order to be more competitive for MillerKnoll',\n",
       "  'corrected_bullet': \"Performed benchmarking analysis using Excel, PowerPoint, and Python by gathering data from competitors' catalogues and comparing it with our own catalogue. I identified differences in margin for those products that outperformed. I understood the current pricing of leading products and evaluated if our prices could be modified in order to be more competitive for MillerKnoll.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'data platform',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Go, Aws, Snowflake',\n",
       "  'why': 'To centralize and organize company data minimazing developing times',\n",
       "  'how': '',\n",
       "  'whom': 'for an electronics multinational the largest TV manufacturer in the world',\n",
       "  'raw_bullet': 'Developed data platform using Python, Go, Aws and Snowflake  To centralize and organize company data minimazing developing times for for an electronics multinational the largest TV manufacturer in the world',\n",
       "  'corrected_bullet': 'Developed a data platform using Python, Go, AWS, and Snowflake to centralize and organize company data, minimizing development times for an electronics multinational, the largest TV manufacturer in the world.'},\n",
       " {'user_id': '',\n",
       "  'what': 'product performance dashboard',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Sql, Snowflake, Tableau',\n",
       "  'why': 'Identify products that were not performing well so the product team could clean the catalogue as well as check the pricing of the products that had many PDP views but no sales',\n",
       "  'how': 'Comparing PDP views and sales',\n",
       "  'whom': 'MillerKnoll',\n",
       "  'raw_bullet': 'Developed product performance dashboard using Sql, Snowflake and Tableau leveraging onComparing PDP views and sales Identify products that were not performing well so the product team could clean the catalogue as well as check the pricing of the products that had many PDP views but no sales for MillerKnoll',\n",
       "  'corrected_bullet': 'Developed a product performance dashboard using SQL, Snowflake, and Tableau to leverage the comparison of PDP views and sales. This allowed the identification of products that were not performing well, enabling the product team to clean the catalog and review the pricing of products that had many PDP views but no sales for MillerKnoll.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Contract margin analysis dashboard',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Sql, Snowflake, Excel, Tableau',\n",
       "  'why': 'To identify potential areas for improvement in pricing strategies',\n",
       "  'how': '',\n",
       "  'whom': 'a fortune 500 manufacturer',\n",
       "  'raw_bullet': 'Developed Contract margin analysis dashboard using Sql, Snowflake, Excel and Tableau  To identify potential areas for improvement in pricing strategies for a fortune 500 manufacturer',\n",
       "  'corrected_bullet': 'Developed a contract margin analysis dashboard using SQL, Snowflake, Excel, and Tableau to identify potential areas for improvement in pricing strategies for a Fortune 500 manufacturer.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data on different data sources',\n",
       "  'verb': 'Compared',\n",
       "  'skills': 'sql',\n",
       "  'why': 'to ensure the correct behaviour of the existing models on the new data source',\n",
       "  'how': 'By comparing the distributions on the overlapping ranges of the equivalent variables ',\n",
       "  'whom': 'a top insurance company in the us',\n",
       "  'raw_bullet': 'Compared data on different data sources using sql leveraging onBy comparing the distributions on the overlapping ranges of the equivalent variables  to ensure the correct behaviour of the existing models on the new data source for a top insurance company in the us',\n",
       "  'corrected_bullet': 'By comparing the distributions on the overlapping ranges of the equivalent variables, I leveraged SQL to compare data from different data sources. This was done to ensure the correct behavior of the existing models on the new data source for a top insurance company in the US.'},\n",
       " {'user_id': '',\n",
       "  'what': 'an R library',\n",
       "  'verb': 'developed',\n",
       "  'skills': 'datatable',\n",
       "  'why': 'To facilitate recurring analysis',\n",
       "  'how': '',\n",
       "  'whom': 'a top insurance company in the us',\n",
       "  'raw_bullet': 'developed an R library using datatable  To facilitate recurring analysis for a top insurance company in the us',\n",
       "  'corrected_bullet': 'Developed an R library using datatable to facilitate recurring analysis for a top insurance company in the US.'},\n",
       " {'user_id': '',\n",
       "  'what': 'an R library',\n",
       "  'verb': 'developed',\n",
       "  'skills': 'data.table',\n",
       "  'why': 'To facilitate recurring analysis',\n",
       "  'how': '',\n",
       "  'whom': 'a top insurance company in the us',\n",
       "  'raw_bullet': 'developed an R library using data.table  To facilitate recurring analysis for a top insurance company in the us',\n",
       "  'corrected_bullet': 'Developed an R library using data.table to facilitate recurring analysis for a top insurance company in the US.'},\n",
       " {'user_id': '',\n",
       "  'what': 'trend forecasting system',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python',\n",
       "  'why': 'Monitor all metrics and KPI to build meaninful alerts for abnormal values',\n",
       "  'how': 'Using Facebook Prophet',\n",
       "  'whom': 'Lead generator company',\n",
       "  'raw_bullet': 'Developed trend forecasting system using Python leveraging onUsing Facebook Prophet Monitor all metrics and KPI to build meaninful alerts for abnormal values for Lead generator company',\n",
       "  'corrected_bullet': 'Developed a trend forecasting system using Python, leveraging Facebook Prophet to monitor all metrics and KPIs in order to build meaningful alerts for abnormal values for a lead generator company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data capture approach',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Sql, Tableau',\n",
       "  'why': 'Improve the tracking system to gather insights over user behaviour',\n",
       "  'how': 'Custom trackers and Snowplow',\n",
       "  'whom': 'Lead generator company',\n",
       "  'raw_bullet': 'Developed data capture approach using Python, Sql and Tableau leveraging onCustom trackers and Snowplow Improve the tracking system to gather insights over user behaviour for Lead generator company',\n",
       "  'corrected_bullet': 'Developed a data capture approach using Python, SQL, and Tableau, leveraging custom trackers and Snowplow to improve the tracking system for gathering insights on user behavior for a lead generator company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'transactions service',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'postgres, c#, s3',\n",
       "  'why': 'Implement a live auction between different ads providers using different ML algorithms to increase the company revenue',\n",
       "  'how': 'Implement custom and scalable web service that allow integration with multiple algorithms and decision making flow on real time',\n",
       "  'whom': 'Lead generator company',\n",
       "  'raw_bullet': 'Developed transactions service using postgres, c# and s3 leveraging onImplement custom and scalable web service that allow integration with multiple algorithms and decision making flow on real time Implement a live auction between different ads providers using different ML algorithms to increase the company revenue for Lead generator company',\n",
       "  'corrected_bullet': 'Developed a transaction service using Postgres, C#, and S3, leveraging the implementation of a custom and scalable web service that allows integration with multiple algorithms and decision-making flow in real-time. I also implemented a live auction between different ads providers using different ML algorithms to increase the company revenue for a Lead generator company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis reports',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Snowflake',\n",
       "  'why': \"To gain insights and understanding about the company's performance and trends\",\n",
       "  'how': 'Implementing a custom logic using python and snowflake to preprocess billions of records and provide custom insights over marketing audiences',\n",
       "  'whom': 'Marketing Company',\n",
       "  'raw_bullet': \"Developed data analysis reports using Python and Snowflake leveraging onImplementing a custom logic using python and snowflake to preprocess billions of records and provide custom insights over marketing audiences To gain insights and understanding about the company's performance and trends for Marketing Company\",\n",
       "  'corrected_bullet': \"Developed data analysis reports using Python and Snowflake, leveraging custom logic implemented with Python and Snowflake to preprocess billions of records and provide custom insights on marketing audiences. This was done to gain insights and understanding about the company's performance and trends for the Marketing Company.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'scalable data pipeline workflows',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, AWS Lambda, S3',\n",
       "  'why': 'Build a workflow to gather, validate, enrich and upload invoices to a database.',\n",
       "  'how': 'Implement several AWS lambdas stages to process and enrich invoices to add them to a database or notify the clients about errors in the submited invoice, reducing time and effort for account clients.',\n",
       "  'whom': 'Personal Project',\n",
       "  'raw_bullet': 'Developed scalable data pipeline workflows using Python, AWS Lambda and S3 leveraging onImplement several AWS lambdas stages to process and enrich invoices to add them to a database or notify the clients about errors in the submited invoice, reducing time and effort for account clients. Build a workflow to gather, validate, enrich and upload invoices to a database. for Personal Project',\n",
       "  'corrected_bullet': 'Developed scalable data pipeline workflows using Python, AWS Lambda, and S3. I leveraged several AWS Lambda stages to process and enrich invoices, adding them to a database or notifying clients about errors in the submitted invoice. This reduced time and effort for account clients. I also built a workflow to gather, validate, enrich, and upload invoices to a database for a personal project.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl data pipelines',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Sql, Dbt, Airflow, Python',\n",
       "  'why': 'To facilitate data governance and compliance with regulatory requirements',\n",
       "  'how': 'distributed processes',\n",
       "  'whom': 'a healthcare company',\n",
       "  'raw_bullet': 'Built etl data pipelines using Sql, Dbt, Airflow and Python leveraging ondistributed processes To facilitate data governance and compliance with regulatory requirements for a healthcare company',\n",
       "  'corrected_bullet': 'Built ETL data pipelines using SQL, Dbt, Airflow, and Python, leveraging distributed processes to facilitate data governance and compliance with regulatory requirements for a healthcare company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'managerial  dashboard',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Sql, Dbt, Airflow, Python',\n",
       "  'why': 'To facilitate data governance and compliance with regulatory requirements',\n",
       "  'how': 'distributed processes',\n",
       "  'whom': 'a healthcare company',\n",
       "  'raw_bullet': 'Built managerial  dashboard using Sql, Dbt, Airflow and Python leveraging ondistributed processes To facilitate data governance and compliance with regulatory requirements for a healthcare company',\n",
       "  'corrected_bullet': 'Built a managerial dashboard using SQL, Dbt, Airflow, and Python, leveraging distributed processes to facilitate data governance and compliance with regulatory requirements for a healthcare company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data warehouse schema',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Snowflake, Python, Spark, Sql, Aws',\n",
       "  'why': 'To enable data analysis and reporting',\n",
       "  'how': '',\n",
       "  'whom': 'Samsung Ads',\n",
       "  'raw_bullet': 'Built data warehouse schema using Snowflake, Python, Spark, Sql and Aws  To enable data analysis and reporting for Samsung Ads',\n",
       "  'corrected_bullet': 'Built a data warehouse schema using Snowflake, Python, Spark, SQL, and AWS to enable data analysis and reporting for Samsung Ads.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data warehouse schema',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Snowflake, Python, Spark, Sql, Aws',\n",
       "  'why': 'To enable data analysis and reporting',\n",
       "  'how': '',\n",
       "  'whom': 'a Giant Consumer Electronics',\n",
       "  'raw_bullet': 'Built data warehouse schema using Snowflake, Python, Spark, Sql and Aws  To enable data analysis and reporting for a Giant Consumer Electronics',\n",
       "  'corrected_bullet': 'Built a data warehouse schema using Snowflake, Python, Spark, SQL, and AWS to enable data analysis and reporting for a Giant Consumer Electronics.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data warehouse schema',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Snowflake, Python, Spark, Sql, Aws',\n",
       "  'why': 'To enable data analysis and reporting',\n",
       "  'how': '',\n",
       "  'whom': 'an electronics company',\n",
       "  'raw_bullet': 'Built data warehouse schema using Snowflake, Python, Spark, Sql and Aws  To enable data analysis and reporting for an electronics company',\n",
       "  'corrected_bullet': 'Built a data warehouse schema using Snowflake, Python, Spark, SQL, and AWS to enable data analysis and reporting for an electronics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl data pipelines',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Spark, Sql, Aws',\n",
       "  'why': 'Facilitate data transformation and enrichment',\n",
       "  'how': '',\n",
       "  'whom': 'an electronics company',\n",
       "  'raw_bullet': 'Built etl data pipelines using Python, Spark, Sql and Aws  Facilitate data transformation and enrichment for an electronics company',\n",
       "  'corrected_bullet': 'Built ETL data pipelines using Python, Spark, SQL, and AWS to facilitate data transformation and enrichment for an electronics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'monitoring strategies',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Sql, Snowflake, Airflow',\n",
       "  'why': 'To keep track of the performance and progress of their strategies',\n",
       "  'how': '',\n",
       "  'whom': 'an electronics company',\n",
       "  'raw_bullet': 'Built monitoring strategies using Python, Sql, Snowflake and Airflow  To keep track of the performance and progress of their strategies for an electronics company',\n",
       "  'corrected_bullet': 'Built monitoring strategies using Python, SQL, Snowflake, and Airflow to keep track of the performance and progress of their strategies for an electronics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'monitoring strategies',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Sql, Snowflake, Airflow',\n",
       "  'why': 'To keep track of the performance and progress of their strategies',\n",
       "  'how': '',\n",
       "  'whom': 'an electronics company',\n",
       "  'raw_bullet': 'Built monitoring strategies using Python, Sql, Snowflake and Airflow  To keep track of the performance and progress of their strategies for an electronics company',\n",
       "  'corrected_bullet': 'Built monitoring strategies using Python, SQL, Snowflake, and Airflow to keep track of the performance and progress of their strategies for an electronics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'monitoring strategies',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Sql, Snowflake, Airflow',\n",
       "  'why': 'To keep track of the performance, progress, and proper functioning of your products.',\n",
       "  'how': '',\n",
       "  'whom': 'an electronics company',\n",
       "  'raw_bullet': 'Built monitoring strategies using Python, Sql, Snowflake and Airflow  To keep track of the performance, progress, and proper functioning of your products. for an electronics company',\n",
       "  'corrected_bullet': 'Built monitoring strategies using Python, SQL, Snowflake, and Airflow to keep track of the performance, progress, and proper functioning of products for an electronics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'monitoring strategies',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Sql, Snowflake, Airflow',\n",
       "  'why': 'To keep track of the performance, progress, and proper functioning of your products.',\n",
       "  'how': '',\n",
       "  'whom': 'an electronics company',\n",
       "  'raw_bullet': 'Built monitoring strategies using Python, Sql, Snowflake and Airflow  To keep track of the performance, progress, and proper functioning of your products. for an electronics company',\n",
       "  'corrected_bullet': 'Built monitoring strategies using Python, SQL, Snowflake, and Airflow to keep track of the performance, progress, and proper functioning of products for an electronics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'monitoring strategies',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Sql, Snowflake, Airflow',\n",
       "  'why': 'To keep track of the performance, progress, and proper functioning of their systems.',\n",
       "  'how': '',\n",
       "  'whom': 'an electronics company',\n",
       "  'raw_bullet': 'Built monitoring strategies using Python, Sql, Snowflake and Airflow  To keep track of the performance, progress, and proper functioning of their systems. for an electronics company',\n",
       "  'corrected_bullet': 'Built monitoring strategies using Python, SQL, Snowflake, and Airflow to keep track of the performance, progress, and proper functioning of their systems for an electronics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'monitoring strategies',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, Sql, Snowflake, Airflow',\n",
       "  'why': 'To track of the performance, progress, and proper functioning of systems',\n",
       "  'how': '',\n",
       "  'whom': 'an electronics company',\n",
       "  'raw_bullet': 'Built monitoring strategies using Python, Sql, Snowflake and Airflow  To track of the performance, progress, and proper functioning of systems for an electronics company',\n",
       "  'corrected_bullet': 'Built monitoring strategies using Python, SQL, Snowflake, and Airflow to track the performance, progress, and proper functioning of systems for an electronics company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl system',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Sql',\n",
       "  'why': 'To extract, transform, and load data from various sources into a central data warehouse or database',\n",
       "  'how': 'aws data lake framework',\n",
       "  'whom': 'Pragma',\n",
       "  'raw_bullet': 'Developed etl system using Python and Sql leveraging onaws data lake framework To extract, transform, and load data from various sources into a central data warehouse or database for Pragma',\n",
       "  'corrected_bullet': 'Developed an ETL system using Python and SQL, leveraging the AWS data lake framework to extract, transform, and load data from various sources into a central data warehouse or database for Pragma.'},\n",
       " {'user_id': '',\n",
       "  'what': 'microservices web application',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Go, Microservices, REST, Postgress, Snowflake',\n",
       "  'why': 'serve the data processing to ui through by REST',\n",
       "  'how': '',\n",
       "  'whom': 'leverage our platform to fuel further innovation in advertising',\n",
       "  'raw_bullet': 'Developed microservices web application using Go, Microservices, REST, Postgress and Snowflake  serve the data processing to ui through by REST for leverage our platform to fuel further innovation in advertising',\n",
       "  'corrected_bullet': 'Developed a microservices web application using Go, Microservices, REST, Postgres, and Snowflake to serve the data processing to the UI through REST, leveraging our platform to fuel further innovation in advertising.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data platform',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Go, Microservices, REST, Postgress, Snowflake',\n",
       "  'why': 'serve the data processing to ui through by REST',\n",
       "  'how': '',\n",
       "  'whom': 'leverage our platform to fuel further innovation in advertising',\n",
       "  'raw_bullet': 'Developed data platform using Go, Microservices, REST, Postgress and Snowflake  serve the data processing to ui through by REST for leverage our platform to fuel further innovation in advertising',\n",
       "  'corrected_bullet': 'Developed a data platform using Go, Microservices, REST, Postgres, and Snowflake to serve the data processing to the UI through REST, in order to leverage our platform and fuel further innovation in advertising.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data platform',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Go, Microservices, REST, Postgress, Snowflake',\n",
       "  'why': 'To enable easy access and retrieval of data for analysis and decision-making',\n",
       "  'how': '',\n",
       "  'whom': 'leverage our platform to fuel further innovation in advertising',\n",
       "  'raw_bullet': 'Developed data platform using Go, Microservices, REST, Postgress and Snowflake  To enable easy access and retrieval of data for analysis and decision-making for leverage our platform to fuel further innovation in advertising',\n",
       "  'corrected_bullet': 'Developed a data platform using Go, Microservices, REST, Postgres, and Snowflake to enable easy access and retrieval of data for analysis and decision-making. This platform will be leveraged to fuel further innovation in advertising.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data platform',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Go, Microservices, REST, Postgress, Snowflake',\n",
       "  'why': 'To enable easy access and retrieval of data for analysis and decision-making',\n",
       "  'how': '',\n",
       "  'whom': 'a leading digital advertising technology company that provides software and services to media agencies, publishers and advertisers',\n",
       "  'raw_bullet': 'Developed data platform using Go, Microservices, REST, Postgress and Snowflake  To enable easy access and retrieval of data for analysis and decision-making for a leading digital advertising technology company that provides software and services to media agencies, publishers and advertisers',\n",
       "  'corrected_bullet': 'Developed a data platform using Go, Microservices, REST, Postgres, and Snowflake to enable easy access and retrieval of data for analysis and decision-making. This platform was developed for a leading digital advertising technology company that provides software and services to media agencies, publishers, and advertisers.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data platform',\n",
       "  'verb': 'Mantain',\n",
       "  'skills': 'Microservices, REST, Python, Scala, Mysql, Spark',\n",
       "  'why': 'To enable easy access and retrieval of data for analysis and decision-making',\n",
       "  'how': '',\n",
       "  'whom': 'a leading digital advertising technology company that provides software and services to media agencies, publishers and advertisers',\n",
       "  'raw_bullet': 'Mantain data platform using Microservices, REST, Python, Scala, Mysql and Spark  To enable easy access and retrieval of data for analysis and decision-making for a leading digital advertising technology company that provides software and services to media agencies, publishers and advertisers',\n",
       "  'corrected_bullet': 'Maintain the data platform using Microservices, REST, Python, Scala, MySQL, and Spark to enable easy access and retrieval of data for analysis and decision-making. This is for a leading digital advertising technology company that provides software and services to media agencies, publishers, and advertisers.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data platform',\n",
       "  'verb': 'Mantain',\n",
       "  'skills': 'Microservices, REST, Python, Scala, Mysql, Spark',\n",
       "  'why': 'To enable easy access and retrieval of data for analysis and decision-making',\n",
       "  'how': '',\n",
       "  'whom': 'a leading digital advertising technology company that provides software and services to media agencies, publishers and advertisers',\n",
       "  'raw_bullet': 'Mantain data platform using Microservices, REST, Python, Scala, Mysql and Spark  To enable easy access and retrieval of data for analysis and decision-making for a leading digital advertising technology company that provides software and services to media agencies, publishers and advertisers',\n",
       "  'corrected_bullet': 'Maintain the data platform using Microservices, REST, Python, Scala, MySQL, and Spark to enable easy access and retrieval of data for analysis and decision-making. This is for a leading digital advertising technology company that provides software and services to media agencies, publishers, and advertisers.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data platform',\n",
       "  'verb': 'Mantain',\n",
       "  'skills': 'React, Python, REST, MongoDB',\n",
       "  'why': 'To enable easy access and retrieval of data for analysis and decision-making',\n",
       "  'how': '',\n",
       "  'whom': 'a leading digital advertising technology company that provides software and services to media agencies, publishers and advertisers',\n",
       "  'raw_bullet': 'Mantain data platform using React, Python, REST and MongoDB  To enable easy access and retrieval of data for analysis and decision-making for a leading digital advertising technology company that provides software and services to media agencies, publishers and advertisers',\n",
       "  'corrected_bullet': 'Maintained the data platform using React, Python, REST, and MongoDB to enable easy access and retrieval of data for analysis and decision-making for a leading digital advertising technology company that provides software and services to media agencies, publishers, and advertisers.'},\n",
       " {'user_id': '',\n",
       "  'what': 'web data visualization',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'React, Python, REST, MongoDB',\n",
       "  'why': 'allow data scientists and quality  assurance to analyze the results',\n",
       "  'how': '',\n",
       "  'whom': 'a leading digital advertising technology company that provides software and services to media agencies, publishers and advertisers',\n",
       "  'raw_bullet': 'Built web data visualization using React, Python, REST and MongoDB  allow data scientists and quality  assurance to analyze the results for a leading digital advertising technology company that provides software and services to media agencies, publishers and advertisers',\n",
       "  'corrected_bullet': 'Built a web data visualization using React, Python, REST, and MongoDB to allow data scientists and quality assurance to analyze the results for a leading digital advertising technology company that provides software and services to media agencies, publishers, and advertisers.'},\n",
       " {'user_id': '',\n",
       "  'what': 'integration system',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Go, Graphql, REST',\n",
       "  'why': 'To enable seamless integration with third-party systems and applications',\n",
       "  'how': '',\n",
       "  'whom': 'InMarket, a leader in 360-degree marketing intelligence and real-time marketing for thousands of major brands',\n",
       "  'raw_bullet': 'Developed integration system using Go, Graphql and REST  To enable seamless integration with third-party systems and applications for InMarket, a leader in 360-degree marketing intelligence and real-time marketing for thousands of major brands',\n",
       "  'corrected_bullet': 'Developed an integration system using Go, Graphql, and REST to enable seamless integration with third-party systems and applications for InMarket, a leader in 360-degree marketing intelligence and real-time marketing for thousands of major brands.'},\n",
       " {'user_id': '',\n",
       "  'what': 'integration system',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Go, Graphql, REST',\n",
       "  'why': 'To enable seamless integration with third-party systems and applications',\n",
       "  'how': '',\n",
       "  'whom': 'a leading digital advertising technology company that provides software and services to media agencies, publishers, and advertisers',\n",
       "  'raw_bullet': 'Developed integration system using Go, Graphql and REST  To enable seamless integration with third-party systems and applications for a leading digital advertising technology company that provides software and services to media agencies, publishers, and advertisers',\n",
       "  'corrected_bullet': 'Developed an integration system using Go, Graphql, and REST to enable seamless integration with third-party systems and applications for a leading digital advertising technology company that provides software and services to media agencies, publishers, and advertisers.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl solutions',\n",
       "  'verb': 'Designed and implemented',\n",
       "  'skills': 'Azure Data Factory, Azure Functions, Azure Service Bus',\n",
       "  'why': 'To support data integration with external partners or third',\n",
       "  'how': '',\n",
       "  'whom': ' a multinational professional services partnership',\n",
       "  'raw_bullet': 'Designed and implemented etl solutions using Azure Data Factory, Azure Functions and Azure Service Bus  To support data integration with external partners or third for  a multinational professional services partnership',\n",
       "  'corrected_bullet': 'Designed and implemented ETL solutions using Azure Data Factory, Azure Functions, and Azure Service Bus to support data integration with external partners or third parties for a multinational professional services partnership.'},\n",
       " {'user_id': '',\n",
       "  'what': 'managment',\n",
       "  'verb': 'lead',\n",
       "  'skills': 'Project Managment, Scrum, Safe Methodoly',\n",
       "  'why': 'To enable efficient team',\n",
       "  'how': '',\n",
       "  'whom': ' a multinational professional services partnership',\n",
       "  'raw_bullet': 'lead managment using Project Managment, Scrum and Safe Methodoly  To enable efficient team for  a multinational professional services partnership',\n",
       "  'corrected_bullet': 'Led management using Project Management, Scrum, and Safe Methodology to enable an efficient team for a multinational professional services partnership.'},\n",
       " {'user_id': '',\n",
       "  'what': 'llm writing assistant',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'LLMs, streamlit, LangChain',\n",
       "  'why': 'To improve the quality and consistency of written content',\n",
       "  'how': 'large language models and prompt engineering best practices',\n",
       "  'whom': 'AI talent provider company',\n",
       "  'raw_bullet': 'Built llm writing assistant using LLMs, streamlit and LangChain leveraging onlarge language models and prompt engineering best practices To improve the quality and consistency of written content for AI talent provider company',\n",
       "  'corrected_bullet': 'Built an LLM writing assistant using LLMs, Streamlit, and LangChain, leveraging large language models and prompt engineering best practices to improve the quality and consistency of written content for an AI talent provider company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'writing assistant',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'LLMs, streamlit, LangChain',\n",
       "  'why': 'To improve the quality and consistency of written content',\n",
       "  'how': 'large language models and prompt engineering best practices',\n",
       "  'whom': 'AI talent provider company',\n",
       "  'raw_bullet': 'Built writing assistant using LLMs, streamlit and LangChain leveraging onlarge language models and prompt engineering best practices To improve the quality and consistency of written content for AI talent provider company',\n",
       "  'corrected_bullet': 'Built a writing assistant using LLMs, Streamlit, and LangChain, leveraging large language models and prompt engineering best practices to improve the quality and consistency of written content for an AI talent provider company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'content generation pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, AWS, Qdrant',\n",
       "  'why': 'To scale content production while maintining quality and brand consistency',\n",
       "  'how': 'large language models and retrieval augmented generation',\n",
       "  'whom': 'a major relocation company',\n",
       "  'raw_bullet': 'Built content generation pipeline using Python, AWS and Qdrant leveraging onlarge language models and retrieval augmented generation To scale content production while maintining quality and brand consistency for a major relocation company',\n",
       "  'corrected_bullet': 'Built a content generation pipeline using Python, AWS, and Qdrant, leveraging large language models and retrieval augmented generation to scale content production while maintaining quality and brand consistency for a major relocation company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Develop ETL pipeline',\n",
       "  'verb': 'developed and maintained data pipelines to ingest consumer data for generating audience insights reports.',\n",
       "  'skills': 'snowflake, python, jira, airflow, confluence',\n",
       "  'why': 'Enabling customers can generate reports to target specific section of audiences with ads, based on their online viewing behavior on TV.',\n",
       "  'how': '',\n",
       "  'whom': 'Consumer Electronics Company',\n",
       "  'raw_bullet': 'developed and maintained data pipelines to ingest consumer data for generating audience insights reports. Develop ETL pipeline using snowflake, python, jira, airflow and confluence  Enabling customers can generate reports to target specific section of audiences with ads, based on their online viewing behavior on TV. for Consumer Electronics Company',\n",
       "  'corrected_bullet': 'Developed and maintained data pipelines to ingest consumer data for generating audience insights reports. I developed an ETL pipeline using Snowflake, Python, Jira, Airflow, and Confluence. This enabled customers to generate reports to target specific sections of audiences with ads, based on their online viewing behavior on TV, for a Consumer Electronics Company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Develop ETL pipeline',\n",
       "  'verb': 'Created and maintained a data warehouse in Snowflake for several data formats from cloud services (AWS S3, Azure, GCP) using Snowflake stored procedures/Snowpipes. Created data-quality ingestion failure scripts using Python to generate an alert when incoming data is malformed. ',\n",
       "  'skills': 'snowflake, python, jira, airflow, confluence, AWS, GCP, Azure',\n",
       "  'why': 'To Maintain inventory data report across all segments and make the data available to 3rd party consumption.',\n",
       "  'how': '',\n",
       "  'whom': 'Goods and Food delivery company',\n",
       "  'raw_bullet': 'Created and maintained a data warehouse in Snowflake for several data formats from cloud services (AWS S3, Azure, GCP) using Snowflake stored procedures/Snowpipes. Created data-quality ingestion failure scripts using Python to generate an alert when incoming data is malformed.  Develop ETL pipeline using snowflake, python, jira, airflow, confluence, AWS, GCP and Azure  To Maintain inventory data report across all segments and make the data available to 3rd party consumption. for Goods and Food delivery company',\n",
       "  'corrected_bullet': 'Created and maintained a data warehouse in Snowflake for several data formats from cloud services such as AWS S3, Azure, and GCP using Snowflake stored procedures and Snowpipes. Additionally, I developed data-quality ingestion failure scripts using Python to generate an alert when incoming data is malformed. Furthermore, I developed an ETL pipeline using Snowflake, Python, Jira, Airflow, Confluence, AWS, GCP, and Azure to maintain an inventory data report across all segments and make the data available for third-party consumption. This was done for a Goods and Food delivery company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Develop ETL pipelines',\n",
       "  'verb': 'Created a data model and ETL pipeline using Pyspark, SparkSQL, and S3 data cleansing and migration to assist the business analyst in data cleansing and organization.',\n",
       "  'skills': 'snowflake, python, jira, airflow, confluence, AWS, Google Big Query',\n",
       "  'why': 'To maintain and make the data available for Data anaylist and management for daily insight in consumer behaviour on project they are most visiting, adding to cart and other actions they are taking. This help them to further streamline the product offering to consumers.',\n",
       "  'how': '',\n",
       "  'whom': 'Adtech company',\n",
       "  'raw_bullet': 'Created a data model and ETL pipeline using Pyspark, SparkSQL, and S3 data cleansing and migration to assist the business analyst in data cleansing and organization. Develop ETL pipelines using snowflake, python, jira, airflow, confluence, AWS and Google Big Query  To maintain and make the data available for Data anaylist and management for daily insight in consumer behaviour on project they are most visiting, adding to cart and other actions they are taking. This help them to further streamline the product offering to consumers. for Adtech company',\n",
       "  'corrected_bullet': 'Created a data model and ETL pipeline using Pyspark, SparkSQL, and S3 for data cleansing and migration. This was done to assist the business analyst in data cleansing and organization. I developed ETL pipelines using snowflake, python, jira, airflow, confluence, AWS, and Google Big Query to maintain and make the data available for data analysts and management. This allowed for daily insights into consumer behavior, such as the projects they are most visiting, items they are adding to cart, and other actions they are taking. This helps to further streamline the product offering to consumers for the Adtech company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'large language model benchmarking arena',\n",
       "  'verb': 'Designed',\n",
       "  'skills': 'Python, Pytorch, HuggingFace',\n",
       "  'why': 'To assess the performance and capabilities of different large language models',\n",
       "  'how': '',\n",
       "  'whom': 'the largest AI benchmarking non-profit',\n",
       "  'raw_bullet': 'Designed large language model benchmarking arena using Python, Pytorch and HuggingFace  To assess the performance and capabilities of different large language models for the largest AI benchmarking non-profit',\n",
       "  'corrected_bullet': 'Designed a large language model benchmarking arena using Python, PyTorch, and HuggingFace to assess the performance and capabilities of various large language models for the largest AI benchmarking non-profit.'},\n",
       " {'user_id': '',\n",
       "  'what': 'second place in a hackathon',\n",
       "  'verb': 'Got',\n",
       "  'skills': 'Pytorch, Langchain, Chainlit, Pinecone',\n",
       "  'why': 'Access to resources and support from the hackathon organizers and sponsors to continue developing their project',\n",
       "  'how': '',\n",
       "  'whom': 'a vector database provider',\n",
       "  'raw_bullet': 'Got second place in a hackathon using Pytorch, Langchain, Chainlit and Pinecone  Access to resources and support from the hackathon organizers and sponsors to continue developing their project for a vector database provider',\n",
       "  'corrected_bullet': 'Achieved second place in a hackathon by utilizing Pytorch, Langchain, Chainlit, and Pinecone. I was granted access to resources and received support from the hackathon organizers and sponsors to further develop my project as a vector database provider.'},\n",
       " {'user_id': '',\n",
       "  'what': 'second place in a hackathon',\n",
       "  'verb': 'Got',\n",
       "  'skills': 'Pytorch, Langchain, Chainlit, Pinecone',\n",
       "  'why': 'Access to resources and support from the hackathon organizers and sponsors to continue developing their project',\n",
       "  'how': 'a retrieval augmented generation model',\n",
       "  'whom': 'a vector database provider',\n",
       "  'raw_bullet': 'Got second place in a hackathon using Pytorch, Langchain, Chainlit and Pinecone leveraging ona retrieval augmented generation model Access to resources and support from the hackathon organizers and sponsors to continue developing their project for a vector database provider',\n",
       "  'corrected_bullet': 'Achieved second place in a hackathon by utilizing Pytorch, Langchain, Chainlit, and Pinecone. I leveraged a retrieval augmented generation model to access resources and support from the hackathon organizers and sponsors, enabling me to continue developing my project for a vector database provider.'},\n",
       " {'user_id': '',\n",
       "  'what': 'a talk',\n",
       "  'verb': 'Presented',\n",
       "  'skills': 'N-HiTS',\n",
       "  'why': 'to provide educational sessions, increase my forecasting skills and share up to date scientific research results with my colleagues',\n",
       "  'how': 'by using animations, custom graphics, and experiment results based on my experience on graphic desing',\n",
       "  'whom': 'Albertsons and Factored',\n",
       "  'raw_bullet': 'Presented a talk using N-HiTS leveraging onby using animations, custom graphics, and experiment results based on my experience on graphic desing to provide educational sessions, increase my forecasting skills and share up to date scientific research results with my colleagues for Albertsons and Factored',\n",
       "  'corrected_bullet': 'Presented a talk using N-HiTS, leveraging animations, custom graphics, and experiment results based on my experience in graphic design. This allowed me to provide educational sessions, enhance my forecasting skills, and share up-to-date scientific research results with my colleagues at Albertsons and Factored.'},\n",
       " {'user_id': '',\n",
       "  'what': 'case study on holiday forecasting',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'PySpark, Sql, StatsForecast',\n",
       "  'why': 'To provide insights and recommendations for optimizing holiday forecasting strategies',\n",
       "  'how': 'by performing historical analysis of two-week sales distributions',\n",
       "  'whom': 'Albertsons',\n",
       "  'raw_bullet': 'Developed case study on holiday forecasting using PySpark, Sql and StatsForecast leveraging onby performing historical analysis of two-week sales distributions To provide insights and recommendations for optimizing holiday forecasting strategies for Albertsons',\n",
       "  'corrected_bullet': 'Developed a case study on holiday forecasting using PySpark, SQL, and StatsForecast by performing historical analysis of two-week sales distributions. The aim was to provide insights and recommendations for optimizing holiday forecasting strategies for Albertsons.'},\n",
       " {'user_id': '',\n",
       "  'what': 'case study on holiday forecasting',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'PySpark, Sql, StatsForecast',\n",
       "  'why': 'To provide insights and recommendations for optimizing holiday forecasting strategies obtaining 28% error reduction on the candy scope',\n",
       "  'how': 'by performing historical analysis of two-week sales distributions',\n",
       "  'whom': 'Albertsons',\n",
       "  'raw_bullet': 'Developed case study on holiday forecasting using PySpark, Sql and StatsForecast leveraging onby performing historical analysis of two-week sales distributions To provide insights and recommendations for optimizing holiday forecasting strategies obtaining 28% error reduction on the candy scope for Albertsons',\n",
       "  'corrected_bullet': 'Developed a case study on holiday forecasting using PySpark, SQL, and StatsForecast by performing historical analysis of two-week sales distributions. This study provided insights and recommendations for optimizing holiday forecasting strategies, resulting in a 28% reduction in error on the candy scope for Albertsons.'},\n",
       " {'user_id': '',\n",
       "  'what': 'search, selection and onboarding process',\n",
       "  'verb': 'Supported',\n",
       "  'skills': 'Soft skills',\n",
       "  'why': 'streamline the selection process, increase the Factored team at Albertsons, and improve customer satisfaction',\n",
       "  'how': '',\n",
       "  'whom': 'Albertsons',\n",
       "  'raw_bullet': 'Supported search, selection and onboarding process using Soft skills  streamline the selection process, increase the Factored team at Albertsons, and improve customer satisfaction for Albertsons',\n",
       "  'corrected_bullet': 'Supported the search, selection, and onboarding process using soft skills to streamline the selection process, increase the Factored team at Albertsons, and improve customer satisfaction for Albertsons.'},\n",
       " {'user_id': '',\n",
       "  'what': 'search, selection and onboarding process of coworkers at Factored ',\n",
       "  'verb': 'Supported',\n",
       "  'skills': 'Soft skills',\n",
       "  'why': 'streamline the selection process, increase the Factored team at Albertsons, and improve customer satisfaction',\n",
       "  'how': '',\n",
       "  'whom': 'Albertsons',\n",
       "  'raw_bullet': 'Supported search, selection and onboarding process of coworkers at Factored  using Soft skills  streamline the selection process, increase the Factored team at Albertsons, and improve customer satisfaction for Albertsons',\n",
       "  'corrected_bullet': 'Supported the search, selection, and onboarding process of coworkers at Factored using soft skills to streamline the selection process, increase the Factored team at Albertsons, and improve customer satisfaction for Albertsons.'},\n",
       " {'user_id': '',\n",
       "  'what': 'search, selection and onboarding process of coworkers at Factored ',\n",
       "  'verb': 'Supported',\n",
       "  'skills': 'Soft skills',\n",
       "  'why': 'streamline the selection process, increase the Factored team, and improve customer satisfaction',\n",
       "  'how': '',\n",
       "  'whom': 'Albertsons',\n",
       "  'raw_bullet': 'Supported search, selection and onboarding process of coworkers at Factored  using Soft skills  streamline the selection process, increase the Factored team, and improve customer satisfaction for Albertsons',\n",
       "  'corrected_bullet': 'Supported the search, selection, and onboarding process of coworkers at Factored using soft skills to streamline the selection process, increase the Factored team, and improve customer satisfaction for Albertsons.'},\n",
       " {'user_id': '',\n",
       "  'what': 'computer vision solution',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python',\n",
       "  'why': 'Control robot movements',\n",
       "  'how': '',\n",
       "  'whom': 'educational',\n",
       "  'raw_bullet': 'Developed computer vision solution using Python  Control robot movements for educational',\n",
       "  'corrected_bullet': 'Developed a computer vision solution using Python to control robot movements for educational purposes.'},\n",
       " {'user_id': '',\n",
       "  'what': 'computer vision solution',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Fusion 360, 3D Printing',\n",
       "  'why': 'Control the moviment of a robot',\n",
       "  'how': '',\n",
       "  'whom': 'educational',\n",
       "  'raw_bullet': 'Developed computer vision solution using Python, Fusion 360 and 3D Printing  Control the moviment of a robot for educational',\n",
       "  'corrected_bullet': 'Developed a computer vision solution using Python, Fusion 360, and 3D printing to control the movement of an educational robot.'},\n",
       " {'user_id': '',\n",
       "  'what': 'computer vision solution',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Fusion 360, 3D Printing',\n",
       "  'why': 'Control the moviment of a robot',\n",
       "  'how': 'using cascad method',\n",
       "  'whom': 'educational',\n",
       "  'raw_bullet': 'Developed computer vision solution using Python, Fusion 360 and 3D Printing leveraging onusing cascad method Control the moviment of a robot for educational',\n",
       "  'corrected_bullet': 'Developed a computer vision solution using Python, Fusion 360, and 3D printing, leveraging the cascading method to control the movement of an educational robot.'},\n",
       " {'user_id': '',\n",
       "  'what': 'computer vision solution',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, Fusion 360, 3D Printing',\n",
       "  'why': 'Control the moviment of a robot',\n",
       "  'how': 'using cascad method',\n",
       "  'whom': 'Graduate Project',\n",
       "  'raw_bullet': 'Developed computer vision solution using Python, Fusion 360 and 3D Printing leveraging onusing cascad method Control the moviment of a robot for Graduate Project',\n",
       "  'corrected_bullet': 'Developed a computer vision solution for my Graduate Project, controlling the movement of a robot using Python, Fusion 360, and 3D Printing, leveraging the cascading method.'},\n",
       " {'user_id': '',\n",
       "  'what': 'multi robotic solution',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, openCV, ROS, Digital Tween, real drone',\n",
       "  'why': 'Control the navigation of a multi robot rescue system',\n",
       "  'how': '',\n",
       "  'whom': 'Undergraduation final project',\n",
       "  'raw_bullet': 'Developed multi robotic solution using Python, openCV, ROS, Digital Tween and real drone  Control the navigation of a multi robot rescue system for Undergraduation final project',\n",
       "  'corrected_bullet': 'Developed a multi-robotic solution for my Undergraduation final project. The solution utilized Python, openCV, ROS, Digital Tween, and real drone control to manage the navigation of a multi-robot rescue system.'},\n",
       " {'user_id': '',\n",
       "  'what': 'multi robotic solution',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, openCV, ROS, Digital Tween, real drone',\n",
       "  'why': 'Control the navigation of a multi robot rescue system',\n",
       "  'how': 'The drone follows the land robot and send it destination directions based on the horizon view. ',\n",
       "  'whom': 'Undergraduation final project',\n",
       "  'raw_bullet': 'Developed multi robotic solution using Python, openCV, ROS, Digital Tween and real drone leveraging onThe drone follows the land robot and send it destination directions based on the horizon view.  Control the navigation of a multi robot rescue system for Undergraduation final project',\n",
       "  'corrected_bullet': 'Developed a multi-robotic solution using Python, openCV, ROS, Digital Tween, and a real drone. The drone followed the land robot and sent it destination directions based on the horizon view. I controlled the navigation of a multi-robot rescue system for my Undergraduation final project.'},\n",
       " {'user_id': '',\n",
       "  'what': 'shadowing in an interview',\n",
       "  'verb': 'Perfomed',\n",
       "  'skills': '',\n",
       "  'why': \"understand different technical roles within the company, and contribute to Factored's growth by helping in the hiring process\",\n",
       "  'how': '',\n",
       "  'whom': 'Factored',\n",
       "  'raw_bullet': \"Perfomed shadowing in an interview using  and undefined  understand different technical roles within the company, and contribute to Factored's growth by helping in the hiring process for Factored\",\n",
       "  'corrected_bullet': \"Performed shadowing in an interview to understand different technical roles within the company and contributed to Factored's growth by helping in the hiring process for Factored.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'shadowing in an interview for teach lead selection',\n",
       "  'verb': 'Perfomed',\n",
       "  'skills': '',\n",
       "  'why': \"understand different technical roles within the company, and contribute to Factored's growth by helping in the hiring process\",\n",
       "  'how': '',\n",
       "  'whom': 'Factored',\n",
       "  'raw_bullet': \"Perfomed shadowing in an interview for teach lead selection using  and undefined  understand different technical roles within the company, and contribute to Factored's growth by helping in the hiring process for Factored\",\n",
       "  'corrected_bullet': \"Performed shadowing in an interview for the team lead selection, using it to understand different technical roles within the company and contribute to Factored's growth by assisting in the hiring process for Factored.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'fatigue and microsleep detection hardware for passenger transportation systems',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'C',\n",
       "  'why': 'To prevent accidents and minimize the risk of injuries or fatalities caused by drowsy driving',\n",
       "  'how': 'real-time signal processing of RPM and infrared lights emission control',\n",
       "  'whom': 'designer company of industrial electronic devices',\n",
       "  'raw_bullet': 'Developed fatigue and microsleep detection hardware for passenger transportation systems using C leveraging onreal-time signal processing of RPM and infrared lights emission control To prevent accidents and minimize the risk of injuries or fatalities caused by drowsy driving for designer company of industrial electronic devices',\n",
       "  'corrected_bullet': 'Developed fatigue and microsleep detection hardware for passenger transportation systems using C, leveraging real-time signal processing of RPM and infrared light emission control. This was done to prevent accidents and minimize the risk of injuries or fatalities caused by drowsy driving for a designer company of industrial electronic devices.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Integrated signal measurement system (ISMS)',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'C, Modbus RTU communication protocol over rs485',\n",
       "  'why': 'trigger alarms due to inappropriate use events or failures of industrial machinery and thus reduce incidents and extend production times.',\n",
       "  'how': '',\n",
       "  'whom': 'the top mining companies in Colombia',\n",
       "  'raw_bullet': 'Built Integrated signal measurement system (ISMS) using C and Modbus RTU communication protocol over rs485  trigger alarms due to inappropriate use events or failures of industrial machinery and thus reduce incidents and extend production times. for the top mining companies in Colombia',\n",
       "  'corrected_bullet': 'Built the Integrated Signal Measurement System (ISMS) using C and the Modbus RTU communication protocol over RS485. This system triggers alarms in response to inappropriate use events or failures of industrial machinery, thereby reducing incidents and extending production times for the top mining companies in Colombia.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Integrated signal measurement system (ISMS)',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'C, Modbus RTU communication protocol over rs485',\n",
       "  'why': 'trigger alarms due to inappropriate use events or failures of industrial machinery and thus reduce incidents and extend production times.',\n",
       "  'how': '',\n",
       "  'whom': 'the top mining companies in Colombia',\n",
       "  'raw_bullet': 'Created Integrated signal measurement system (ISMS) using C and Modbus RTU communication protocol over rs485  trigger alarms due to inappropriate use events or failures of industrial machinery and thus reduce incidents and extend production times. for the top mining companies in Colombia',\n",
       "  'corrected_bullet': 'Created the Integrated Signal Measurement System (ISMS) using C and Modbus RTU communication protocol over RS485 to trigger alarms due to inappropriate use events or failures of industrial machinery, thus reducing incidents and extending production times for the top mining companies in Colombia.'},\n",
       " {'user_id': '',\n",
       "  'what': 'stress tests',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'python, gurobi',\n",
       "  'why': 'Reduce costs',\n",
       "  'how': '',\n",
       "  'whom': 'raizen',\n",
       "  'raw_bullet': 'Developed stress tests using python and gurobi  Reduce costs for raizen',\n",
       "  'corrected_bullet': 'Developed stress tests using Python and Gurobi to reduce costs for Raizen.'},\n",
       " {'user_id': '',\n",
       "  'what': 'stress tests',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'python, gurobi',\n",
       "  'why': 'Reduce costs',\n",
       "  'how': 'optimization methods',\n",
       "  'whom': 'raizen',\n",
       "  'raw_bullet': 'Developed stress tests using python and gurobi leveraging onoptimization methods Reduce costs for raizen',\n",
       "  'corrected_bullet': 'Developed stress tests using Python and Gurobi, leveraging optimization methods to reduce costs for Raizen.'},\n",
       " {'user_id': '',\n",
       "  'what': 'pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Snowflake, SQL, Python',\n",
       "  'why': 'Enable decision-maker teams to resolve business questions',\n",
       "  'how': '',\n",
       "  'whom': 'Giant Consumer Electronics',\n",
       "  'raw_bullet': 'Built pipeline using Snowflake, SQL, Python  Enable decision-maker teams to resolve business questions for Giant Consumer Electronics',\n",
       "  'corrected_bullet': 'Built a pipeline using Snowflake, SQL, and Python to enable decision-maker teams to resolve business questions for Giant Consumer Electronics.'},\n",
       " {'user_id': '',\n",
       "  'what': 'pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python, SQL, Snowflake',\n",
       "  'why': 'Enable decision-maker teams to resolve business questions',\n",
       "  'how': '',\n",
       "  'whom': 'Giant Consumer Electronics',\n",
       "  'raw_bullet': 'Built pipeline using Python, SQL and Snowflake  Enable decision-maker teams to resolve business questions for Giant Consumer Electronics',\n",
       "  'corrected_bullet': 'Built a pipeline using Python, SQL, and Snowflake to enable decision-maker teams to resolve business questions for Giant Consumer Electronics.'},\n",
       " {'user_id': '',\n",
       "  'what': 'nlp model',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Pytorch',\n",
       "  'why': 'To enhance search engine capabilities and improve search results relevance',\n",
       "  'how': 'by creating a binary classification model to asess the relation between two texts',\n",
       "  'whom': 'Factored',\n",
       "  'raw_bullet': 'Created nlp model using Pytorch leveraging onby creating a binary classification model to asess the relation between two texts To enhance search engine capabilities and improve search results relevance for Factored',\n",
       "  'corrected_bullet': 'Created an NLP model using PyTorch by leveraging a binary classification model to assess the relation between two texts. This was done to enhance search engine capabilities and improve search result relevance for Factored.'},\n",
       " {'user_id': '',\n",
       "  'what': 'nlp model',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Pytorch',\n",
       "  'why': 'To enhance search engine capabilities and improve search results relevance',\n",
       "  'how': 'by creating a binary classification model to asess the relation between two texts',\n",
       "  'whom': 'Factored',\n",
       "  'raw_bullet': 'Created nlp model using Pytorch leveraging onby creating a binary classification model to asess the relation between two texts To enhance search engine capabilities and improve search results relevance for Factored',\n",
       "  'corrected_bullet': 'Created an NLP model using PyTorch by leveraging a binary classification model to assess the relation between two texts. This was done to enhance search engine capabilities and improve search result relevance for Factored.'},\n",
       " {'user_id': '',\n",
       "  'what': 'nlp model',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Pytorch',\n",
       "  'why': 'To enhance search engine capabilities and improve search results relevance',\n",
       "  'how': 'by creating a binary classification model to asess the relation between two texts',\n",
       "  'whom': 'Factored Sales Wiz tool',\n",
       "  'raw_bullet': 'Created nlp model using Pytorch leveraging onby creating a binary classification model to asess the relation between two texts To enhance search engine capabilities and improve search results relevance for Factored Sales Wiz tool',\n",
       "  'corrected_bullet': 'Created an NLP model using PyTorch by leveraging on creating a binary classification model to assess the relation between two texts. This was done to enhance search engine capabilities and improve search results relevance for the Factored Sales Wiz tool.'},\n",
       " {'user_id': '',\n",
       "  'what': 'optimization algorithm',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Javascript',\n",
       "  'why': 'To streamline operations and improve overall performance',\n",
       "  'how': \"Built a more efficient version of the client's library system, by eliminating redundancy and increasing cohesion.\",\n",
       "  'whom': 'Samsung',\n",
       "  'raw_bullet': \"Built optimization algorithm using Javascript leveraging onBuilt a more efficient version of the client's library system, by eliminating redundancy and increasing cohesion. To streamline operations and improve overall performance for Samsung\",\n",
       "  'corrected_bullet': \"Built an optimization algorithm using Javascript, leveraging it to create a more efficient version of the client's library system. This involved eliminating redundancy and increasing cohesion to streamline operations and improve overall performance for Samsung.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'interactive  dashboards',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'superset, python, snowflake',\n",
       "  'why': 'To provide real-time data visualization and analysis',\n",
       "  'how': '',\n",
       "  'whom': 'venture capital company',\n",
       "  'raw_bullet': 'Built interactive  dashboards using superset, python and snowflake  To provide real-time data visualization and analysis for venture capital company',\n",
       "  'corrected_bullet': 'Built interactive dashboards using Superset, Python, and Snowflake to provide real-time data visualization and analysis for a venture capital company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'mlops system',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': 'TF Serving, MLFlow, DVC',\n",
       "  'why': 'To streamline and automate the process of deploying machine learning models in production',\n",
       "  'how': '',\n",
       "  'whom': 'one of the biggest home camera retailers of the US',\n",
       "  'raw_bullet': 'Implemented mlops system using TF Serving, MLFlow and DVC  To streamline and automate the process of deploying machine learning models in production for one of the biggest home camera retailers of the US',\n",
       "  'corrected_bullet': 'Implemented the MLOps system using TF Serving, MLFlow, and DVC to streamline and automate the process of deploying machine learning models in production for one of the biggest home camera retailers in the US.'},\n",
       " {'user_id': '',\n",
       "  'what': 'mlops system',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': 'TF Serving, MLFlow, DVC',\n",
       "  'why': 'To automate the process of deploying machine learning models in production and improve reproducibility',\n",
       "  'how': '',\n",
       "  'whom': 'one of the biggest home camera retailers of the US',\n",
       "  'raw_bullet': 'Implemented mlops system using TF Serving, MLFlow and DVC  To automate the process of deploying machine learning models in production and improve reproducibility for one of the biggest home camera retailers of the US',\n",
       "  'corrected_bullet': 'Implemented the MLOps system using TF Serving, MLFlow, and DVC to automate the process of deploying machine learning models in production and improve reproducibility for one of the biggest home camera retailers in the US.'},\n",
       " {'user_id': '',\n",
       "  'what': 'interactive  dashboards',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'superset, python, snowflake, dbt, fivetran',\n",
       "  'why': 'To provide real-time data visualization and analysis that could improve the performance of the sales team',\n",
       "  'how': 'extracting data from raw excel files using fivetran, then transforming it using dbt and python, and finally creating interactive visualizations using superset',\n",
       "  'whom': 'retail company',\n",
       "  'raw_bullet': 'Built interactive  dashboards using superset, python, snowflake, dbt and fivetran leveraging onextracting data from raw excel files using fivetran, then transforming it using dbt and python, and finally creating interactive visualizations using superset To provide real-time data visualization and analysis that could improve the performance of the sales team for retail company',\n",
       "  'corrected_bullet': 'Built interactive dashboards using Superset, Python, Snowflake, dbt, and Fivetran. I leveraged the extraction of data from raw Excel files using Fivetran, then transformed it using dbt and Python. Finally, I created interactive visualizations using Superset to provide real-time data visualization and analysis that could improve the performance of the sales team for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'interactive  dashboards',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'superset, python, snowflake, dbt, fivetran',\n",
       "  'why': 'To provide real-time data visualization and analysis that could improve the performance of each sales team in every retail store',\n",
       "  'how': 'extracting data from raw excel files using fivetran, then transforming it using dbt and python, and finally creating interactive visualizations using superset',\n",
       "  'whom': 'retail company',\n",
       "  'raw_bullet': 'Built interactive  dashboards using superset, python, snowflake, dbt and fivetran leveraging onextracting data from raw excel files using fivetran, then transforming it using dbt and python, and finally creating interactive visualizations using superset To provide real-time data visualization and analysis that could improve the performance of each sales team in every retail store for retail company',\n",
       "  'corrected_bullet': 'Built interactive dashboards using Superset, Python, Snowflake, dbt, and Fivetran. Leveraged the extraction of data from raw Excel files using Fivetran, then transformed it using dbt and Python. Finally, created interactive visualizations using Superset to provide real-time data visualization and analysis. This could improve the performance of each sales team in every retail store for the retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'interactive  dashboards',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'superset, python, snowflake, dbt, fivetran',\n",
       "  'why': 'To provide real-time data visualization and analysis that could improve the performance of each sales team in every retail store',\n",
       "  'how': 'extracting data from raw excel files using fivetran, then transforming it using dbt and python, and finally creating interactive visualizations using superset',\n",
       "  'whom': 'retail company',\n",
       "  'raw_bullet': 'Built interactive  dashboards using superset, python, snowflake, dbt and fivetran leveraging onextracting data from raw excel files using fivetran, then transforming it using dbt and python, and finally creating interactive visualizations using superset To provide real-time data visualization and analysis that could improve the performance of each sales team in every retail store for retail company',\n",
       "  'corrected_bullet': 'Built interactive dashboards using Superset, Python, Snowflake, dbt, and Fivetran. Leveraged the extraction of data from raw Excel files using Fivetran, then transformed it using dbt and Python. Finally, created interactive visualizations using Superset to provide real-time data visualization and analysis. This could improve the performance of each sales team in every retail store for the retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'interactive  dashboards',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'superset, python, snowflake, dbt, fivetran',\n",
       "  'why': 'To provide real-time data visualization and analysis that could be used to improve the performance of each sales team in every retail store',\n",
       "  'how': 'extracting data from raw excel files using fivetran, then transforming it using dbt and python, and finally creating interactive visualizations using superset',\n",
       "  'whom': 'retail company',\n",
       "  'raw_bullet': 'Built interactive  dashboards using superset, python, snowflake, dbt and fivetran leveraging onextracting data from raw excel files using fivetran, then transforming it using dbt and python, and finally creating interactive visualizations using superset To provide real-time data visualization and analysis that could be used to improve the performance of each sales team in every retail store for retail company',\n",
       "  'corrected_bullet': 'Built interactive dashboards using Superset, Python, Snowflake, dbt, and Fivetran. Leveraged the extraction of data from raw Excel files using Fivetran, then transformed it using dbt and Python. Finally, created interactive visualizations using Superset to provide real-time data visualization and analysis. These tools were used to improve the performance of each sales team in every retail store for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'nlp assistant',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'python, langchain, gradio',\n",
       "  'why': 'To analyze performance of sales teams in retail stores and provide tailored recommendations to improve sales',\n",
       "  'how': '',\n",
       "  'whom': 'retail company',\n",
       "  'raw_bullet': 'Developed nlp assistant using python, langchain and gradio  To analyze performance of sales teams in retail stores and provide tailored recommendations to improve sales for retail company',\n",
       "  'corrected_bullet': 'Developed an NLP assistant using Python, Langchain, and Gradio to analyze the performance of sales teams in retail stores and provide tailored recommendations to improve sales for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'nlp assistant',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'python, langchain, gradio, gpt model',\n",
       "  'why': 'To analyze performance of sales teams in retail stores and provide tailored recommendations to improve sales',\n",
       "  'how': '',\n",
       "  'whom': 'retail company',\n",
       "  'raw_bullet': 'Developed nlp assistant using python, langchain, gradio and gpt model  To analyze performance of sales teams in retail stores and provide tailored recommendations to improve sales for retail company',\n",
       "  'corrected_bullet': 'Developed an NLP assistant using Python, Langchain, Gradio, and GPT model to analyze the performance of sales teams in retail stores and provide tailored recommendations to improve sales for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'nlp assistant',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'python, langchain, gradio',\n",
       "  'why': 'To analyze performance of sales teams in retail stores and provide tailored recommendations to improve sales',\n",
       "  'how': '',\n",
       "  'whom': 'retail company',\n",
       "  'raw_bullet': 'Developed nlp assistant using python, langchain and gradio  To analyze performance of sales teams in retail stores and provide tailored recommendations to improve sales for retail company',\n",
       "  'corrected_bullet': 'Developed an NLP assistant using Python, Langchain, and Gradio to analyze the performance of sales teams in retail stores and provide tailored recommendations to improve sales for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'nlp assistant',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'python, langchain, gradio',\n",
       "  'why': 'To analyze performance of the sales teams in retail stores to provide tailored recommendations to improve sales',\n",
       "  'how': '',\n",
       "  'whom': 'retail company',\n",
       "  'raw_bullet': 'Developed nlp assistant using python, langchain and gradio  To analyze performance of the sales teams in retail stores to provide tailored recommendations to improve sales for retail company',\n",
       "  'corrected_bullet': 'Developed an NLP assistant using Python, Langchain, and Gradio to analyze the performance of the sales teams in retail stores and provide tailored recommendations to improve sales for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'nlp assistant',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'python, langchain, gradio',\n",
       "  'why': 'To analyze performance of the sales teams in retail stores and provide tailored recommendations to increase sales',\n",
       "  'how': '',\n",
       "  'whom': 'retail company',\n",
       "  'raw_bullet': 'Developed nlp assistant using python, langchain and gradio  To analyze performance of the sales teams in retail stores and provide tailored recommendations to increase sales for retail company',\n",
       "  'corrected_bullet': 'Developed an NLP assistant using Python, Langchain, and Gradio to analyze the performance of the sales teams in retail stores and provide tailored recommendations to increase sales for a retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'sales strategy',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Tableau, Analytics, negotiation',\n",
       "  'why': 'To respond to changing market trends and customer demands',\n",
       "  'how': '',\n",
       "  'whom': 'ab inbev',\n",
       "  'raw_bullet': 'Developed sales strategy using Tableau, Analytics and negotiation  To respond to changing market trends and customer demands for ab inbev',\n",
       "  'corrected_bullet': 'Developed sales strategy using Tableau, Analytics, and negotiation to respond to changing market trends and customer demands for AB InBev.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Python',\n",
       "  'why': 'To support business intelligence and reporting',\n",
       "  'how': 'a data lake',\n",
       "  'whom': 'Factored',\n",
       "  'raw_bullet': 'Built etl pipeline using Python leveraging ona data lake To support business intelligence and reporting for Factored',\n",
       "  'corrected_bullet': 'Built an ETL pipeline using Python, leveraging a data lake to support business intelligence and reporting for Factored.'},\n",
       " {'user_id': '',\n",
       "  'what': 'ELT data pipeline',\n",
       "  'verb': 'Build',\n",
       "  'skills': 'Snowpipes, Storage Integrations, Snowflake, SQL',\n",
       "  'why': 'To generate reports for customers, sales and supply areas to keep track of their KPIs',\n",
       "  'how': 'leveraging Dimensional modelling',\n",
       "  'whom': 'US-based retail company',\n",
       "  'raw_bullet': 'Build ELT data pipeline using Snowpipes, Storage Integrations, Snowflake and SQL leveraging onleveraging Dimensional modelling To generate reports for customers, sales and supply areas to keep track of their KPIs for US-based retail company',\n",
       "  'corrected_bullet': 'To build the ELT data pipeline, Snowpipes, Storage Integrations, Snowflake, and SQL are utilized, leveraging Dimensional modeling. This enables the generation of reports for customers, sales, and supply areas, allowing them to keep track of their KPIs for a US-based retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'ELT data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Snowpipes, Storage Integrations, Snowflake, SQL',\n",
       "  'why': 'To generate reports for customers, sales and supply areas to keep track of their KPIs',\n",
       "  'how': 'leveraging Dimensional modelling',\n",
       "  'whom': 'US-based retail company',\n",
       "  'raw_bullet': 'Built ELT data pipeline using Snowpipes, Storage Integrations, Snowflake and SQL leveraging onleveraging Dimensional modelling To generate reports for customers, sales and supply areas to keep track of their KPIs for US-based retail company',\n",
       "  'corrected_bullet': 'Built an ELT data pipeline using Snowpipes, Storage Integrations, Snowflake, and SQL, leveraging Dimensional modeling to generate reports for customers, sales, and supply areas, keeping track of their KPIs for a US-based retail company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'business applications',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Python, java, react',\n",
       "  'why': 'to organize data for hiring new members',\n",
       "  'how': 'by rasearching and using UX methodology',\n",
       "  'whom': 'for a multinational HR software ',\n",
       "  'raw_bullet': 'Developed business applications using Python, java and react leveraging onby rasearching and using UX methodology to organize data for hiring new members for for a multinational HR software ',\n",
       "  'corrected_bullet': 'Developed business applications using Python, Java, and React, leveraging research and UX methodology to organize data for hiring new members for a multinational HR software.'},\n",
       " {'user_id': '',\n",
       "  'what': 'market basket analysis',\n",
       "  'verb': 'Built',\n",
       "  'skills': 'Sql, Tableau',\n",
       "  'why': 'To maximize sales and revenue',\n",
       "  'how': 'reinforcement learning techniques',\n",
       "  'whom': 'a company',\n",
       "  'raw_bullet': 'Built market basket analysis using Sql and Tableau leveraging onreinforcement learning techniques To maximize sales and revenue for a company',\n",
       "  'corrected_bullet': 'Built market basket analysis using SQL and Tableau, leveraging reinforcement learning techniques to maximize sales and revenue for a company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'market basket analysis',\n",
       "  'verb': 'Developed',\n",
       "  'skills': 'Tableau, Python, Excel',\n",
       "  'why': 'To identify cross-selling and upselling opportunities',\n",
       "  'how': 'multi-level marketing',\n",
       "  'whom': 'factored',\n",
       "  'raw_bullet': 'Developed market basket analysis using Tableau, Python and Excel leveraging onmulti-level marketing To identify cross-selling and upselling opportunities for factored',\n",
       "  'corrected_bullet': 'Developed market basket analysis using Tableau, Python, and Excel, leveraging multi-level marketing to identify cross-selling and upselling opportunities for factored.'},\n",
       " {'user_id': '',\n",
       "  'what': 'chatbot',\n",
       "  'verb': 'Created',\n",
       "  'skills': 'Python',\n",
       "  'why': 'To enhance user experience by providing quick and accurate responses',\n",
       "  'how': '',\n",
       "  'whom': 'Private equity firm',\n",
       "  'raw_bullet': 'Created chatbot using Python  To enhance user experience by providing quick and accurate responses for Private equity firm',\n",
       "  'corrected_bullet': 'Created a chatbot using Python to enhance the user experience by providing quick and accurate responses for a private equity firm.'},\n",
       " {'user_id': '',\n",
       "  'what': 'fraud detection algorithm',\n",
       "  'verb': 'Experienced',\n",
       "  'skills': '',\n",
       "  'why': \"To identify and prevent fraudulent activities within the company's operations\",\n",
       "  'how': '',\n",
       "  'whom': 'a jail',\n",
       "  'raw_bullet': \"Experienced fraud detection algorithm using Python  To identify and prevent fraudulent activities within the company's operations for a jail\",\n",
       "  'corrected_bullet': \"Mplemented a fraud detection algorithm using Python to identify and prevent fraudulent activities within the company's operations for a jail.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': '',\n",
       "  'why': 'To efficiently process and manage large volumes of data',\n",
       "  'how': 'Using DAGs logics',\n",
       "  'whom': 'TESTE',\n",
       "  'raw_bullet': 'Implemented data pipeline using Airflow, Spark and Python leveraging onUsing DAGs logics To efficiently process and manage large volumes of data for TESTE',\n",
       "  'corrected_bullet': 'Mplemented a data pipeline using Airflow, Spark, and Python, leveraging DAG logic to efficiently process and manage large volumes of data for TESTE.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Translate stakeholder business rules into effective data processing workflows through the application of Apache Spark (Pyspark) , Python, and SQL. This process enables the creation of essential Key Performance Indicators (KPIs) for key users.',\n",
       "  'verb': 'Helped',\n",
       "  'skills': '',\n",
       "  'why': 'Translate stakeholder business rules into effective data processing workflows',\n",
       "  'how': '',\n",
       "  'whom': 'TESTE',\n",
       "  'raw_bullet': 'Helped Translate stakeholder business rules into effective data processing workflows through the application of Apache Spark (Pyspark) , Python, and SQL. This process enables the creation of essential Key Performance Indicators (KPIs) for key users. using Sql  Translate stakeholder business rules into effective data processing workflows for TESTE',\n",
       "  'corrected_bullet': 'Translated stakeholder business rules into effective data processing workflows through the application of Apache Spark (Pyspark), Python, and SQL. This process enabled the creation of essential Key Performance Indicators (KPIs) for key users. Using SQL, translated stakeholder business rules into effective data processing workflows for TESTE.'},\n",
       " {'user_id': '',\n",
       "  'what': 'APIs and workflows for testing models and generating images',\n",
       "  'verb': 'Deployed',\n",
       "  'skills': '',\n",
       "  'why': 'high image throughput',\n",
       "  'how': '',\n",
       "  'whom': 'illustrators',\n",
       "  'raw_bullet': 'Deployed APIs and workflows for testing models and generating images using Docker and Python  high image throughput for illustrators',\n",
       "  'corrected_bullet': 'Deployed APIs and workflows for testing models and generating images using Docker and Python, ensuring high image throughput for illustrators.'},\n",
       " {'user_id': '',\n",
       "  'what': 'APIs and workflows for testing models and generating images',\n",
       "  'verb': 'Deployed',\n",
       "  'skills': '',\n",
       "  'why': 'model availability',\n",
       "  'how': '',\n",
       "  'whom': 'illustrators',\n",
       "  'raw_bullet': 'Deployed APIs and workflows for testing models and generating images using Docker and Python  model availability for illustrators',\n",
       "  'corrected_bullet': 'Deployed APIs and workflows for testing models and generating images using Docker and Python to ensure model availability for illustrators.'},\n",
       " {'user_id': '',\n",
       "  'what': 'a solution that could grant and revoke user access to certain folders and medical study files',\n",
       "  'verb': 'Design and implemented',\n",
       "  'skills': '',\n",
       "  'why': 'Streamline access management: The solution allows for efficient management of user access, making it easier for administrators to grant access to new employees, revoke access for terminated employees',\n",
       "  'how': 'testing and designing software components',\n",
       "  'whom': 'GSK, the 6th biggest pharmaceutical manufacturer',\n",
       "  'raw_bullet': 'Design and implemented a solution that could grant and revoke user access to certain folders and medical study files using Python, Azure, Azure Databricks, Azure Data Lake Storage and Azure Active Directory leveraging ontesting and designing software components Streamline access management: The solution allows for efficient management of user access, making it easier for administrators to grant access to new employees, revoke access for terminated employees for GSK, the 6th biggest pharmaceutical manufacturer',\n",
       "  'corrected_bullet': 'Designed and implemented a solution that can grant and revoke user access to specific folders and medical study files using Python, Azure, Azure Databricks, Azure Data Lake Storage, and Azure Active Directory. Leveraging on testing and designing software components, the solution streamlines access management. It allows for efficient management of user access, making it easier for administrators to grant access to new employees and revoke access for terminated employees for GSK, the 6th biggest pharmaceutical manufacturer.'},\n",
       " {'user_id': '',\n",
       "  'what': 'covering Python, Git, Object-Oriented Programming (OOP), Django, REST APIs, ORM, and Test-Driven Development (TDD). Engaged with a diverse group of +200 students from across Latin America and Spain, overseeing +150 successful completions. Also, managed a team of +9 tutors.',\n",
       "  'verb': 'Taught a course ',\n",
       "  'skills': '',\n",
       "  'why': 'To provide education and training on important programming languages and concepts such as Python, Git, Object-Oriented Programming (OOP), Django, REST APIs, ORM, and Test-Driven Development (TDD)',\n",
       "  'how': 'Achieved an impressive course rating of 4.9/5 and reviewed over +50 final projects, including the creation of portfolios.',\n",
       "  'whom': 'Coderhouse',\n",
       "  'raw_bullet': 'Taught a course  covering Python, Git, Object-Oriented Programming (OOP), Django, REST APIs, ORM, and Test-Driven Development (TDD). Engaged with a diverse group of +200 students from across Latin America and Spain, overseeing +150 successful completions. Also, managed a team of +9 tutors. using python, docker, django and git leveraging onAchieved an impressive course rating of 4.9/5 and reviewed over +50 final projects, including the creation of portfolios. To provide education and training on important programming languages and concepts such as Python, Git, Object-Oriented Programming (OOP), Django, REST APIs, ORM, and Test-Driven Development (TDD) for Coderhouse',\n",
       "  'corrected_bullet': 'Taught a course that covered Python, Git, Object-Oriented Programming (OOP), Django, REST APIs, ORM, and Test-Driven Development (TDD). I engaged with a diverse group of over 200 students from across Latin America and Spain, overseeing more than 150 successful completions. Additionally, I managed a team of 9 tutors, utilizing Python, Docker, Django, and Git to enhance the learning experience. I achieved an impressive course rating of 4.9/5 and reviewed over 50 final projects, including the creation of portfolios. My goal was to provide education and training on important programming languages and concepts such as Python, Git, Object-Oriented Programming (OOP), Django, REST APIs, ORM'},\n",
       " {'user_id': '',\n",
       "  'what': 'Specialized in Big Data projects and data product development.',\n",
       "  'verb': 'Enhanced existing data pipelines and incorporated new requirements. Refactored solutions for scalability, implementing generic pipelines suitable for various dataset sizes. Contributed to the design and development of new data products. Engaged weekly in data breakdown, pipeline design, parallelization strategies, and Scala code development. Actively participated in SCRUM methodologies, including daily meetings, sprint planning, backlog refinement, and sprint reviews. Collaborated with an interdisciplinary team to ensure effective development processes.',\n",
       "  'skills': '',\n",
       "  'why': 'The purpose was to leverage advanced technologies within the Hadoop Ecosystem to enhance data handling capabilities, improve scalability, and support the development of new data products.',\n",
       "  'how': 'Utilized technologies within the Hadoop Ecosystem, including Spark, Hive, Impala, HDFS, NiFi, Kafka, Sqoop, and programming languages such as Python and Scala, all on the Cloudera platform.',\n",
       "  'whom': 'The work was primarily for a leading pharmaceutical brazilian company.',\n",
       "  'raw_bullet': 'Enhanced existing data pipelines and incorporated new requirements. Refactored solutions for scalability, implementing generic pipelines suitable for various dataset sizes. Contributed to the design and development of new data products. Engaged weekly in data breakdown, pipeline design, parallelization strategies, and Scala code development. Actively participated in SCRUM methodologies, including daily meetings, sprint planning, backlog refinement, and sprint reviews. Collaborated with an interdisciplinary team to ensure effective development processes. Specialized in Big Data projects and data product development. using python, hadoop, spark, hive, hdfs, kafka, NiFi, sqoop and scala leveraging onUtilized technologies within the Hadoop Ecosystem, including Spark, Hive, Impala, HDFS, NiFi, Kafka, Sqoop, and programming languages such as Python and Scala, all on the Cloudera platform. The purpose was to leverage advanced technologies within the Hadoop Ecosystem to enhance data handling capabilities, improve scalability, and support the development of new data products. for The work was primarily for a leading pharmaceutical brazilian company.',\n",
       "  'corrected_bullet': 'Enhanced existing data pipelines and incorporated new requirements. Refactored solutions for scalability, implementing generic pipelines suitable for various dataset sizes. Contributed to the design and development of new data products. Engaged weekly in data breakdown, pipeline design, parallelization strategies, and Scala code development. Actively participated in SCRUM methodologies, including daily meetings, sprint planning, backlog refinement, and sprint reviews. Collaborated with an interdisciplinary team to ensure effective development processes. Specialized in Big Data projects and data product development, utilizing technologies within the Hadoop Ecosystem, including Spark, Hive, Impala, HDFS, NiFi, Kafka, Sqoop, and programming languages such as Python and Scala, all on the Cloudera platform. The purpose was'},\n",
       " {'user_id': '',\n",
       "  'what': 'Design, developed, deployed and sell of my own Management Software',\n",
       "  'verb': 'Design and development',\n",
       "  'skills': '',\n",
       "  'why': 'create a powerfull management application with simple user experience',\n",
       "  'how': '',\n",
       "  'whom': 'My own project',\n",
       "  'raw_bullet': 'Design and development Design, developed, deployed and sell of my own Management Software using python, django, postgres and C#  create a powerfull management application with simple user experience for My own project',\n",
       "  'corrected_bullet': 'Designed, developed, deployed, and sold my own Management Software using Python, Django, Postgres, and C#. Created a powerful management application with a simple user experience for my own project.'},\n",
       " {'user_id': '',\n",
       "  'what': 'with Python and creating an Event-Driven Architecture on AWS. using lambda, dynamodb, sqs',\n",
       "  'verb': 'Development of WhatsApp Chatbots ',\n",
       "  'skills': '',\n",
       "  'why': 'Creating an Event-Driven Architecture on AWS allows for efficient and scalable application development',\n",
       "  'how': '',\n",
       "  'whom': 'a internet company',\n",
       "  'raw_bullet': 'Development of WhatsApp Chatbots  with Python and creating an Event-Driven Architecture on AWS. using lambda, dynamodb, sqs using python, Lambda, Dynamodb, Aws and sqs  Creating an Event-Driven Architecture on AWS allows for efficient and scalable application development for a internet company',\n",
       "  'corrected_bullet': 'Developing WhatsApp Chatbots with Python and creating an Event-Driven Architecture on AWS using Lambda, DynamoDB, and SQS enables efficient and scalable application development for an internet company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Taught Scala to more than 50 Data Engineers',\n",
       "  'verb': 'Scala Teacher',\n",
       "  'skills': '',\n",
       "  'why': \"To empower the data engineers to utilize Scala's functional programming capabilities in their daily activities\",\n",
       "  'how': '',\n",
       "  'whom': 'BBVA IT Spain.',\n",
       "  'raw_bullet': \"Scala Teacher Taught Scala to more than 50 Data Engineers using scala, sbt, oop, functional programming and Aws  To empower the data engineers to utilize Scala's functional programming capabilities in their daily activities for BBVA IT Spain.\",\n",
       "  'corrected_bullet': \"The Scala teacher taught Scala to more than 50 data engineers using Scala, sbt, OOP, functional programming, and AWS to empower the data engineers to utilize Scala's functional programming capabilities in their daily activities for BBVA IT Spain.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'presented a project to the local government to give us money to equipate and start a programming club for people in my city, providing computers, internet, monitors, and many more. and also mentored a programming club for gilrs',\n",
       "  'verb': 'Founder of the \"Programalo\" Programming Club and Mentor at the Girls Programming Club.',\n",
       "  'skills': '',\n",
       "  'why': 'To provide opportunities for people in the city to learn and develop programming skills',\n",
       "  'how': '',\n",
       "  'whom': 'For godoy cruz guvernment',\n",
       "  'raw_bullet': 'Founder of the \"Programalo\" Programming Club and Mentor at the Girls Programming Club. presented a project to the local government to give us money to equipate and start a programming club for people in my city, providing computers, internet, monitors, and many more. and also mentored a programming club for gilrs using python, javascripts and git  To provide opportunities for people in the city to learn and develop programming skills for For godoy cruz guvernment',\n",
       "  'corrected_bullet': 'As the founder of the \"Programalo\" Programming Club and a mentor at the Girls Programming Club, I presented a project to the local government. The aim was to secure funding for equipping and starting a programming club in my city. This initiative would provide computers, internet access, monitors, and other necessary resources. Additionally, I mentored a programming club for girls, where we utilized Python, JavaScript, and Git. The goal was to offer opportunities for individuals in the city to learn and develop programming skills. This project was specifically targeted towards the Godoy Cruz government.'},\n",
       " {'user_id': '',\n",
       "  'what': 'created videos about python and coding and with Videos Surpassing 40,000 Views.',\n",
       "  'verb': 'YouTube Channel videos creation',\n",
       "  'skills': '',\n",
       "  'why': 'To engage with the audience and build a community of python and coding enthusiasts',\n",
       "  'how': '',\n",
       "  'whom': 'myself',\n",
       "  'raw_bullet': 'YouTube Channel videos creation created videos about python and coding and with Videos Surpassing 40,000 Views. using python, git and linux  To engage with the audience and build a community of python and coding enthusiasts for myself',\n",
       "  'corrected_bullet': 'Created YouTube Channel videos about Python and coding, surpassing 40,000 views. Used Python, Git, and Linux to engage with the audience and build a community of Python and coding enthusiasts for myself.'},\n",
       " {'user_id': '',\n",
       "  'what': 'we created a project called \"Smart Micromeasurement Network\" to implement iot networks to efficientizice the data collection of our water system in Argentina',\n",
       "  'verb': 'Award-Winning Project',\n",
       "  'skills': '',\n",
       "  'why': 'To contribute to the overall sustainability and effectiveness of our water system',\n",
       "  'how': '',\n",
       "  'whom': 'AySA - Water Conservation Theme',\n",
       "  'raw_bullet': 'Award-Winning Project we created a project called \"Smart Micromeasurement Network\" to implement iot networks to efficientizice the data collection of our water system in Argentina using iot, lora, mqtt and cloud  To contribute to the overall sustainability and effectiveness of our water system for AySA - Water Conservation Theme',\n",
       "  'corrected_bullet': 'Award-Winning Project: We created a project called \"Smart Micromeasurement Network\" to implement IoT networks to efficiently collect data from our water system in Argentina using IoT, LoRa, MQTT, and cloud technology. This project aims to contribute to the overall sustainability and effectiveness of our water system for AySA - Water Conservation Theme.'},\n",
       " {'user_id': '',\n",
       "  'what': 'real time application',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To provide a low cost remote controlled robot for visually inspecting limited access environments ',\n",
       "  'how': '',\n",
       "  'whom': \"Bachelor's Thesis\",\n",
       "  'raw_bullet': \"Built real time application using Java and C   To provide a low cost remote controlled robot for visually inspecting limited access environments  for Bachelor's Thesis\",\n",
       "  'corrected_bullet': \"Built a real-time application using Java and C to provide a low-cost remote-controlled robot for visually inspecting limited access environments for my Bachelor's Thesis.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis reports',\n",
       "  'verb': 'Prepared',\n",
       "  'skills': '',\n",
       "  'why': 'To assess the effectiveness of marketing campaigns',\n",
       "  'how': 'kjhbkjbkj',\n",
       "  'whom': 'for a car rental company',\n",
       "  'raw_bullet': 'Prepared data analysis reports using Sql, Tableau and Powerbi leveraging onkjhbkjbkj To assess the effectiveness of marketing campaigns for for a car rental company',\n",
       "  'corrected_bullet': 'Prepared data analysis reports using SQL, Tableau, and PowerBI, leveraging onkjhbkjbkj to assess the effectiveness of marketing campaigns for a car rental company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': '',\n",
       "  'why': 'To collect and store data from various sources',\n",
       "  'how': 'this way',\n",
       "  'whom': 'freeway',\n",
       "  'raw_bullet': 'Implemented data pipeline using Airflow, Python, Spark and Snowflake leveraging onthis way To collect and store data from various sources for freeway',\n",
       "  'corrected_bullet': 'Mplemented a data pipeline using Airflow, Python, Spark, and Snowflake to collect and store data from various sources for freeway.'},\n",
       " {'user_id': '',\n",
       "  'what': 'big data architectures',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To handle near real time data processing',\n",
       "  'how': '',\n",
       "  'whom': 'banking company, the 5th largest in Brazil',\n",
       "  'raw_bullet': 'Developed big data architectures using Spark, Python, Databricks and Azure  To handle near real time data processing for banking company, the 5th largest in Brazil',\n",
       "  'corrected_bullet': 'Developed big data architectures using Spark, Python, Databricks, and Azure to handle near real-time data processing for a banking company, the 5th largest in Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data processing pipeline',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To measure software development practices, being used as a performance evaluation metric for managers and their employees',\n",
       "  'how': '',\n",
       "  'whom': 'banking company, the 5th largest in Brazil',\n",
       "  'raw_bullet': 'Developed data processing pipeline using Sql, Python and Javascript  To measure software development practices, being used as a performance evaluation metric for managers and their employees for banking company, the 5th largest in Brazil',\n",
       "  'corrected_bullet': 'Developed a data processing pipeline using SQL, Python, and Javascript to measure software development practices. It was used as a performance evaluation metric for managers and their employees at the 5th largest banking company in Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Applied',\n",
       "  'skills': '',\n",
       "  'why': 'To streamline and automate the process of collecting, processing, and analyzing data',\n",
       "  'how': '',\n",
       "  'whom': 'campervan rental marketplace',\n",
       "  'raw_bullet': 'Applied data pipeline using Airbyte, Airflow, Python, Sql and BigQuery  To streamline and automate the process of collecting, processing, and analyzing data for campervan rental marketplace',\n",
       "  'corrected_bullet': 'Applied data pipeline using Airbyte, Airflow, Python, SQL, and BigQuery to streamline and automate the process of collecting, processing, and analyzing data for a campervan rental marketplace.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To streamline and automate the process of collecting, processing, and analyzing data',\n",
       "  'how': '',\n",
       "  'whom': 'campervan rental marketplace',\n",
       "  'raw_bullet': 'Built data pipeline using Airbyte, Airflow, Python, Sql and BigQuery  To streamline and automate the process of collecting, processing, and analyzing data for campervan rental marketplace',\n",
       "  'corrected_bullet': 'Built a data pipeline using Airbyte, Airflow, Python, SQL, and BigQuery to streamline and automate the process of collecting, processing, and analyzing data for a campervan rental marketplace.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Ad-hoc data analysisc',\n",
       "  'verb': 'Performed',\n",
       "  'skills': '',\n",
       "  'why': 'To gain insights and make decisions in a timely manner',\n",
       "  'how': '',\n",
       "  'whom': 'campervan rental marketplace',\n",
       "  'raw_bullet': 'Performed Ad-hoc data analysisc using Python  To gain insights and make decisions in a timely manner for campervan rental marketplace',\n",
       "  'corrected_bullet': 'Performed ad-hoc data analysis using Python to gain insights and make decisions in a timely manner for a campervan rental marketplace.'},\n",
       " {'user_id': '',\n",
       "  'what': 'pricing proposals models',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance storytelling and presentations with compelling visuals',\n",
       "  'how': '',\n",
       "  'whom': 'campervan rental marketplace',\n",
       "  'raw_bullet': 'Developed pricing proposals models using Powerbi  To enhance storytelling and presentations with compelling visuals for campervan rental marketplace',\n",
       "  'corrected_bullet': 'Developed pricing proposal models using PowerBI to enhance storytelling and presentations with compelling visuals for the campervan rental marketplace.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data visualizations',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance storytelling and presentations with compelling visuals',\n",
       "  'how': '',\n",
       "  'whom': 'campervan rental marketplace',\n",
       "  'raw_bullet': 'Developed data visualizations using Powerbi  To enhance storytelling and presentations with compelling visuals for campervan rental marketplace',\n",
       "  'corrected_bullet': 'Developed data visualizations using PowerBI to enhance storytelling and presentations with compelling visuals for the campervan rental marketplace.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To efficiently collect, process, and analyze large amounts of data',\n",
       "  'how': '',\n",
       "  'whom': 'ALLOS',\n",
       "  'raw_bullet': 'Built data pipeline using Python, Spark and Airflow  To efficiently collect, process, and analyze large amounts of data for ALLOS',\n",
       "  'corrected_bullet': 'Built a data pipeline using Python, Spark, and Airflow to efficiently collect, process, and analyze large amounts of data for ALLOS.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Designed',\n",
       "  'skills': '',\n",
       "  'why': 'To extract and load the data into a centralized database',\n",
       "  'how': '',\n",
       "  'whom': 'agricultural industry',\n",
       "  'raw_bullet': 'Designed data pipeline using Databricks and Spark  To extract and load the data into a centralized database for agricultural industry',\n",
       "  'corrected_bullet': 'Designed a data pipeline using Databricks and Spark to extract and load the data into a centralized database for the agricultural industry.'},\n",
       " {'user_id': '',\n",
       "  'what': 'managerial  dashboard',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'to improve the housing situation in tokyo',\n",
       "  'how': '',\n",
       "  'whom': 'housing industry',\n",
       "  'raw_bullet': 'Developed managerial  dashboard using  and undefined  to improve the housing situation in tokyo for housing industry',\n",
       "  'corrected_bullet': 'Developed a managerial dashboard using an undefined tool to improve the housing situation in Tokyo for the housing industry.'},\n",
       " {'user_id': '',\n",
       "  'what': 'home-sharing startup',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'to improve the housing situation in tokyo',\n",
       "  'how': '',\n",
       "  'whom': 'housing industry',\n",
       "  'raw_bullet': 'Developed home-sharing startup using  and undefined  to improve the housing situation in tokyo for housing industry',\n",
       "  'corrected_bullet': 'Developed a home-sharing startup using undefined technology to improve the housing situation in Tokyo for the housing industry.'},\n",
       " {'user_id': '',\n",
       "  'what': 'risk data mart',\n",
       "  'verb': 'Managed',\n",
       "  'skills': '',\n",
       "  'why': 'To gain better insight into specific areas of the business',\n",
       "  'how': '',\n",
       "  'whom': 'Banco do Brazil',\n",
       "  'raw_bullet': 'Managed risk data mart using Data  To gain better insight into specific areas of the business for Banco do Brazil',\n",
       "  'corrected_bullet': 'Managed risk data mart using Data to gain better insight into specific areas of the business for Banco do Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'financial  performance',\n",
       "  'verb': 'create an application to retrieve and summarize information about healthcare industry',\n",
       "  'skills': '',\n",
       "  'why': 'To generate content for email, banner, social media or internal publications',\n",
       "  'how': 'using retrieval augmented generation',\n",
       "  'whom': 'Siemens Healthineers',\n",
       "  'raw_bullet': 'create an application to retrieve and summarize information about healthcare industry financial  performance using Azure Open AI, Azure ML Prompt Flow and Azure Search leveraging onusing retrieval augmented generation To generate content for email, banner, social media or internal publications for Siemens Healthineers',\n",
       "  'corrected_bullet': 'Created an application to retrieve and summarize information about healthcare industry financial performance using Azure Open AI, Azure ML Prompt Flow, and Azure Search, leveraging retrieval augmented generation to generate content for email, banners, social media, or internal publications for Siemens Healthineers.'},\n",
       " {'user_id': '',\n",
       "  'what': 'financial  performance',\n",
       "  'verb': 'Managed',\n",
       "  'skills': '',\n",
       "  'why': 'To make informed decisions on resource allocation and investment',\n",
       "  'how': 'based on current activities',\n",
       "  'whom': 'finances',\n",
       "  'raw_bullet': 'Managed financial  performance using Sql leveraging onbased on current activities To make informed decisions on resource allocation and investment for finances',\n",
       "  'corrected_bullet': 'Managed financial performance using SQL based on current activities to make informed decisions on resource allocation and investment for finances.'},\n",
       " {'user_id': '',\n",
       "  'what': 'metabase  dashboards',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To track key performance indicators (KPIs) and metrics',\n",
       "  'how': '',\n",
       "  'whom': 'Cidadania4u',\n",
       "  'raw_bullet': 'Built metabase  dashboards using Sql  To track key performance indicators (KPIs) and metrics for Cidadania4u',\n",
       "  'corrected_bullet': 'Built Metabase dashboards using SQL to track key performance indicators (KPIs) and metrics for Cidadania4u.'},\n",
       " {'user_id': '',\n",
       "  'what': 'metabase  dashboards',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To track key performance indicators (KPIs) and metrics',\n",
       "  'how': '',\n",
       "  'whom': 'Cidadania4u',\n",
       "  'raw_bullet': 'Created metabase  dashboards using Sql  To track key performance indicators (KPIs) and metrics for Cidadania4u',\n",
       "  'corrected_bullet': 'Created metabase dashboards using SQL to track key performance indicators (KPIs) and metrics for Cidadania4u.'},\n",
       " {'user_id': '',\n",
       "  'what': 'product data dashboards',\n",
       "  'verb': 'Azured',\n",
       "  'skills': '',\n",
       "  'why': 'To track key performance indicators (KPIs) and metrics',\n",
       "  'how': 'Ingestion with Fivetran and Python',\n",
       "  'whom': 'Cidadania4u',\n",
       "  'raw_bullet': 'Azured product data dashboards using Snowflake and Sql leveraging onIngestion with Fivetran and Python To track key performance indicators (KPIs) and metrics for Cidadania4u',\n",
       "  'corrected_bullet': 'Azured product data dashboards using Snowflake and SQL, leveraging on ingestion with Fivetran and Python to track key performance indicators (KPIs) and metrics for Cidadania4u.'},\n",
       " {'user_id': '',\n",
       "  'what': 'snowflake data warehouse',\n",
       "  'verb': 'Azured',\n",
       "  'skills': '',\n",
       "  'why': 'To track key performance indicators (KPIs) and metrics',\n",
       "  'how': 'Ingestion with Fivetran and Python',\n",
       "  'whom': 'Cidadania4u',\n",
       "  'raw_bullet': 'Azured snowflake data warehouse using Snowflake and Sql leveraging onIngestion with Fivetran and Python To track key performance indicators (KPIs) and metrics for Cidadania4u',\n",
       "  'corrected_bullet': 'Using Snowflake and SQL, we azure the snowflake data warehouse, leveraging ingestion with Fivetran and Python to track key performance indicators (KPIs) and metrics for Cidadania4u.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data warehouse',\n",
       "  'verb': 'Designed',\n",
       "  'skills': '',\n",
       "  'why': 'To track key performance indicators (KPIs) and metrics',\n",
       "  'how': 'Ingestion with Fivetran and Python',\n",
       "  'whom': 'Cidadania4u',\n",
       "  'raw_bullet': 'Designed data warehouse using Snowflake and Sql leveraging onIngestion with Fivetran and Python To track key performance indicators (KPIs) and metrics for Cidadania4u',\n",
       "  'corrected_bullet': 'Designed data warehouse using Snowflake and SQL, leveraging ingestion with Fivetran and Python to track key performance indicators (KPIs) and metrics for Cidadania4u.'},\n",
       " {'user_id': '',\n",
       "  'what': 'complex sql queries',\n",
       "  'verb': 'Designed',\n",
       "  'skills': '',\n",
       "  'why': 'To meet specific business requirements or analytical needs',\n",
       "  'how': 'using dbt',\n",
       "  'whom': 'Cidadania4u',\n",
       "  'raw_bullet': 'Designed complex sql queries using Sql and dbt leveraging onusing dbt To meet specific business requirements or analytical needs for Cidadania4u',\n",
       "  'corrected_bullet': 'Designed complex SQL queries using SQL and dbt to meet specific business requirements or analytical needs for Cidadania4u.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data strategy',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To improve decision-making processes',\n",
       "  'how': '',\n",
       "  'whom': 'Magal Data Analytics',\n",
       "  'raw_bullet': 'Developed data strategy using Sql, Snowflake, Python and metabase  To improve decision-making processes for Magal Data Analytics',\n",
       "  'corrected_bullet': 'Developed a data strategy using SQL, Snowflake, Python, and Metabase to improve decision-making processes for Magal Data Analytics.'},\n",
       " {'user_id': '',\n",
       "  'what': 'model orchestration workflow',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance visibility and transparency into workflow progress and performance',\n",
       "  'how': '',\n",
       "  'whom': 'Magal Data Analytics',\n",
       "  'raw_bullet': 'Built model orchestration workflow using mozart  To enhance visibility and transparency into workflow progress and performance for Magal Data Analytics',\n",
       "  'corrected_bullet': 'Built model orchestration workflow using Mozart to enhance visibility and transparency into workflow progress and performance for Magal Data Analytics.'},\n",
       " {'user_id': '',\n",
       "  'what': 'model orchestration workflow',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance visibility and transparency into workflow progress and performance',\n",
       "  'how': '',\n",
       "  'whom': 'Magal Data Analytics',\n",
       "  'raw_bullet': 'Built model orchestration workflow using Mozart Data  To enhance visibility and transparency into workflow progress and performance for Magal Data Analytics',\n",
       "  'corrected_bullet': 'Built a model orchestration workflow using Mozart Data to enhance visibility and transparency into workflow progress and performance for Magal Data Analytics.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data ingestion',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': '',\n",
       "  'why': 'To collect and store data from various sources',\n",
       "  'how': '',\n",
       "  'whom': 'Magal Data Analytics',\n",
       "  'raw_bullet': 'Implemented data ingestion using Fivetran and Python  To collect and store data from various sources for Magal Data Analytics',\n",
       "  'corrected_bullet': 'Mplemented data ingestion using Fivetran and Python to collect and store data from various sources for Magal Data Analytics.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data ingestion',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': '',\n",
       "  'why': 'To collect and store data from various sources',\n",
       "  'how': '',\n",
       "  'whom': 'Cidadania4u',\n",
       "  'raw_bullet': 'Implemented data ingestion using Fivetran and Python  To collect and store data from various sources for Cidadania4u',\n",
       "  'corrected_bullet': 'Mplemented data ingestion using Fivetran and Python to collect and store data from various sources for Cidadania4u.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data strategy',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To improve decision-making processes within the company',\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': 'Developed data strategy using Mozart Data, Python, Sql, Fivetran and Metabase  To improve decision-making processes within the company for Firstbase.io',\n",
       "  'corrected_bullet': 'Developed a data strategy using Mozart Data, Python, SQL, Fivetran, and Metabase to improve decision-making processes within the company for Firstbase.io.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data ingestion',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': '',\n",
       "  'why': 'To collect and store large amounts of data from various sources',\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': 'Implemented data ingestion using Fivetran  To collect and store large amounts of data from various sources for Firstbase.io',\n",
       "  'corrected_bullet': 'Mplemented data ingestion using Fivetran to collect and store large amounts of data from various sources for Firstbase.io.'},\n",
       " {'user_id': '',\n",
       "  'what': 'kpi  reports',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': \"To track the company's performance and progress towards its goals\",\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': \"Built kpi  reports using python and Sql  To track the company's performance and progress towards its goals for Firstbase.io\",\n",
       "  'corrected_bullet': \"Built KPI reports using Python and SQL to track the company's performance and progress towards its goals for Firstbase.io.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'kpi  reports',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': \"To track the company's performance and progress towards its goals\",\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': \"Built kpi  reports using python, Sql and Metabase  To track the company's performance and progress towards its goals for Firstbase.io\",\n",
       "  'corrected_bullet': \"Built KPI reports using Python, SQL, and Metabase to track the company's performance and progress towards its goals for Firstbase.io.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline integration',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': '',\n",
       "  'why': 'To improve data accessibility and availability for analysis and reporting',\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': 'Implemented data pipeline integration using Snowflake  To improve data accessibility and availability for analysis and reporting for Firstbase.io',\n",
       "  'corrected_bullet': 'Mplemented data pipeline integration using Snowflake to improve data accessibility and availability for analysis and reporting for Firstbase.io.'},\n",
       " {'user_id': '',\n",
       "  'what': 'internal web app',\n",
       "  'verb': 'Integrated',\n",
       "  'skills': '',\n",
       "  'why': 'Increase efficiency and productivity',\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': 'Integrated internal web app using JavaScript and Retool  Increase efficiency and productivity for Firstbase.io',\n",
       "  'corrected_bullet': 'Ntegrated an internal web app using JavaScript and Retool to increase efficiency and productivity for Firstbase.io.'},\n",
       " {'user_id': '',\n",
       "  'what': 'statistical samples size studies',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'Increase efficiency and productivity',\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': 'Created statistical samples size studies using JavaScript and Retool  Increase efficiency and productivity for Firstbase.io',\n",
       "  'corrected_bullet': 'Created statistical sample size studies using JavaScript and Retool to increase efficiency and productivity for Firstbase.io.'},\n",
       " {'user_id': '',\n",
       "  'what': 'internal web app',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'Increase efficiency and productivity',\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': 'Created internal web app using JavaScript and Retool  Increase efficiency and productivity for Firstbase.io',\n",
       "  'corrected_bullet': 'Created an internal web app using JavaScript and Retool to increase efficiency and productivity for Firstbase.io.'},\n",
       " {'user_id': '',\n",
       "  'what': 'managerial  dashboard',\n",
       "  'verb': 'Experienced',\n",
       "  'skills': '',\n",
       "  'why': 'To track key performance indicators',\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': 'Experienced managerial  dashboard using Metabase  To track key performance indicators for Firstbase.io',\n",
       "  'corrected_bullet': 'Experienced managing a dashboard using Metabase to track key performance indicators for Firstbase.io.'},\n",
       " {'user_id': '',\n",
       "  'what': 'dashboards',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To track key performance indicators',\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': 'Built dashboards using Metabase  To track key performance indicators for Firstbase.io',\n",
       "  'corrected_bullet': 'Built dashboards using Metabase to track key performance indicators for Firstbase.io.'},\n",
       " {'user_id': '',\n",
       "  'what': 'asset operations processes',\n",
       "  'verb': 'Mapped',\n",
       "  'skills': '',\n",
       "  'why': 'To identify inefficiencies and areas for improvement',\n",
       "  'how': '',\n",
       "  'whom': 'Loft',\n",
       "  'raw_bullet': 'Mapped asset operations processes using Miro  To identify inefficiencies and areas for improvement for Loft',\n",
       "  'corrected_bullet': 'Mapped asset operations processes using Miro to identify inefficiencies and areas for improvement for Loft.'},\n",
       " {'user_id': '',\n",
       "  'what': 'several  tools',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To streamline processes and increase efficiency',\n",
       "  'how': '',\n",
       "  'whom': 'Loft',\n",
       "  'raw_bullet': 'Built several  tools using Python  To streamline processes and increase efficiency for Loft',\n",
       "  'corrected_bullet': 'Built several tools using Python to streamline processes and increase efficiency for Loft.'},\n",
       " {'user_id': '',\n",
       "  'what': 'reverse ETL',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To streamline processes and increase efficiency',\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': 'Developed reverse ETL using Python  To streamline processes and increase efficiency for Firstbase.io',\n",
       "  'corrected_bullet': 'Developed reverse ETL using Python to streamline processes and increase efficiency for Firstbase.io.'},\n",
       " {'user_id': '',\n",
       "  'what': 'reverse ETL',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'enrich CRM data',\n",
       "  'how': '',\n",
       "  'whom': 'Firstbase.io',\n",
       "  'raw_bullet': 'Developed reverse ETL using Python  enrich CRM data for Firstbase.io',\n",
       "  'corrected_bullet': 'Developed reverse ETL using Python to enrich CRM data for Firstbase.io.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': '',\n",
       "  'why': 'To improve data accessibility and availability for analysis and decision-making',\n",
       "  'how': '',\n",
       "  'whom': 'Loft',\n",
       "  'raw_bullet': 'Implemented data pipeline using Python  To improve data accessibility and availability for analysis and decision-making for Loft',\n",
       "  'corrected_bullet': 'Mplemented a data pipeline using Python to improve data accessibility and availability for analysis and decision-making for Loft.'},\n",
       " {'user_id': '',\n",
       "  'what': 'experiment tracking',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To make informed decisions based on data',\n",
       "  'how': '',\n",
       "  'whom': 'Loft',\n",
       "  'raw_bullet': 'Created experiment tracking using Python and Data  To make informed decisions based on data for Loft',\n",
       "  'corrected_bullet': 'Created experiment tracking using Python and Data to make informed decisions based on data for Loft.'},\n",
       " {'user_id': '',\n",
       "  'what': 'experiment tracking',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To make informed decisions based on data',\n",
       "  'how': 'testing hypotheses and writing documentation',\n",
       "  'whom': 'Loft',\n",
       "  'raw_bullet': 'Created experiment tracking using Python and Data leveraging ontesting hypotheses and writing documentation To make informed decisions based on data for Loft',\n",
       "  'corrected_bullet': 'Created experiment tracking using Python and data leveraging on testing hypotheses and writing documentation to make informed decisions based on data for Loft.'},\n",
       " {'user_id': '',\n",
       "  'what': 'scoring system',\n",
       "  'verb': 'Improved',\n",
       "  'skills': '',\n",
       "  'why': \"To provide a more accurate evaluation of Loft's unsold properties\",\n",
       "  'how': '',\n",
       "  'whom': 'Loft',\n",
       "  'raw_bullet': \"Improved scoring system using Python  To provide a more accurate evaluation of Loft's unsold properties for Loft\",\n",
       "  'corrected_bullet': \"Mproved scoring system using Python to provide a more accurate evaluation of Loft's unsold properties for Loft.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis reports',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To track and monitor business performance',\n",
       "  'how': '',\n",
       "  'whom': 'VTEX',\n",
       "  'raw_bullet': 'Created data analysis reports using Powerbi and Excel  To track and monitor business performance for VTEX',\n",
       "  'corrected_bullet': 'Created data analysis reports using PowerBI and Excel to track and monitor business performance for VTEX.'},\n",
       " {'user_id': '',\n",
       "  'what': 'managerial  dashboard',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To measure software development practices, being used as a performance evaluation metric for managers and their employees',\n",
       "  'how': '',\n",
       "  'whom': 'banking company, the 5th largest in Brazil',\n",
       "  'raw_bullet': 'Created managerial  dashboard using Sql and Tableau  To measure software development practices, being used as a performance evaluation metric for managers and their employees for banking company, the 5th largest in Brazil',\n",
       "  'corrected_bullet': 'Created a managerial dashboard using SQL and Tableau to measure software development practices. It was used as a performance evaluation metric for managers and their employees at a banking company, the 5th largest in Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data lake',\n",
       "  'verb': 'Managed',\n",
       "  'skills': '',\n",
       "  'why': 'To measure software development practices, being used as a performance evaluation metric for managers and their employees',\n",
       "  'how': '',\n",
       "  'whom': 'banking company, the 5th largest in Brazil',\n",
       "  'raw_bullet': 'Managed data lake using Sql, Hadoop and Spark  To measure software development practices, being used as a performance evaluation metric for managers and their employees for banking company, the 5th largest in Brazil',\n",
       "  'corrected_bullet': 'Managed data lake using SQL, Hadoop, and Spark to measure software development practices. It was used as a performance evaluation metric for managers and their employees at a banking company, the 5th largest in Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data lake',\n",
       "  'verb': 'Managed',\n",
       "  'skills': '',\n",
       "  'why': 'To enable faster data processing and analysis',\n",
       "  'how': '',\n",
       "  'whom': 'banking company, the 5th largest in Brazil',\n",
       "  'raw_bullet': 'Managed data lake using Sql, Hadoop and Spark  To enable faster data processing and analysis for banking company, the 5th largest in Brazil',\n",
       "  'corrected_bullet': 'Managed data lake using SQL, Hadoop, and Spark to enable faster data processing and analysis for a banking company, the 5th largest in Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'multiple  dashboards',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To enable faster data processing and analysis',\n",
       "  'how': '',\n",
       "  'whom': 'banking company, the 5th largest in Brazil',\n",
       "  'raw_bullet': 'Developed multiple  dashboards using Kibana, Elasticsearch and Javascript  To enable faster data processing and analysis for banking company, the 5th largest in Brazil',\n",
       "  'corrected_bullet': 'Developed multiple dashboards using Kibana, Elasticsearch, and Javascript to enable faster data processing and analysis for a banking company, the 5th largest in Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'several data pipelines',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To handle large volumes of data',\n",
       "  'how': 'From ',\n",
       "  'whom': 'banking company, the 5th largest in Brazil',\n",
       "  'raw_bullet': 'Built several data pipelines using Alteryx, Python, Java and SQL leveraging onFrom  To handle large volumes of data for banking company, the 5th largest in Brazil',\n",
       "  'corrected_bullet': 'Built several data pipelines using Alteryx, Python, Java, and SQL to handle large volumes of data for a banking company, the 5th largest in Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'several data pipelines',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To handle large volumes of data',\n",
       "  'how': 'SQLServer',\n",
       "  'whom': 'banking company, the 5th largest in Brazil',\n",
       "  'raw_bullet': 'Built several data pipelines using Alteryx, Python, Java and SQL leveraging onSQLServer To handle large volumes of data for banking company, the 5th largest in Brazil',\n",
       "  'corrected_bullet': 'Built several data pipelines using Alteryx, Python, Java, and SQL, leveraging on SQL Server to handle large volumes of data for a banking company, the 5th largest in Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data infrastructure',\n",
       "  'verb': 'Supported',\n",
       "  'skills': '',\n",
       "  'why': 'To handle large volumes of data',\n",
       "  'how': 'SQLServer',\n",
       "  'whom': 'banking company, the 5th largest in Brazil',\n",
       "  'raw_bullet': 'Supported data infrastructure using  and undefined leveraging onSQLServer To handle large volumes of data for banking company, the 5th largest in Brazil',\n",
       "  'corrected_bullet': 'Supported data infrastructure using and leveraging on SQL Server to handle large volumes of data for a banking company, the 5th largest in Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'infrastructure business applications',\n",
       "  'verb': 'Supported',\n",
       "  'skills': '',\n",
       "  'why': 'To ensure smooth and efficient operation of business processes',\n",
       "  'how': '',\n",
       "  'whom': 'banking company, the 5th largest in Brazil',\n",
       "  'raw_bullet': 'Supported infrastructure business applications using Linux and Hadoop  To ensure smooth and efficient operation of business processes for banking company, the 5th largest in Brazil',\n",
       "  'corrected_bullet': 'Supported infrastructure business applications using Linux and Hadoop to ensure the smooth and efficient operation of business processes for a banking company, the 5th largest in Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'administrative  software',\n",
       "  'verb': 'Provided',\n",
       "  'skills': '',\n",
       "  'why': 'To improve efficiency and productivity within the company',\n",
       "  'how': '',\n",
       "  'whom': 'banking company, the 5th largest in Brazil',\n",
       "  'raw_bullet': 'Provided administrative  software using Linux and Bash  To improve efficiency and productivity within the company for banking company, the 5th largest in Brazil',\n",
       "  'corrected_bullet': 'Provided administrative software using Linux and Bash to improve efficiency and productivity within the company for a banking company, the 5th largest in Brazil.'},\n",
       " {'user_id': '',\n",
       "  'what': 'microservices web application',\n",
       "  'verb': 'Reacted',\n",
       "  'skills': '',\n",
       "  'why': 'Improve scalability and flexibility',\n",
       "  'how': 'Openshift',\n",
       "  'whom': 'Itau',\n",
       "  'raw_bullet': 'Reacted microservices web application using Angular, Cassandra, SQLServer and Kafka leveraging onOpenshift Improve scalability and flexibility for Itau',\n",
       "  'corrected_bullet': 'Reacted microservices web application using Angular, Cassandra, SQLServer, and Kafka leveraging on Openshift to improve scalability and flexibility for Itau.'},\n",
       " {'user_id': '',\n",
       "  'what': 'etl solutions',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance data security and compliance through centralized data management and access controls',\n",
       "  'how': 'Openshift',\n",
       "  'whom': 'Itau',\n",
       "  'raw_bullet': 'Built etl solutions using Angular, Cassandra, SQLServer and Kafka leveraging onOpenshift To enhance data security and compliance through centralized data management and access controls for Itau',\n",
       "  'corrected_bullet': 'Built ETL solutions using Angular, Cassandra, SQL Server, and Kafka, leveraging on Openshift to enhance data security and compliance through centralized data management and access controls for Itau.'},\n",
       " {'user_id': '',\n",
       "  'what': 'management plan',\n",
       "  'verb': 'Changed',\n",
       "  'skills': '',\n",
       "  'why': 'To align with new goals and objectives',\n",
       "  'how': '',\n",
       "  'whom': 'Itau',\n",
       "  'raw_bullet': 'Changed management plan using Angular, Cassandra, SQLServer and Kafka  To align with new goals and objectives for Itau',\n",
       "  'corrected_bullet': 'Changed the management plan using Angular, Cassandra, SQLServer, and Kafka to align with new goals and objectives for Itau.'},\n",
       " {'user_id': '',\n",
       "  'what': 'business strategy',\n",
       "  'verb': 'Defined',\n",
       "  'skills': '',\n",
       "  'why': 'To measure and track progress towards objectives',\n",
       "  'how': '',\n",
       "  'whom': 'Itau',\n",
       "  'raw_bullet': 'Defined business strategy using Angular, Cassandra, SQLServer and Kafka  To measure and track progress towards objectives for Itau',\n",
       "  'corrected_bullet': 'Defined business strategy using Angular, Cassandra, SQLServer, and Kafka to measure and track progress towards objectives for Itau.'},\n",
       " {'user_id': '',\n",
       "  'what': 'business strategy',\n",
       "  'verb': 'Defined',\n",
       "  'skills': '',\n",
       "  'why': 'To measure and track progress towards objectives',\n",
       "  'how': '',\n",
       "  'whom': 'Itau',\n",
       "  'raw_bullet': 'Defined business strategy using Agile, Sql and Tableau  To measure and track progress towards objectives for Itau',\n",
       "  'corrected_bullet': 'Defined business strategy using Agile, SQL, and Tableau to measure and track progress towards objectives for Itau.'},\n",
       " {'user_id': '',\n",
       "  'what': 'business strategy',\n",
       "  'verb': 'Defined',\n",
       "  'skills': '',\n",
       "  'why': 'To align all employees towards common goals',\n",
       "  'how': 'SQLServer and Tableau',\n",
       "  'whom': 'Itau',\n",
       "  'raw_bullet': 'Defined business strategy using Agile leveraging onSQLServer and Tableau To align all employees towards common goals for Itau',\n",
       "  'corrected_bullet': 'Defined business strategy using Agile, leveraging on SQL Server and Tableau to align all employees towards common goals for Itau.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Designed',\n",
       "  'skills': '',\n",
       "  'why': 'To automate data processing tasks',\n",
       "  'how': '',\n",
       "  'whom': 'Itau',\n",
       "  'raw_bullet': 'Designed data pipeline using Sql and Tableau  To automate data processing tasks for Itau',\n",
       "  'corrected_bullet': 'Designed data pipeline using SQL and Tableau to automate data processing tasks for Itau.'},\n",
       " {'user_id': '',\n",
       "  'what': 'managerial  dashboard',\n",
       "  'verb': 'Designed',\n",
       "  'skills': '',\n",
       "  'why': 'To track key performance indicators and metrics in one centralized location',\n",
       "  'how': '',\n",
       "  'whom': 'Itau',\n",
       "  'raw_bullet': 'Designed managerial  dashboard using Tableau  To track key performance indicators and metrics in one centralized location for Itau',\n",
       "  'corrected_bullet': 'Designed a managerial dashboard using Tableau to track key performance indicators and metrics in one centralized location for Itau.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis reports',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To improve operational efficiency and optimize processes',\n",
       "  'how': '',\n",
       "  'whom': 'Itau',\n",
       "  'raw_bullet': 'Developed data analysis reports using Excel, Sql, Python and Powerbi  To improve operational efficiency and optimize processes for Itau',\n",
       "  'corrected_bullet': 'Developed data analysis reports using Excel, SQL, Python, and PowerBI to improve operational efficiency and optimize processes for Itau.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data processing pipeline',\n",
       "  'verb': 'Azured',\n",
       "  'skills': '',\n",
       "  'why': 'To improve operational efficiency and optimize processes',\n",
       "  'how': '',\n",
       "  'whom': 'Itau',\n",
       "  'raw_bullet': 'Azured data processing pipeline using Excel, Sql, Python and Powerbi  To improve operational efficiency and optimize processes for Itau',\n",
       "  'corrected_bullet': 'Azured data processing pipeline was used to improve operational efficiency and optimize processes for Itau, utilizing Excel, SQL, Python, and PowerBI.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': '',\n",
       "  'why': 'To automate data collection and processing',\n",
       "  'how': '',\n",
       "  'whom': 'Itau',\n",
       "  'raw_bullet': 'Implemented data pipeline using Excel, Sql, Python and Powerbi  To automate data collection and processing for Itau',\n",
       "  'corrected_bullet': 'Mplemented a data pipeline using Excel, SQL, Python, and PowerBI to automate data collection and processing for Itau.'},\n",
       " {'user_id': '',\n",
       "  'what': 'marketing campaign strategies',\n",
       "  'verb': 'Supported',\n",
       "  'skills': '',\n",
       "  'why': 'To increase brand awareness',\n",
       "  'how': 'by reaching stakeholders',\n",
       "  'whom': 'for an ads agency',\n",
       "  'raw_bullet': 'Supported marketing campaign strategies using Tableau and Sql leveraging onby reaching stakeholders To increase brand awareness for for an ads agency',\n",
       "  'corrected_bullet': 'Supported marketing campaign strategies using Tableau and SQL to leverage reaching stakeholders in order to increase brand awareness for an advertising agency.'},\n",
       " {'user_id': '',\n",
       "  'what': 'marketing campaign strategies',\n",
       "  'verb': 'Supported',\n",
       "  'skills': '',\n",
       "  'why': 'To increase brand awareness and increased sales by 20%',\n",
       "  'how': 'by reaching stakeholders',\n",
       "  'whom': 'for an ads agency',\n",
       "  'raw_bullet': 'Supported marketing campaign strategies using Tableau and Sql leveraging onby reaching stakeholders To increase brand awareness and increased sales by 20% for for an ads agency',\n",
       "  'corrected_bullet': 'Supported marketing campaign strategies using Tableau and SQL to leverage reaching stakeholders in order to increase brand awareness and sales by 20% for an advertising agency.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Led the implementation of data pipelines supporting OpenFinance',\n",
       "  'verb': 'integrating sources like SQL Server, Oracle DB, and APIs',\n",
       "  'skills': '',\n",
       "  'why': 'Enhance data quality and consistency across different sources',\n",
       "  'how': '',\n",
       "  'whom': 'Investiment Fintech',\n",
       "  'raw_bullet': 'integrating sources like SQL Server, Oracle DB, and APIs Led the implementation of data pipelines supporting OpenFinance using Airflow, Sql and Python  Enhance data quality and consistency across different sources for Investiment Fintech',\n",
       "  'corrected_bullet': 'Led the implementation of data pipelines supporting OpenFinance using Airflow, SQL, and Python, integrating sources like SQL Server, Oracle DB, and APIs. Enhanced data quality and consistency across different sources for Investment Fintech.'},\n",
       " {'user_id': '',\n",
       "  'what': 'deep learning models',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance customer experience and improve baby safety',\n",
       "  'how': 'Head pose estimation deep learning model, pet interaction machine learning model',\n",
       "  'whom': 'eletronics multinational company',\n",
       "  'raw_bullet': 'Developed deep learning models using Python, Pytorch and OpenCV leveraging onHead pose estimation deep learning model, pet interaction machine learning model To enhance customer experience and improve baby safety for eletronics multinational company',\n",
       "  'corrected_bullet': 'Developed deep learning models using Python, Pytorch, and OpenCV, leveraging on head pose estimation deep learning model and pet interaction machine learning model to enhance customer experience and improve baby safety for an electronics multinational company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'deep learning models',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance customer experience and improve baby safety',\n",
       "  'how': 'Head pose estimation deep learning model, pet interaction machine learning model',\n",
       "  'whom': 'eletronics multinational company',\n",
       "  'raw_bullet': 'Developed deep learning models using Python, Pytorch, OpenCV and Scikit-learn leveraging onHead pose estimation deep learning model, pet interaction machine learning model To enhance customer experience and improve baby safety for eletronics multinational company',\n",
       "  'corrected_bullet': 'Developed deep learning models using Python, Pytorch, OpenCV, and Scikit-learn leveraging on Head pose estimation deep learning model, pet interaction machine learning model to enhance customer experience and improve baby safety for an electronics multinational company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'deep learning models',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance customer experience and improve baby safety',\n",
       "  'how': 'SixDRepNet deep neural network, Random Forest Classifier',\n",
       "  'whom': 'eletronics multinational company',\n",
       "  'raw_bullet': 'Developed deep learning models using Python, Pytorch, OpenCV and Scikit-learn leveraging onSixDRepNet deep neural network, Random Forest Classifier To enhance customer experience and improve baby safety for eletronics multinational company',\n",
       "  'corrected_bullet': 'Developed deep learning models using Python, Pytorch, OpenCV, and Scikit-learn leveraging on SixDRepNet deep neural network, Random Forest Classifier to enhance customer experience and improve baby safety for an electronics multinational company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'deep learning models',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance customer experience and improve baby safety',\n",
       "  'how': 'Head pose estimation deep learning model, pet interaction machine learning model',\n",
       "  'whom': 'eletronics multinational company',\n",
       "  'raw_bullet': 'Developed deep learning models using Python, Pytorch, OpenCV and Scikit-learn leveraging onHead pose estimation deep learning model, pet interaction machine learning model To enhance customer experience and improve baby safety for eletronics multinational company',\n",
       "  'corrected_bullet': 'Developed deep learning models using Python, Pytorch, OpenCV, and Scikit-learn leveraging on Head pose estimation deep learning model, pet interaction machine learning model to enhance customer experience and improve baby safety for an electronics multinational company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'deep learning models',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance customer experience and improve baby safety',\n",
       "  'how': 'Head pose estimation deep learning model, pet interaction machine learning model',\n",
       "  'whom': 'eletronics multinational company',\n",
       "  'raw_bullet': 'Developed deep learning models using Python, Pytorch, OpenCV, Scikit-learn, ONNX and Quantization leveraging onHead pose estimation deep learning model, pet interaction machine learning model To enhance customer experience and improve baby safety for eletronics multinational company',\n",
       "  'corrected_bullet': 'Developed deep learning models using Python, Pytorch, OpenCV, Scikit-learn, ONNX, and Quantization leveraging on head pose estimation deep learning model, pet interaction machine learning model to enhance customer experience and improve baby safety for an electronics multinational company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'deep learning models',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance customer experience and improve baby safety',\n",
       "  'how': 'Head pose estimation deep learning model, pet interaction machine learning model',\n",
       "  'whom': 'eletronics multinational company',\n",
       "  'raw_bullet': 'Developed deep learning models using Python, Pytorch, Scikit-learn, ONNX, Quantization and Computer Vision leveraging onHead pose estimation deep learning model, pet interaction machine learning model To enhance customer experience and improve baby safety for eletronics multinational company',\n",
       "  'corrected_bullet': 'Developed deep learning models using Python, Pytorch, Scikit-learn, ONNX, Quantization, and Computer Vision, leveraging on head pose estimation deep learning model, pet interaction machine learning model to enhance customer experience and improve baby safety for an electronics multinational company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Machine learning automatic training pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To increase efficiency and productivity',\n",
       "  'how': 'Airflow jobs, MLFlow model registry and experiment tracking',\n",
       "  'whom': 'research institute',\n",
       "  'raw_bullet': 'Built Machine learning automatic training pipeline using Python, Scikit-learn, Airflow and MLFlow leveraging onAirflow jobs, MLFlow model registry and experiment tracking To increase efficiency and productivity for research institute',\n",
       "  'corrected_bullet': 'Built a machine learning automatic training pipeline using Python, Scikit-learn, Airflow, and MLFlow, leveraging on Airflow jobs, MLFlow model registry, and experiment tracking to increase efficiency and productivity for a research institute.'},\n",
       " {'user_id': '',\n",
       "  'what': 'CI/CD pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To increase efficiency and productivity',\n",
       "  'how': 'Azure pipelines',\n",
       "  'whom': 'research institute',\n",
       "  'raw_bullet': 'Built CI/CD pipeline using Azure and CI/CD leveraging onAzure pipelines To increase efficiency and productivity for research institute',\n",
       "  'corrected_bullet': 'Built CI/CD pipeline using Azure and CI/CD leveraging on Azure pipelines to increase efficiency and productivity for a research institute.'},\n",
       " {'user_id': '',\n",
       "  'what': 'CI/CD pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To increase efficiency and productivity',\n",
       "  'how': 'Azure pipelines',\n",
       "  'whom': 'research institute',\n",
       "  'raw_bullet': 'Built CI/CD pipeline using Azure, CI/CD and Docker leveraging onAzure pipelines To increase efficiency and productivity for research institute',\n",
       "  'corrected_bullet': 'Built CI/CD pipeline using Azure, CI/CD, and Docker, leveraging Azure pipelines to increase efficiency and productivity for a research institute.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Data Analysis platform',\n",
       "  'verb': 'Designed and built',\n",
       "  'skills': '',\n",
       "  'why': 'To increase efficiency and productivity',\n",
       "  'how': '',\n",
       "  'whom': 'Multinational car company',\n",
       "  'raw_bullet': 'Designed and built Data Analysis platform using React, JavaScript, Python, Django, GCP, Docker and CI/CD  To increase efficiency and productivity for Multinational car company',\n",
       "  'corrected_bullet': 'Designed and built a Data Analysis platform using React, JavaScript, Python, Django, GCP, Docker, and CI/CD to increase efficiency and productivity for a Multinational car company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Data Analysis platform',\n",
       "  'verb': 'Designed and built',\n",
       "  'skills': '',\n",
       "  'why': 'To increase efficiency and productivity',\n",
       "  'how': '',\n",
       "  'whom': 'multinational car company',\n",
       "  'raw_bullet': 'Designed and built Data Analysis platform using React, JavaScript, Python, Django, GCP, Docker and CI/CD  To increase efficiency and productivity for multinational car company',\n",
       "  'corrected_bullet': 'Designed and built a Data Analysis platform using React, JavaScript, Python, Django, GCP, Docker, and CI/CD to increase efficiency and productivity for a multinational car company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Data Analysis platform',\n",
       "  'verb': 'Designed and built',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance the overall performance and competitiveness of the company',\n",
       "  'how': '',\n",
       "  'whom': 'multinational car company',\n",
       "  'raw_bullet': 'Designed and built Data Analysis platform using React, JavaScript, Python, Django, GCP, Docker and CI/CD  To enhance the overall performance and competitiveness of the company for multinational car company',\n",
       "  'corrected_bullet': 'Designed and built a Data Analysis platform using React, JavaScript, Python, Django, GCP, Docker, and CI/CD to enhance the overall performance and competitiveness of the company for a multinational car company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Data Analysis platform',\n",
       "  'verb': 'Designed and built',\n",
       "  'skills': '',\n",
       "  'why': 'To make data-driven decisions',\n",
       "  'how': '',\n",
       "  'whom': 'multinational car company',\n",
       "  'raw_bullet': 'Designed and built Data Analysis platform using React, JavaScript, Python, Django, GCP, Docker and CI/CD  To make data-driven decisions for multinational car company',\n",
       "  'corrected_bullet': 'Designed and built a Data Analysis platform using React, JavaScript, Python, Django, GCP, Docker, and CI/CD to make data-driven decisions for a multinational car company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'mlops platform',\n",
       "  'verb': 'Designed',\n",
       "  'skills': '',\n",
       "  'why': 'To streamline and automate the deployment of machine learning models',\n",
       "  'how': 'Lambda, Sagemaker Endpoint, GitLab CI',\n",
       "  'whom': 'financial industry company',\n",
       "  'raw_bullet': 'Designed mlops platform using Aws leveraging onLambda, Sagemaker Endpoint, GitLab CI To streamline and automate the deployment of machine learning models for financial industry company',\n",
       "  'corrected_bullet': 'Designed mlops platform using AWS, leveraging Lambda, Sagemaker Endpoint, and GitLab CI to streamline and automate the deployment of machine learning models for a financial industry company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'mlops platform',\n",
       "  'verb': 'Designed',\n",
       "  'skills': '',\n",
       "  'why': 'To streamline and automate the deployment of machine learning models',\n",
       "  'how': 'Lambda, Sagemaker Endpoint, AWS ECR, GitLab CI',\n",
       "  'whom': 'financial industry company',\n",
       "  'raw_bullet': 'Designed mlops platform using Aws leveraging onLambda, Sagemaker Endpoint, AWS ECR, GitLab CI To streamline and automate the deployment of machine learning models for financial industry company',\n",
       "  'corrected_bullet': 'Designed mlops platform using AWS, leveraging Lambda, Sagemaker Endpoint, AWS ECR, and GitLab CI to streamline and automate the deployment of machine learning models for a financial industry company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'mlops platform',\n",
       "  'verb': 'Designed',\n",
       "  'skills': '',\n",
       "  'why': 'To streamline and automate the deployment of machine learning models',\n",
       "  'how': 'Lambda, Sagemaker Endpoint, AWS ECR',\n",
       "  'whom': 'financial industry company',\n",
       "  'raw_bullet': 'Designed mlops platform using Aws leveraging onLambda, Sagemaker Endpoint, AWS ECR To streamline and automate the deployment of machine learning models for financial industry company',\n",
       "  'corrected_bullet': 'Designed mlops platform using AWS, leveraging Lambda, Sagemaker Endpoint, and AWS ECR to streamline and automate the deployment of machine learning models for a financial industry company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'mlops system',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To improve the efficiency and effectiveness of machine learning projects',\n",
       "  'how': 'Terraform for creating the cloud infrastructure, Kubeflow for automatic training pipeline, Vertex AI for experimenting, tracking, deploying and monitoring the machine learning models.',\n",
       "  'whom': 'multinational car company',\n",
       "  'raw_bullet': 'Built mlops system using Python, GCP, Vertex AI, Kubeflow, FastAPI, Terraform, CI/CD and Docker leveraging onTerraform for creating the cloud infrastructure, Kubeflow for automatic training pipeline, Vertex AI for experimenting, tracking, deploying and monitoring the machine learning models. To improve the efficiency and effectiveness of machine learning projects for multinational car company',\n",
       "  'corrected_bullet': 'Built an MLOps system using Python, GCP, Vertex AI, Kubeflow, FastAPI, Terraform, CI/CD, and Docker, leveraging Terraform for creating the cloud infrastructure, Kubeflow for automatic training pipeline, Vertex AI for experimenting, tracking, deploying, and monitoring the machine learning models. This was done to improve the efficiency and effectiveness of machine learning projects for a multinational car company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'mlops system',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To improve the efficiency and effectiveness of machine learning projects',\n",
       "  'how': 'Terraform for creating the cloud infrastructure, Kubeflow for automatic training pipeline, Vertex AI for experimenting, tracking, deploying and monitoring the machine learning models and FastAPI to communicate with the machine learning model..',\n",
       "  'whom': 'multinational car company',\n",
       "  'raw_bullet': 'Built mlops system using Python, GCP, Vertex AI, Kubeflow, FastAPI, Terraform, CI/CD and Docker leveraging onTerraform for creating the cloud infrastructure, Kubeflow for automatic training pipeline, Vertex AI for experimenting, tracking, deploying and monitoring the machine learning models and FastAPI to communicate with the machine learning model.. To improve the efficiency and effectiveness of machine learning projects for multinational car company',\n",
       "  'corrected_bullet': 'Built mlops system using Python, GCP, Vertex AI, Kubeflow, FastAPI, Terraform, CI/CD, and Docker leveraging on Terraform for creating the cloud infrastructure, Kubeflow for automatic training pipeline, Vertex AI for experimenting, tracking, deploying, and monitoring the machine learning models, and FastAPI to communicate with the machine learning model. This was done to improve the efficiency and effectiveness of machine learning projects for a multinational car company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'mlops system',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To improve the efficiency and effectiveness of machine learning projects',\n",
       "  'how': 'Terraform for creating the cloud infrastructure, Kubeflow for automatic training pipeline, Vertex AI for experimenting, tracking, deploying and monitoring the machine learning models and FastAPI to communicate with deployed models.',\n",
       "  'whom': 'multinational car company',\n",
       "  'raw_bullet': 'Built mlops system using Python, GCP, Vertex AI, Kubeflow, FastAPI, Terraform, CI/CD and Docker leveraging onTerraform for creating the cloud infrastructure, Kubeflow for automatic training pipeline, Vertex AI for experimenting, tracking, deploying and monitoring the machine learning models and FastAPI to communicate with deployed models. To improve the efficiency and effectiveness of machine learning projects for multinational car company',\n",
       "  'corrected_bullet': 'Built an MLOps system using Python, GCP, Vertex AI, Kubeflow, FastAPI, Terraform, CI/CD, and Docker, leveraging Terraform for creating the cloud infrastructure, Kubeflow for automatic training pipeline, Vertex AI for experimenting, tracking, deploying, and monitoring the machine learning models, and FastAPI to communicate with deployed models. This was done to improve the efficiency and effectiveness of machine learning projects for a multinational car company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis reports',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': \"To identify impact of rupture on company's revenue\",\n",
       "  'how': '',\n",
       "  'whom': 'Retail company',\n",
       "  'raw_bullet': \"Developed data analysis reports using Python and SQL  To identify impact of rupture on company's revenue for Retail company\",\n",
       "  'corrected_bullet': \"Developed data analysis reports using Python and SQL to identify the impact of rupture on the company's revenue for a retail company.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'automatic training and deployment pipeline of linear regression model',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To ensure consistency and reproducibility in the model building process',\n",
       "  'how': '',\n",
       "  'whom': 'Retailing company',\n",
       "  'raw_bullet': 'Built automatic training and deployment pipeline of linear regression model using Python, MLFlow, Airflow and Sagemaker Studio  To ensure consistency and reproducibility in the model building process for Retailing company',\n",
       "  'corrected_bullet': 'Built an automatic training and deployment pipeline of a linear regression model using Python, MLFlow, Airflow, and Sagemaker Studio to ensure consistency and reproducibility in the model building process for a Retailing company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis platform',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To make data-driven decisions about players health',\n",
       "  'how': '',\n",
       "  'whom': 'Soccer club',\n",
       "  'raw_bullet': 'Developed data analysis platform using Python, Streamlit, HTML, CSS, Docker, Heroku and Scikit-learn  To make data-driven decisions about players health for Soccer club',\n",
       "  'corrected_bullet': \"Developed a data analysis platform using Python, Streamlit, HTML, CSS, Docker, Heroku, and Scikit-learn to make data-driven decisions about players' health for a soccer club.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'machine learning system',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': '',\n",
       "  'why': 'to build a platform for deforestation detection',\n",
       "  'how': '',\n",
       "  'whom': 'multinational telecommunications company',\n",
       "  'raw_bullet': 'Implemented machine learning system using Python, Apache Nifi, Huawei Cloud, Computer Vision, Tensorflow, Docker Compose, Hadoop and GCP  to build a platform for deforestation detection for multinational telecommunications company',\n",
       "  'corrected_bullet': 'Mplemented a machine learning system using Python, Apache Nifi, Huawei Cloud, Computer Vision, Tensorflow, Docker Compose, Hadoop, and GCP to build a platform for deforestation detection for a multinational telecommunications company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'out-of-stock prediction model',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To improve customer satisfaction by ensuring products are always available',\n",
       "  'how': 'The model is an ensemble of different models (unsupervise, supervise and human-input signals) that laverages information of sales, shipments, donations, and item visibility to make the decisions',\n",
       "  'whom': 'retail industry',\n",
       "  'raw_bullet': 'Built out-of-stock prediction model using Python, Sql, Pandas, Scikit-learn, MLflow and Snowflake leveraging onThe model is an ensemble of different models (unsupervise, supervise and human-input signals) that laverages information of sales, shipments, donations, and item visibility to make the decisions To improve customer satisfaction by ensuring products are always available for retail industry',\n",
       "  'corrected_bullet': 'Built an out-of-stock prediction model using Python, SQL, Pandas, Scikit-learn, MLflow, and Snowflake. The model is an ensemble of different models (unsupervised, supervised, and human-input signals) that leverages information on sales, shipments, donations, and item visibility to make decisions. The goal is to improve customer satisfaction by ensuring products are always available in the retail industry.'},\n",
       " {'user_id': '',\n",
       "  'what': 'out-of-stock prediction model',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To improve customer satisfaction by ensuring products are always available',\n",
       "  'how': 'The model is an ensemble of different models (unsupervise, supervise and human-input signals) that laverages information of sales, shipments, donations, and item visibility to make the decisions',\n",
       "  'whom': 'USA retailer',\n",
       "  'raw_bullet': 'Built out-of-stock prediction model using Python, Sql, Pandas, Scikit-learn, MLflow and Snowflake leveraging onThe model is an ensemble of different models (unsupervise, supervise and human-input signals) that laverages information of sales, shipments, donations, and item visibility to make the decisions To improve customer satisfaction by ensuring products are always available for USA retailer',\n",
       "  'corrected_bullet': 'Built an out-of-stock prediction model using Python, SQL, Pandas, Scikit-learn, MLflow, and Snowflake. The model is an ensemble of different models (unsupervised, supervised, and human-input signals) that leverages information on sales, shipments, donations, and item visibility to make decisions. The goal is to improve customer satisfaction by ensuring products are always available for a USA retailer.'},\n",
       " {'user_id': '',\n",
       "  'what': 'backend',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'to build a nats messaging system',\n",
       "  'how': '',\n",
       "  'whom': 'United Kingdom Airspace',\n",
       "  'raw_bullet': 'Developed backend using C++  to build a nats messaging system for United Kingdom Airspace',\n",
       "  'corrected_bullet': 'Developed backend using C++ to build a NATS messaging system for United Kingdom Airspace.'},\n",
       " {'user_id': '',\n",
       "  'what': 'simulation tool',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To simulate different scenarios and predict outcomes',\n",
       "  'how': '',\n",
       "  'whom': 'united kingdom airspace',\n",
       "  'raw_bullet': 'Built simulation tool using c++  To simulate different scenarios and predict outcomes for united kingdom airspace',\n",
       "  'corrected_bullet': 'Built a simulation tool using C++ to simulate different scenarios and predict outcomes for United Kingdom airspace.'},\n",
       " {'user_id': '',\n",
       "  'what': 'web management system',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To streamline and automate website maintenance processes',\n",
       "  'how': '',\n",
       "  'whom': 'for an streaming platform',\n",
       "  'raw_bullet': 'Built web management system using C++ and GCP  To streamline and automate website maintenance processes for for an streaming platform',\n",
       "  'corrected_bullet': 'Built a web management system using C++ and GCP to streamline and automate website maintenance processes for a streaming platform.'},\n",
       " {'user_id': '',\n",
       "  'what': 'pricing model',\n",
       "  'verb': 'Analyzed and developed',\n",
       "  'skills': '',\n",
       "  'why': 'to offer fair pricing to clients based on the value extracted from their purchase',\n",
       "  'how': '',\n",
       "  'whom': 'a leading online job board',\n",
       "  'raw_bullet': 'Analyzed and developed pricing model using Python and language models with embeddings  to offer fair pricing to clients based on the value extracted from their purchase for a leading online job board',\n",
       "  'corrected_bullet': 'Analyzed and developed a pricing model using Python and language models with embeddings to offer fair pricing to clients based on the value extracted from their purchases for a leading online job board.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To improve overall operational efficiency and business intelligence',\n",
       "  'how': '',\n",
       "  'whom': 'uDocz',\n",
       "  'raw_bullet': 'Built data pipeline using Python  To improve overall operational efficiency and business intelligence for uDocz',\n",
       "  'corrected_bullet': 'Built a data pipeline using Python to improve overall operational efficiency and business intelligence for uDocz.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To collect and process data from various sources',\n",
       "  'how': '',\n",
       "  'whom': 'telecom',\n",
       "  'raw_bullet': 'Developed data pipeline using Python, Airflow, Spark and beam  To collect and process data from various sources for telecom',\n",
       "  'corrected_bullet': 'Developed a data pipeline using Python, Airflow, Spark, and Beam to collect and process data from various sources for telecom.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To collect and process data from various sources',\n",
       "  'how': '',\n",
       "  'whom': 'telecom',\n",
       "  'raw_bullet': 'Developed data pipeline using Python, Airflow, Spark and beam  To collect and process data from various sources for telecom',\n",
       "  'corrected_bullet': 'Developed a data pipeline using Python, Airflow, Spark, and Beam to collect and process data from various sources for telecom.'},\n",
       " {'user_id': '',\n",
       "  'what': 'web management system',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To integrate with other systems and tools for seamless operations',\n",
       "  'how': 'xyz',\n",
       "  'whom': 'for a telcos industry',\n",
       "  'raw_bullet': 'Developed web management system using Python, Java, React and Sql leveraging onxyz To integrate with other systems and tools for seamless operations for for a telcos industry',\n",
       "  'corrected_bullet': 'Developed a web management system using Python, Java, React, and SQL, leveraging on XYZ to integrate with other systems and tools for seamless operations in the telcos industry.'},\n",
       " {'user_id': '',\n",
       "  'what': 'back end application',\n",
       "  'verb': 'Supported',\n",
       "  'skills': '',\n",
       "  'why': 'To integrate with other systems and applications',\n",
       "  'how': 'Using debugging tools to catch the errors in real time',\n",
       "  'whom': 'NATS Client',\n",
       "  'raw_bullet': 'Supported back end application using C++, ADA and GDB leveraging onUsing debugging tools to catch the errors in real time To integrate with other systems and applications for NATS Client',\n",
       "  'corrected_bullet': 'Supported back end application using C++, ADA, and GDB, leveraging debugging tools to catch errors in real time and integrate with other systems and applications for NATS Client.'},\n",
       " {'user_id': '',\n",
       "  'what': 'back end application',\n",
       "  'verb': 'Supported',\n",
       "  'skills': '',\n",
       "  'why': 'To enhance user experience and satisfaction',\n",
       "  'how': 'Using debugging tools to catch the errors in real time',\n",
       "  'whom': 'NATS Client',\n",
       "  'raw_bullet': 'Supported back end application using C++, ADA and GDB leveraging onUsing debugging tools to catch the errors in real time To enhance user experience and satisfaction for NATS Client',\n",
       "  'corrected_bullet': 'Supported back end application using C++, ADA, and GDB, leveraging debugging tools to catch errors in real time to enhance user experience and satisfaction for NATS Client.'},\n",
       " {'user_id': '',\n",
       "  'what': 'A wheater message generator',\n",
       "  'verb': 'Built ',\n",
       "  'skills': '',\n",
       "  'why': 'To test current functionality of the message receiver',\n",
       "  'how': '',\n",
       "  'whom': 'internal use',\n",
       "  'raw_bullet': 'Built  A wheater message generator using C++, ADA and MVC  To test current functionality of the message receiver for internal use',\n",
       "  'corrected_bullet': 'Built a weather message generator using C++, ADA, and MVC to test the current functionality of the message receiver for internal use.'},\n",
       " {'user_id': '',\n",
       "  'what': 'New aispace management features',\n",
       "  'verb': 'Implement',\n",
       "  'skills': '',\n",
       "  'why': 'To have a more dynamic control for the airspace',\n",
       "  'how': '',\n",
       "  'whom': 'Poland airports',\n",
       "  'raw_bullet': 'Implement New aispace management features using ADA, MVC and C++  To have a more dynamic control for the airspace for Poland airports',\n",
       "  'corrected_bullet': 'Mplemented new airspace management features using ADA, MVC, and C++ to have a more dynamic control for the airspace for Poland airports.'},\n",
       " {'user_id': '',\n",
       "  'what': 'New aispace management features',\n",
       "  'verb': 'Implement',\n",
       "  'skills': '',\n",
       "  'why': 'To have a more dynamic control for the airspace',\n",
       "  'how': '',\n",
       "  'whom': 'Poland airports',\n",
       "  'raw_bullet': 'Implement New aispace management features using ADA and C++  To have a more dynamic control for the airspace for Poland airports',\n",
       "  'corrected_bullet': 'Mplemented new airspace management features using ADA and C++ to have a more dynamic control for the airspace for Poland airports.'},\n",
       " {'user_id': '',\n",
       "  'what': 'New aispace management features',\n",
       "  'verb': 'Implement',\n",
       "  'skills': '',\n",
       "  'why': 'To have a more dynamic control for the airspace',\n",
       "  'how': 'Applying coding standards ',\n",
       "  'whom': 'Poland airports',\n",
       "  'raw_bullet': 'Implement New aispace management features using ADA and C++ leveraging onApplying coding standards  To have a more dynamic control for the airspace for Poland airports',\n",
       "  'corrected_bullet': 'Mplemented new airspace management features using ADA and C++, leveraging on applying coding standards to have a more dynamic control for the airspace for Poland airports.'},\n",
       " {'user_id': '',\n",
       "  'what': 'New aispace management features',\n",
       "  'verb': 'Implement',\n",
       "  'skills': '',\n",
       "  'why': 'To have a more dynamic control over the airspace',\n",
       "  'how': 'Appying clean code concepts and internal coding standards',\n",
       "  'whom': 'Poland airports',\n",
       "  'raw_bullet': 'Implement New aispace management features using ADA and C++ leveraging onAppying clean code concepts and internal coding standards To have a more dynamic control over the airspace for Poland airports',\n",
       "  'corrected_bullet': 'Mplemented new airspace management features using ADA and C++, leveraging clean code concepts and internal coding standards to have a more dynamic control over the airspace for Poland airports.'},\n",
       " {'user_id': '',\n",
       "  'what': 'dimensional  models',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To streamline and organize complex data structures',\n",
       "  'how': '',\n",
       "  'whom': 'internal factored project',\n",
       "  'raw_bullet': 'Created dimensional  models using dbt, SQL, AWS and Redshift  To streamline and organize complex data structures for internal factored project',\n",
       "  'corrected_bullet': 'Created dimensional models using dbt, SQL, AWS, and Redshift to streamline and organize complex data structures for an internal factored project.'},\n",
       " {'user_id': '',\n",
       "  'what': 'exploratory  analysis',\n",
       "  'verb': 'Conducted',\n",
       "  'skills': '',\n",
       "  'why': 'To understand the market landscape',\n",
       "  'how': '',\n",
       "  'whom': 'internal factored project',\n",
       "  'raw_bullet': 'Conducted exploratory  analysis using Snowflake, Python, Tableau and Sql  To understand the market landscape for internal factored project',\n",
       "  'corrected_bullet': 'Conducted exploratory analysis using Snowflake, Python, Tableau, and SQL to understand the market landscape for internal factored project.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data warehouse infrastructure',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To provide a single source of truth for decision-making',\n",
       "  'how': '',\n",
       "  'whom': 'EEmovel',\n",
       "  'raw_bullet': 'Built data warehouse infrastructure using AWS Redshift  To provide a single source of truth for decision-making for EEmovel',\n",
       "  'corrected_bullet': 'Built data warehouse infrastructure using AWS Redshift to provide a single source of truth for decision-making for EEmovel.'},\n",
       " {'user_id': '',\n",
       "  'what': 'machine learning models',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'Increase sales representatives retention in the company, while increasing their productivity',\n",
       "  'how': 'XGBoost, Random Forest, Logistic Regression and various Machine Learning algorithms. The process involved feature engineering, feature selection, hyperparameter tuning, and model testing.',\n",
       "  'whom': 'one of the biggest insurance companies in the US',\n",
       "  'raw_bullet': 'Developed machine learning models using Python, Scikitlearn, R, Github and AWS leveraging onXGBoost, Random Forest, Logistic Regression and various Machine Learning algorithms. The process involved feature engineering, feature selection, hyperparameter tuning, and model testing. Increase sales representatives retention in the company, while increasing their productivity for one of the biggest insurance companies in the US',\n",
       "  'corrected_bullet': 'Developed machine learning models using Python, Scikitlearn, R, Github, and AWS leveraging on XGBoost, Random Forest, Logistic Regression, and various Machine Learning algorithms. The process involved feature engineering, feature selection, hyperparameter tuning, and model testing. Increased sales representatives retention in the company, while increasing their productivity for one of the biggest insurance companies in the US.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Favorite list feature',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To give to the user the posibility to add or remove tittles of their preference',\n",
       "  'how': '',\n",
       "  'whom': 'LATAM users',\n",
       "  'raw_bullet': 'Developed Favorite list feature using C++, You.I Engine and Command pattern  To give to the user the posibility to add or remove tittles of their preference for LATAM users',\n",
       "  'corrected_bullet': 'Developed the Favorite list feature using C++, You.I Engine, and Command pattern to give users the possibility to add or remove titles of their preference for LATAM users.'},\n",
       " {'user_id': '',\n",
       "  'what': 'ci cd system',\n",
       "  'verb': 'Use',\n",
       "  'skills': '',\n",
       "  'why': 'To deploy the software for the different devices',\n",
       "  'how': '',\n",
       "  'whom': 'DirectTV GO',\n",
       "  'raw_bullet': 'Use ci cd system using Gitlab  To deploy the software for the different devices for DirectTV GO',\n",
       "  'corrected_bullet': 'Used CI/CD system using GitLab to deploy the software for the different devices for DirectTV GO.'},\n",
       " {'user_id': '',\n",
       "  'what': 'production bug reports',\n",
       "  'verb': 'Fix',\n",
       "  'skills': '',\n",
       "  'why': 'To comply with quality standards',\n",
       "  'how': '',\n",
       "  'whom': 'QA team',\n",
       "  'raw_bullet': 'Fix production bug reports using C++ and Xcode debugging tool  To comply with quality standards for QA team',\n",
       "  'corrected_bullet': 'Fixed production bug reports using C++ and Xcode debugging tool to comply with quality standards for QA team.'},\n",
       " {'user_id': '',\n",
       "  'what': 'production bug reports',\n",
       "  'verb': 'Fix',\n",
       "  'skills': '',\n",
       "  'why': 'To comply with quality standards',\n",
       "  'how': '',\n",
       "  'whom': 'QA team of DirectTV',\n",
       "  'raw_bullet': 'Fix production bug reports using C++ and Xcode debugging tool  To comply with quality standards for QA team of DirectTV',\n",
       "  'corrected_bullet': 'Fixed production bug reports using C++ and Xcode debugging tool to comply with quality standards for the QA team of DirectTV.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Information of a title',\n",
       "  'verb': 'Display',\n",
       "  'skills': '',\n",
       "  'why': 'To provide the user with concise summary of the content',\n",
       "  'how': 'By consuming REST API Endpoints, paring th information and filling the predefined layers with the information',\n",
       "  'whom': 'Direct TV GO application',\n",
       "  'raw_bullet': 'Display Information of a title using C++ and You.I Engine leveraging onBy consuming REST API Endpoints, paring th information and filling the predefined layers with the information To provide the user with concise summary of the content for Direct TV GO application',\n",
       "  'corrected_bullet': 'Displayed information of a title using C++ and You.I Engine by leveraging REST API Endpoints, parsing the information, and filling the predefined layers with the information to provide the user with a concise summary of the content for the Direct TV GO application.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Reports of traffic meters',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To monitor the traffic of the different products',\n",
       "  'how': 'Creating SQL queries to extract the data loaded to the database',\n",
       "  'whom': 'Verizon Billing system',\n",
       "  'raw_bullet': 'Created Reports of traffic meters using C++ and Oracle DB leveraging onCreating SQL queries to extract the data loaded to the database To monitor the traffic of the different products for Verizon Billing system',\n",
       "  'corrected_bullet': 'Created reports of traffic meters using C++ and Oracle DB, leveraging on creating SQL queries to extract the data loaded to the database to monitor the traffic of the different products for Verizon Billing system.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Reports of traffic meters',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To monitor the traffic of the different products',\n",
       "  'how': 'Creating SQL queries to extract the data loaded into the database',\n",
       "  'whom': 'Verizon Billing system',\n",
       "  'raw_bullet': 'Created Reports of traffic meters using C++ and Oracle DB leveraging onCreating SQL queries to extract the data loaded into the database To monitor the traffic of the different products for Verizon Billing system',\n",
       "  'corrected_bullet': 'Created reports of traffic meters using C++ and Oracle DB, leveraging on creating SQL queries to extract the data loaded into the database to monitor the traffic of the different products for Verizon Billing system.'},\n",
       " {'user_id': '',\n",
       "  'what': 'development of a mortality model',\n",
       "  'verb': 'Supported',\n",
       "  'skills': '',\n",
       "  'why': 'update mortality tables for an specific product, in such way that the pricing of the products is consistent with the actual mortality risk of the clients',\n",
       "  'how': 'PH COX Model, statistical tests and optimization algorithms',\n",
       "  'whom': 'one of the biggest insurance companies the US',\n",
       "  'raw_bullet': 'Supported development of a mortality model using Python and R  leveraging onPH COX Model, statistical tests and optimization algorithms update mortality tables for an specific product, in such way that the pricing of the products is consistent with the actual mortality risk of the clients for one of the biggest insurance companies the US',\n",
       "  'corrected_bullet': 'Supported development of a mortality model using Python and R, leveraging the PH COX Model, statistical tests, and optimization algorithms to update mortality tables for a specific product. This ensured that the pricing of the products was consistent with the actual mortality risk of the clients for one of the biggest insurance companies in the US.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data processing pipeline',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To improve data quality and integrity',\n",
       "  'how': 'by creatin one single source of truth',\n",
       "  'whom': 'for a real estate company',\n",
       "  'raw_bullet': 'Developed data processing pipeline using Python, dbt and Airflow leveraging onby creatin one single source of truth To improve data quality and integrity for for a real estate company',\n",
       "  'corrected_bullet': 'Developed a data processing pipeline using Python, dbt, and Airflow, leveraging on creating one single source of truth to improve data quality and integrity for a real estate company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data lake',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To provide a single source of truth for decision-making',\n",
       "  'how': '',\n",
       "  'whom': 'EEmovel',\n",
       "  'raw_bullet': 'Built data lake using S3  To provide a single source of truth for decision-making for EEmovel',\n",
       "  'corrected_bullet': 'Built a data lake using S3 to provide a single source of truth for decision-making for EEmovel.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data lake',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To enable easy access and sharing of data across the organization',\n",
       "  'how': '',\n",
       "  'whom': 'EEmovel',\n",
       "  'raw_bullet': 'Built data lake using S3  To enable easy access and sharing of data across the organization for EEmovel',\n",
       "  'corrected_bullet': 'Built a data lake using S3 to enable easy access and sharing of data across the organization for EEmovel.'},\n",
       " {'user_id': '',\n",
       "  'what': 'computer vision solution (PoC)',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To develop a new solution that would inform in real time the bus occupation in the public transportation system of So Paulo',\n",
       "  'how': 'images from cameras that was installed over each door in the buses',\n",
       "  'whom': 'University',\n",
       "  'raw_bullet': 'Developed computer vision solution (PoC) using Python and OpenCV leveraging onimages from cameras that was installed over each door in the buses To develop a new solution that would inform in real time the bus occupation in the public transportation system of So Paulo for University',\n",
       "  'corrected_bullet': 'Developed a computer vision solution (PoC) using Python and OpenCV, leveraging images from cameras installed over each door in the buses to create a new solution that informed in real time the bus occupancy in the public transportation system of So Paulo for the University.'},\n",
       " {'user_id': '',\n",
       "  'what': 'reporting solution',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To make informed business decisions',\n",
       "  'how': '',\n",
       "  'whom': 'Operations',\n",
       "  'raw_bullet': 'Created reporting solution using Powerbi and Sql  To make informed business decisions for Operations',\n",
       "  'corrected_bullet': 'Created a reporting solution using PowerBI and SQL to make informed business decisions for Operations.'},\n",
       " {'user_id': '',\n",
       "  'what': 'reporting solution',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To make informed business decisions',\n",
       "  'how': '',\n",
       "  'whom': 'Operations',\n",
       "  'raw_bullet': 'Created reporting solution using Powerbi  To make informed business decisions for Operations',\n",
       "  'corrected_bullet': 'Created a reporting solution using PowerBI to make informed business decisions for Operations.'},\n",
       " {'user_id': '',\n",
       "  'what': 'reporting solution',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To measure, monitor and improve the company reputation and public image analyzing reveiws across differenct platmforms',\n",
       "  'how': '',\n",
       "  'whom': 'Operations',\n",
       "  'raw_bullet': 'Created reporting solution using Powerbi  To measure, monitor and improve the company reputation and public image analyzing reveiws across differenct platmforms for Operations',\n",
       "  'corrected_bullet': \"Created a reporting solution using PowerBI to measure, monitor, and improve the company's reputation and public image by analyzing reviews across different platforms for Operations.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'payment risk analysis',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To identify potential financial risks associated with payment delays or defaults',\n",
       "  'how': 'Creating an API to extrac data from credit bureaus sending it to BigQuery   ',\n",
       "  'whom': 'for the collections department of an HR services company',\n",
       "  'raw_bullet': 'Built payment risk analysis using Python, SQL, PowerBI, Metabase, Airflow and BigQuery leveraging onCreating an API to extrac data from credit bureaus sending it to BigQuery    To identify potential financial risks associated with payment delays or defaults for for the collections department of an HR services company',\n",
       "  'corrected_bullet': 'Built payment risk analysis using Python, SQL, PowerBI, Metabase, Airflow, and BigQuery, leveraging on creating an API to extract data from credit bureaus and sending it to BigQuery. This was done to identify potential financial risks associated with payment delays or defaults for the collections department of an HR services company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'payment risk analysis',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To identify potential financial risks associated with payment delays or defaults',\n",
       "  'how': 'Creating an API to extrac data from credit bureaus sending it to BigQuery   ',\n",
       "  'whom': 'for the collections department of an HR services company',\n",
       "  'raw_bullet': 'Built payment risk analysis using Python, SQL, PowerBI, Metabase, Airflow and BigQuery leveraging onCreating an API to extrac data from credit bureaus sending it to BigQuery    To identify potential financial risks associated with payment delays or defaults for for the collections department of an HR services company',\n",
       "  'corrected_bullet': 'Built payment risk analysis using Python, SQL, PowerBI, Metabase, Airflow, and BigQuery by leveraging on creating an API to extract data from credit bureaus and sending it to BigQuery. This was done to identify potential financial risks associated with payment delays or defaults for the collections department of an HR services company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis reports',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To make informed business decisions',\n",
       "  'how': '',\n",
       "  'whom': 'transport network company',\n",
       "  'raw_bullet': 'Created data analysis reports using Sql, Tableau, Powerbi and Python  To make informed business decisions for transport network company',\n",
       "  'corrected_bullet': 'Created data analysis reports using SQL, Tableau, PowerBI, and Python to make informed business decisions for a transport network company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis reports',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To make informed business decisions',\n",
       "  'how': 'Making and in debth analysis and presenting it to the stakeholders and CEO to help them to take better decisions',\n",
       "  'whom': 'transport network company',\n",
       "  'raw_bullet': 'Created data analysis reports using Sql, Tableau, Powerbi and Python leveraging onMaking and in debth analysis and presenting it to the stakeholders and CEO to help them to take better decisions To make informed business decisions for transport network company',\n",
       "  'corrected_bullet': 'Created data analysis reports using SQL, Tableau, PowerBI, and Python, leveraging in-depth analysis and presenting them to stakeholders and the CEO to help them make informed business decisions for a transport network company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis reports',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To make informed business decisions',\n",
       "  'how': 'Making and in debth analysis and presenting it to the stakeholders of finance area to help them to take better decisions and to know how company is going',\n",
       "  'whom': 'transport network company',\n",
       "  'raw_bullet': 'Created data analysis reports using Sql, Tableau, Powerbi and Python leveraging onMaking and in debth analysis and presenting it to the stakeholders of finance area to help them to take better decisions and to know how company is going To make informed business decisions for transport network company',\n",
       "  'corrected_bullet': 'Created data analysis reports using SQL, Tableau, PowerBI, and Python, leveraging in-depth analysis and presenting them to stakeholders in the finance area to help them make better decisions and understand how the company was going to make informed business decisions for a transport network company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'data analysis dashboards',\n",
       "  'verb': 'optimize',\n",
       "  'skills': '',\n",
       "  'why': 'to deliver clear, accurate and valuable information to customers, ensuring effective communication and informed decision-making',\n",
       "  'how': 'Understanding the main goal of each dashboard and enhance it',\n",
       "  'whom': 'trasnport network company',\n",
       "  'raw_bullet': 'optimize data analysis dashboards using Tableau and Powerbi leveraging onUnderstanding the main goal of each dashboard and enhance it to deliver clear, accurate and valuable information to customers, ensuring effective communication and informed decision-making for trasnport network company',\n",
       "  'corrected_bullet': 'Optimized data analysis dashboards using Tableau and Powerbi by leveraging an understanding of the main goal of each dashboard and enhancing it to deliver clear, accurate, and valuable information to customers. This ensured effective communication and informed decision-making for a transportation network company.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Performed a study of the concentration of particulate matter less than 2.5 micrometers in Bogota',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To contribute to research on air quality and public health in Bogota',\n",
       "  'how': '',\n",
       "  'whom': 'University Thesis',\n",
       "  'raw_bullet': 'Created Performed a study of the concentration of particulate matter less than 2.5 micrometers in Bogota using R Studio, Python and SQL  To contribute to research on air quality and public health in Bogota for University Thesis',\n",
       "  'corrected_bullet': 'Performed a study of the concentration of particulate matter less than 2.5 micrometers in Bogota using R Studio, Python, and SQL to contribute to research on air quality and public health in Bogota for my university thesis.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Performed a study of the concentration of particulate matter less than 2.5 micrometers in Bogota',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To contribute to research on air quality and public health in Bogota',\n",
       "  'how': 'Making an analysis about air quality and developing a Bayesian Model to predict this information ',\n",
       "  'whom': 'University Thesis',\n",
       "  'raw_bullet': 'Created Performed a study of the concentration of particulate matter less than 2.5 micrometers in Bogota using R Studio, Python and SQL leveraging onMaking an analysis about air quality and developing a Bayesian Model to predict this information  To contribute to research on air quality and public health in Bogota for University Thesis',\n",
       "  'corrected_bullet': 'Performed a study of the concentration of particulate matter less than 2.5 micrometers in Bogota using R Studio, Python, and SQL, leveraging on creating an analysis about air quality and developing a Bayesian Model to predict this information. This was done to contribute to research on air quality and public health in Bogota for my University Thesis.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Script receiver of files',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To move file to their corresponding input folder of the process',\n",
       "  'how': 'using regular expresions pattern (regex)',\n",
       "  'whom': 'Verizon Billing system',\n",
       "  'raw_bullet': 'Created Script receiver of files using Perl leveraging onusing regular expresions pattern (regex) To move file to their corresponding input folder of the process for Verizon Billing system',\n",
       "  'corrected_bullet': 'Created a Perl script to receive files, leveraging regular expression patterns (regex) to move the files to their corresponding input folder for the Verizon Billing system process.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Automated tasks',\n",
       "  'verb': 'Enhanced',\n",
       "  'skills': '',\n",
       "  'why': 'execute scheduled task automatic',\n",
       "  'how': 'Using cronjobs',\n",
       "  'whom': 'Verizon Billing system',\n",
       "  'raw_bullet': 'Enhanced Automated tasks using Cron and Unix leveraging onUsing cronjobs execute scheduled task automatic for Verizon Billing system',\n",
       "  'corrected_bullet': 'Enhanced automated tasks using Cron and Unix by leveraging cronjobs to execute scheduled tasks automatically for the Verizon Billing system.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Backup process',\n",
       "  'verb': 'Created ',\n",
       "  'skills': '',\n",
       "  'why': 'Save logging files ',\n",
       "  'how': 'Analyzing the size of the files and swaping the file to a new one',\n",
       "  'whom': 'Verizon Billing system',\n",
       "  'raw_bullet': 'Created  Backup process using Python leveraging onAnalyzing the size of the files and swaping the file to a new one Save logging files  for Verizon Billing system',\n",
       "  'corrected_bullet': 'Created a backup process using Python by analyzing the size of the files and swapping the file to a new one. Saved logging files for the Verizon Billing system.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Slot machine',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'Provide greater entertainment to the user and expand the market',\n",
       "  'how': '',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Developed Slot machine using C, C++, Mysql, OpenGL and Linux  Provide greater entertainment to the user and expand the market for Intervision gaming',\n",
       "  'corrected_bullet': 'Developed a slot machine using C, C++, MySQL, OpenGL, and Linux to provide greater entertainment to the user and expand the market for Intervision Gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Slot machine',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'Provide a new expirence to the user and expand the market',\n",
       "  'how': '',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Developed Slot machine using C, C++, Mysql, OpenGL and Linux  Provide a new expirence to the user and expand the market for Intervision gaming',\n",
       "  'corrected_bullet': 'Developed a slot machine using C, C++, MySQL, OpenGL, and Linux to provide a new experience to the user and expand the market for Intervision Gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Graphical interface',\n",
       "  'verb': 'implemented',\n",
       "  'skills': '',\n",
       "  'why': 'To improve user experience',\n",
       "  'how': '',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'implemented Graphical interface using C++, OpenGL and Linux  To improve user experience for Intervision gaming',\n",
       "  'corrected_bullet': 'Mplemented a graphical interface using C++, OpenGL, and Linux to improve the user experience for Intervision gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Graphical interface of the slot machine',\n",
       "  'verb': 'implemented',\n",
       "  'skills': '',\n",
       "  'why': 'To improve user experience',\n",
       "  'how': '',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'implemented Graphical interface of the slot machine using C++, OpenGL and Linux  To improve user experience for Intervision gaming',\n",
       "  'corrected_bullet': 'Mplemented the graphical interface of the slot machine using C++, OpenGL, and Linux to improve user experience for Intervision Gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'System architecture of the backed',\n",
       "  'verb': 'Designed',\n",
       "  'skills': '',\n",
       "  'why': 'To ensure scalability and flexibility in the system',\n",
       "  'how': '',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Designed System architecture of the backed using C++, Linux, IPC and Bash  To ensure scalability and flexibility in the system for Intervision gaming',\n",
       "  'corrected_bullet': 'Designed system architecture of the backend using C++, Linux, IPC, and Bash to ensure scalability and flexibility in the system for Intervision gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'System architecture of the backed',\n",
       "  'verb': 'Designed',\n",
       "  'skills': '',\n",
       "  'why': 'To ensure scalability and flexibility in the system',\n",
       "  'how': '',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Designed System architecture of the backed using C++, Linux, Mysql, IPC and Bash  To ensure scalability and flexibility in the system for Intervision gaming',\n",
       "  'corrected_bullet': 'Designed system architecture of the backend using C++, Linux, MySQL, IPC, and Bash to ensure scalability and flexibility in the system for Intervision Gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Driver for peripherals of the slot machine',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': '',\n",
       "  'why': 'stablish the comunication between the machine and the peripherals',\n",
       "  'how': 'Implementing the comunication protocol for each device',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Implemented Driver for peripherals of the slot machine using C, USB, RS232 and Linux leveraging onImplementing the comunication protocol for each device stablish the comunication between the machine and the peripherals for Intervision gaming',\n",
       "  'corrected_bullet': 'Mplemented driver for peripherals of the slot machine using C, USB, RS232, and Linux, leveraging on implementing the communication protocol for each device to establish communication between the machine and the peripherals for Intervision gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Installer application',\n",
       "  'verb': 'Implemented',\n",
       "  'skills': '',\n",
       "  'why': 'To minimize human errors by automating tasks and increase productivity',\n",
       "  'how': '',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Implemented Installer application using C++, Qt and SSH  To minimize human errors by automating tasks and increase productivity for Intervision gaming',\n",
       "  'corrected_bullet': 'Mplemented Installer application using C++, Qt, and SSH to minimize human errors by automating tasks and increase productivity for Intervision gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Installer application',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To minimize human errors by automating tasks and increase productivity',\n",
       "  'how': '',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Created Installer application using C++, Qt and SSH  To minimize human errors by automating tasks and increase productivity for Intervision gaming',\n",
       "  'corrected_bullet': 'Created an Installer application using C++, Qt, and SSH to minimize human errors by automating tasks and increase productivity for Intervision gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Installer application',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To automate the installation process of a slot machine ',\n",
       "  'how': '',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Created Installer application using C++, Qt and SSH  To automate the installation process of a slot machine  for Intervision gaming',\n",
       "  'corrected_bullet': 'Created an Installer application using C++, Qt, and SSH to automate the installation process of a slot machine for Intervision Gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Installer application',\n",
       "  'verb': 'Created',\n",
       "  'skills': '',\n",
       "  'why': 'To automate the installation process of a slot machine ',\n",
       "  'how': 'By using Qt for the graphical interface and ssh to transfer the files through a local network',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Created Installer application using C++, Qt and SSH leveraging onBy using Qt for the graphical interface and ssh to transfer the files through a local network To automate the installation process of a slot machine  for Intervision gaming',\n",
       "  'corrected_bullet': 'Created Installer application using C++, Qt, and SSH leveraging on Qt for the graphical interface and SSH to transfer the files through a local network to automate the installation process of a slot machine for Intervision gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Team of developers',\n",
       "  'verb': 'Managed',\n",
       "  'skills': '',\n",
       "  'why': 'To ensure that project goals are met on time',\n",
       "  'how': '',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Managed Team of developers using Scrum  To ensure that project goals are met on time for Intervision gaming',\n",
       "  'corrected_bullet': 'Managed a team of developers using Scrum to ensure that project goals were met on time for Intervision Gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Media rendering process of the games',\n",
       "  'verb': 'Enhanced',\n",
       "  'skills': '',\n",
       "  'why': 'To have a nice performance and smoth transitions',\n",
       "  'how': '',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Enhanced Media rendering process of the games using C++, Linux and OpenGL  To have a nice performance and smoth transitions for Intervision gaming',\n",
       "  'corrected_bullet': 'Enhanced the media rendering process of the games using C++, Linux, and OpenGL to achieve nice performance and smooth transitions for Intervision gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'Media rendering process of the games',\n",
       "  'verb': 'Enhanced',\n",
       "  'skills': '',\n",
       "  'why': 'To have a nice performance and smoth transitions',\n",
       "  'how': 'intentifing bootlenecks',\n",
       "  'whom': 'Intervision gaming',\n",
       "  'raw_bullet': 'Enhanced Media rendering process of the games using C++, Linux and OpenGL leveraging onintentifing bootlenecks To have a nice performance and smoth transitions for Intervision gaming',\n",
       "  'corrected_bullet': 'Enhanced the media rendering process of the games using C++, Linux, and OpenGL by leveraging on identifying bottlenecks to achieve nice performance and smooth transitions for Intervision gaming.'},\n",
       " {'user_id': '',\n",
       "  'what': 'dashboards',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To improve communication and collaboration among team members',\n",
       "  'how': '',\n",
       "  'whom': 'Carta',\n",
       "  'raw_bullet': 'Built dashboards using Looker and Sql  To improve communication and collaboration among team members for Carta',\n",
       "  'corrected_bullet': 'Built dashboards using Looker and SQL to improve communication and collaboration among team members for Carta.'},\n",
       " {'user_id': '',\n",
       "  'what': 'dbt tests',\n",
       "  'verb': 'Built',\n",
       "  'skills': '',\n",
       "  'why': 'To improve the overall quality and reliability of the software',\n",
       "  'how': '',\n",
       "  'whom': 'Carta',\n",
       "  'raw_bullet': 'Built dbt tests using Dbt and Sql  To improve the overall quality and reliability of the software for Carta',\n",
       "  'corrected_bullet': 'Built dbt tests using Dbt and SQL to improve the overall quality and reliability of the software for Carta.'},\n",
       " {'user_id': '',\n",
       "  'what': 'decision making process',\n",
       "  'verb': 'Led',\n",
       "  'skills': '',\n",
       "  'why': \"To align decision-making with the company's overall goals and objectives\",\n",
       "  'how': '',\n",
       "  'whom': 'Pessoalize',\n",
       "  'raw_bullet': \"Led decision making process using Data, Sql and Analytics  To align decision-making with the company's overall goals and objectives for Pessoalize\",\n",
       "  'corrected_bullet': \"Led the decision-making process using data, SQL, and analytics to align decision-making with the company's overall goals and objectives for Pessoalize.\"},\n",
       " {'user_id': '',\n",
       "  'what': 'data pipeline',\n",
       "  'verb': 'Developed',\n",
       "  'skills': '',\n",
       "  'why': 'To increase efficiency and productivity in data processing and analysis operations',\n",
       "  'how': '',\n",
       "  'whom': 'Carta',\n",
       "  'raw_bullet': 'Developed data pipeline using Dbt and Snowflake  To increase efficiency and productivity in data processing and analysis operations for Carta',\n",
       "  'corrected_bullet': 'Developed a data pipeline using Dbt and Snowflake to increase efficiency and productivity in data processing and analysis operations for Carta.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bullet_points_as_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 437/437 [14:40<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    "all_bullets_entities = compute_ner_for_all_bullets(bullets=bullet_points_as_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>what</th>\n",
       "      <th>how</th>\n",
       "      <th>why</th>\n",
       "      <th>whom</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recommendation System</td>\n",
       "      <td>Utilized Tensorflow, PyTorch, and natural lang...</td>\n",
       "      <td>To provide personalized recommendations based ...</td>\n",
       "      <td>Major relocation company</td>\n",
       "      <td>Tensorflow, NLP, Fargate, Python, PyTorch, Uns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recommendation System</td>\n",
       "      <td>Developed using Tensorflow, NLP, and Python, l...</td>\n",
       "      <td>To provide personalized recommendations based ...</td>\n",
       "      <td>Major relocation company</td>\n",
       "      <td>Tensorflow, NLP, Python, Unsupervised Learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>General goal performance dashboard</td>\n",
       "      <td>Built using Tableau, Python and Sql, leveragin...</td>\n",
       "      <td>To provide a visual representation of key perf...</td>\n",
       "      <td>Retail company</td>\n",
       "      <td>Tableau, Python, SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Desktop application</td>\n",
       "      <td>Built using Python, React, SQL, hyperthreading...</td>\n",
       "      <td>To enable offline access and functionality, re...</td>\n",
       "      <td>Software company</td>\n",
       "      <td>Python, React, SQL, Hyperthreading, Java applets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARIMA time series model</td>\n",
       "      <td>Built using Python, Sklearn, and Sktime levera...</td>\n",
       "      <td>To identify and detect anomalies or outliers i...</td>\n",
       "      <td>Retail company</td>\n",
       "      <td>Python, Sklearn, Sktime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Enhanced Media rendering process of the games</td>\n",
       "      <td>Using C++, Linux, and OpenGL to optimize perfo...</td>\n",
       "      <td>To improve performance and create smoother tra...</td>\n",
       "      <td>Intervision gaming</td>\n",
       "      <td>C++, Linux, OpenGL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Dashboards</td>\n",
       "      <td>Built using Looker and SQL</td>\n",
       "      <td>To improve communication and collaboration amo...</td>\n",
       "      <td>Carta</td>\n",
       "      <td>Looker, SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>dbt tests</td>\n",
       "      <td>built using Dbt and Sql</td>\n",
       "      <td>To improve the overall quality and reliability...</td>\n",
       "      <td>Carta</td>\n",
       "      <td>dbt, SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Decision-making process alignment with company...</td>\n",
       "      <td>Using Data, SQL, and Analytics</td>\n",
       "      <td>To align decision-making with company's overal...</td>\n",
       "      <td>Pessoalize</td>\n",
       "      <td>Data, SQL, Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>Data pipeline using Dbt and Snowflake</td>\n",
       "      <td>Developed using Dbt and Snowflake. The process...</td>\n",
       "      <td>To increase efficiency and productivity in dat...</td>\n",
       "      <td>Carta</td>\n",
       "      <td>Dbt, Snowflake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  what  \\\n",
       "0                                Recommendation System   \n",
       "1                                Recommendation System   \n",
       "2                   General goal performance dashboard   \n",
       "3                                  Desktop application   \n",
       "4                              ARIMA time series model   \n",
       "..                                                 ...   \n",
       "432      Enhanced Media rendering process of the games   \n",
       "433                                         Dashboards   \n",
       "434                                          dbt tests   \n",
       "435  Decision-making process alignment with company...   \n",
       "436              Data pipeline using Dbt and Snowflake   \n",
       "\n",
       "                                                   how  \\\n",
       "0    Utilized Tensorflow, PyTorch, and natural lang...   \n",
       "1    Developed using Tensorflow, NLP, and Python, l...   \n",
       "2    Built using Tableau, Python and Sql, leveragin...   \n",
       "3    Built using Python, React, SQL, hyperthreading...   \n",
       "4    Built using Python, Sklearn, and Sktime levera...   \n",
       "..                                                 ...   \n",
       "432  Using C++, Linux, and OpenGL to optimize perfo...   \n",
       "433                         Built using Looker and SQL   \n",
       "434                            built using Dbt and Sql   \n",
       "435                     Using Data, SQL, and Analytics   \n",
       "436  Developed using Dbt and Snowflake. The process...   \n",
       "\n",
       "                                                   why  \\\n",
       "0    To provide personalized recommendations based ...   \n",
       "1    To provide personalized recommendations based ...   \n",
       "2    To provide a visual representation of key perf...   \n",
       "3    To enable offline access and functionality, re...   \n",
       "4    To identify and detect anomalies or outliers i...   \n",
       "..                                                 ...   \n",
       "432  To improve performance and create smoother tra...   \n",
       "433  To improve communication and collaboration amo...   \n",
       "434  To improve the overall quality and reliability...   \n",
       "435  To align decision-making with company's overal...   \n",
       "436  To increase efficiency and productivity in dat...   \n",
       "\n",
       "                         whom  \\\n",
       "0    Major relocation company   \n",
       "1    Major relocation company   \n",
       "2              Retail company   \n",
       "3            Software company   \n",
       "4              Retail company   \n",
       "..                        ...   \n",
       "432        Intervision gaming   \n",
       "433                     Carta   \n",
       "434                     Carta   \n",
       "435                Pessoalize   \n",
       "436                     Carta   \n",
       "\n",
       "                                                skills  \n",
       "0    Tensorflow, NLP, Fargate, Python, PyTorch, Uns...  \n",
       "1    Tensorflow, NLP, Python, Unsupervised Learning...  \n",
       "2                                 Tableau, Python, SQL  \n",
       "3     Python, React, SQL, Hyperthreading, Java applets  \n",
       "4                              Python, Sklearn, Sktime  \n",
       "..                                                 ...  \n",
       "432                                 C++, Linux, OpenGL  \n",
       "433                                        Looker, SQL  \n",
       "434                                           dbt, SQL  \n",
       "435                               Data, SQL, Analytics  \n",
       "436                                     Dbt, Snowflake  \n",
       "\n",
       "[437 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bullets_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_entities = extract_entities(bullet_point=bullet_points_as_dict[9][\"raw_bullet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'what': 'Azured machine learning platform using Aws',\n",
       " 'how': 'By leveraging the Azure machine learning platform and incorporating AWS services to develop intelligent applications and services',\n",
       " 'why': 'To facilitate the development of intelligent applications and services for a retail company',\n",
       " 'whom': 'Retail company',\n",
       " 'skills': 'Azure, AWS'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
